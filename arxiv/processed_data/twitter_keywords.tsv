	abstract	arxiv_id	author_text	created	doi	num_authors	num_categories	title	updated	categories_str	url
74349	 In current study, a mechanism to extract traffic related information such as congestion and incidents from textual data from the internet is proposed. The current source of data is Twitter. As the data being considered is extremely large in size automated models are developed to stream, download, and mine the data in real-time. Furthermore, if any tweet has traffic related information then the models should be able to infer and extract this data. Currently, the data is collected only for United States and a total of 120,000 geo-tagged traffic related tweets are extracted, while six million geo-tagged non-traffic related tweets are retrieved and classification models are trained. Furthermore, this data is used for various kinds of spatial and temporal analysis. A mechanism to calculate level of traffic congestion, safety, and traffic perception for cities in U.S. is proposed. Traffic congestion and safety rankings for the various urban areas are obtained and then they are statistically validated with existing widely adopted rankings. Traffic perception depicts the attitude and perception of people towards the traffic. It is also seen that traffic related data when visualized spatially and temporally provides the same pattern as the actual traffic flows for various urban areas. When visualized at the city level, it is clearly visible that the flow of tweets is similar to flow of vehicles and that the traffic related tweets are representative of traffic within the cities. With all the findings in current study, it is shown that significant amount of traffic related information can be extracted from Twitter and other sources on internet. Furthermore, Twitter and these data sources are freely available and are not bound by spatial and temporal limitations. That is, wherever there is a user there is a potential for data. 	1801.05088	Chandra Khatri	2018-01-15		1	5	Real-time Road Traffic Information Detection Through Social Media		cs.CL, cs.AI, cs.CY, cs.IR, cs.SI	https://arxiv.org/pdf/1801.05088.pdf
146554	 Adversarial samples are strategically modified samples, which are crafted with the purpose of fooling a classifier at hand. An attacker introduces specially crafted adversarial samples to a deployed classifier, which are being mis-classified by the classifier. However, the samples are perceived to be drawn from entirely different classes and thus it becomes hard to detect the adversarial samples. Most of the prior works have been focused on synthesizing adversarial samples in the image domain. In this paper, we propose a new method of crafting adversarial text samples by modification of the original samples. Modifications of the original text samples are done by deleting or replacing the important or salient words in the text or by introducing new words in the text sample. Our algorithm works best for the datasets which have sub-categories within each of the classes of examples. While crafting adversarial samples, one of the key constraint is to generate meaningful sentences which can at pass off as legitimate from language (English) viewpoint. Experimental results on IMDB movie review dataset for sentiment analysis and Twitter dataset for gender detection show the efficiency of our proposed method. 	1707.02812	Suranjana Samanta, Sameep Mehta	2017-07-10		2	4	Towards Crafting Text Adversarial Samples		cs.LG, cs.AI, cs.CL, cs.CV	https://arxiv.org/pdf/1707.02812.pdf
222638	 Due to exponential growth of complex data, graph structure has become increasingly important to model various entities and their interactions, with many interesting applications including, bioinformatics, social network analysis, etc. Depending on the complexity of the data, the underlying graph model can be a simple directed/undirected and/or weighted/un-weighted graph to a complex graph (aka multi-attributed graph) where vertices and edges are labelled with multi-dimensional vectors. In this paper, we present a novel weighted distance measure based on weighted Euclidean norm which is defined as a function of both vertex and edge attributes, and it can be used for various graph analysis tasks including classification and cluster analysis. The proposed distance measure has flexibility to increase/decrease the weightage of edge labels while calculating the distance between vertex-pairs. We have also proposed a MAGDist algorithm, which reads multi-attributed graph stored in CSV files containing the list of vertex vectors and edge vectors, and calculates the distance between each vertex-pair using the proposed weighted distance measure. Finally, we have proposed a multi-attributed similarity graph generation algorithm, MAGSim, which reads the output of MAGDist algorithm and generates a similarity graph that can be analysed using classification and clustering algorithms. The significance and accuracy of the proposed distance measure and algorithms is evaluated on Iris and Twitter data sets, and it is found that the similarity graph generated by our proposed method yields better clustering results than the existing similarity graph generation methods. 	1801.07150	Muhammad Abulaish, Jahiruddin	2018-01-22		2	4	A Novel Weighted Distance Measure for Multi-Attributed Graph		cs.SI, cs.AI, cs.DS, cs.IR	https://arxiv.org/pdf/1801.07150.pdf
225951	 Social media provides a mechanism for people to engage with social causes across a range of issues. It also provides a strategic tool to those looking to advance a cause to exchange, promote or publicize their ideas. In such instances, AI can be either an asset if used appropriately or a barrier. One of the key issues for a workforce diversity campaign is to understand in real-time who is participating - specifically, whether the participants are individuals or organizations, and in case of individuals, whether they are male or female. In this paper, we present a study to demonstrate a case for AI for social good that develops a model to infer in real-time the different user types participating in a cause-driven hashtag campaign on Twitter, ILookLikeAnEngineer (ILLAE). A generic framework is devised to classify a Twitter user into three classes: organization, male and female in a real-time manner. The framework is tested against two datasets (ILLAE and a general dataset) and outperforms the baseline binary classifiers for categorizing organization/individual and male/female. The proposed model can be applied to future social cause-driven campaigns to get real-time insights on the macro-level social behavior of participants. 	1804.09304	Habib Karbasian, Hemant Purohit, Rajat Handa, Aqdas Malik, Aditya Johri	2018-04-24		5	2	Real-Time Inference of User Types to Assist with More Inclusive Social Media Activism Campaigns		cs.SI, cs.AI	https://arxiv.org/pdf/1804.09304.pdf
239793	 The problem of detecting bots, automated social media accounts governed by software but disguising as human users, has strong implications. For example, bots have been used to sway political elections by distorting online discourse, to manipulate the stock market, or to push anti-vaccine conspiracy theories that caused health epidemics. Most techniques proposed to date detect bots at the account level, by processing large amount of social media posts, and leveraging information from network structure, temporal dynamics, sentiment analysis, etc. In this paper, we propose a deep neural network based on contextual long short-term memory (LSTM) architecture that exploits both content and metadata to detect bots at the tweet level: contextual features are extracted from user metadata and fed as auxiliary input to LSTM deep nets processing the tweet text. Another contribution that we make is proposing a technique based on synthetic minority oversampling to generate a large labeled dataset, suitable for deep nets training, from a minimal amount of labeled data (roughly 3,000 examples of sophisticated Twitter bots). We demonstrate that, from just one single tweet, our architecture can achieve high classification accuracy (AUC > 96%) in separating bots from humans. We apply the same architecture to account-level bot detection, achieving nearly perfect classification accuracy (AUC > 99%). Our system outperforms previous state of the art while leveraging a small and interpretable set of features yet requiring minimal training data. 	1802.04289	Sneha Kudugunta, Emilio Ferrara	2018-02-12		2	2	Deep Neural Networks for Bot Detection	2018-02-18	cs.AI, cs.SI	https://arxiv.org/pdf/1802.04289.pdf
299491	 Variation in language is ubiquitous, particularly in newer forms of writing such as social media. Fortunately, variation is not random, it is often linked to social properties of the author. In this paper, we show how to exploit social networks to make sentiment analysis more robust to social language variation. The key idea is linguistic homophily: the tendency of socially linked individuals to use language in similar ways. We formalize this idea in a novel attention-based neural network architecture, in which attention is divided among several basis models, depending on the author's position in the social network. This has the effect of smoothing the classification function across the social network, and makes it possible to induce personalized classifiers even for authors for whom there is no labeled data or demographic metadata. This model significantly improves the accuracies of sentiment analysis on Twitter and on review data. 	1511.06052	Yi Yang, Jacob Eisenstein	2015-11-18		2	3	Overcoming Language Variation in Sentiment Analysis with Social Attention	2017-08-26	cs.CL, cs.AI, cs.SI	https://arxiv.org/pdf/1511.06052.pdf
339588	 The ever-growing datasets published on Linked Open Data mainly contain encyclopedic information. However, there is a lack of quality structured and semantically annotated datasets extracted from unstructured real-time sources. In this paper, we present principles for developing a knowledge graph of interlinked events using the case study of news headlines published on Twitter which is a real-time and eventful source of fresh information. We represent the essential pipeline containing the required tasks ranging from choosing background data model, event annotation (i.e., event recognition and classification), entity annotation and eventually interlinking events. The state-of-the-art is limited to domain-specific scenarios for recognizing and classifying events, whereas this paper plays the role of a domain-agnostic road-map for developing a knowledge graph of interlinked events. 	1808.02022	Saeedeh Shekarpour, Ankita Saxena, Krishnaprasad Thirunarayan, Valerie L. Shalin, Amit Sheth	2018-08-05		5	3	Principles for Developing a Knowledge Graph of Interlinked Events from News Headlines on Twitter		cs.CL, cs.AI, cs.IR	https://arxiv.org/pdf/1808.02022.pdf
350558	 What are the limits of automated Twitter sentiment classification? We analyze a large set of manually labeled tweets in different languages, use them as training data, and construct automated classification models. It turns out that the quality of classification models depends much more on the quality and size of training data than on the type of the model trained. Experimental results indicate that there is no statistically significant difference between the performance of the top classification models. We quantify the quality of training data by applying various annotator agreement measures, and identify the weakest points of different datasets. We show that the model performance approaches the inter-annotator agreement when the size of the training set is sufficiently large. However, it is crucial to regularly monitor the self- and inter-annotator agreements since this improves the training datasets and consequently the model performance. Finally, we show that there is strong evidence that humans perceive the sentiment classes (negative, neutral, and positive) as ordered. 	1602.07563	Igor Mozetic, Miha Grcar, Jasmina Smailovic	2016-02-24	10.1371/journal.pone.0155036	3	2	Multilingual Twitter Sentiment Classification: The Role of Human Annotators	2016-05-05	cs.CL, cs.AI	https://arxiv.org/pdf/1602.07563.pdf
463634	" Pathogenic Social Media (PSM) accounts such as terrorist supporters exploit large communities of supporters for conducting attacks on social media. Early detection of these accounts is crucial as they are high likely to be key users in making a harmful message ""viral"". In this paper, we make the first attempt on utilizing causal inference to identify PSMs within a short time frame around their activity. We propose a time-decay causality metric and incorporate it into a causal community detection-based algorithm. The proposed algorithm is applied to groups of accounts sharing similar causality features and is followed by a classification algorithm to classify accounts as PSM or not. Unlike existing techniques that take significant time to collect information such as network, cascade path, or content, our scheme relies solely on action log of users. Results on a real-world dataset from Twitter demonstrate effectiveness and efficiency of our approach. We achieved precision of 0.84 for detecting PSMs only based on their first 10 days of activity; the misclassified accounts were then detected 10 days later. "	1809.09331	Hamidreza Alvari, Elham Shaabani, Paulo Shakarian	2018-09-25		3	3	Early Identification of Pathogenic Social Media Accounts		cs.SI, cs.AI, cs.LG	https://arxiv.org/pdf/1809.09331.pdf
621606	 The enormous amount of texts published daily by Internet users has fostered the development of methods to analyze this content in several natural language processing areas, such as sentiment analysis. The main goal of this task is to classify the polarity of a message. Even though many approaches have been proposed for sentiment analysis, some of the most successful ones rely on the availability of large annotated corpus, which is an expensive and time-consuming process. In recent years, distant supervision has been used to obtain larger datasets. So, inspired by these techniques, in this paper we extend such approaches to incorporate popular graphic symbols used in electronic messages, the emojis, in order to create a large sentiment corpus for Portuguese. Trained on almost one million tweets, several models were tested in both same domain and cross-domain corpora. Our methods obtained very competitive results in five annotated corpora from mixed domains (Twitter and product reviews), which proves the domain-independent property of such approach. In addition, our results suggest that the combination of emoticons and emojis is able to properly capture the sentiment of a message. 	1707.02657	Edilson A. Corrêa, Vanessa Q. Marinho, Leandro B. dos Santos, Thales F. C. Bertaglia, Marcos V. Treviso, Henrico B. Brum	2017-07-09		6	3	PELESent: Cross-domain polarity classification using distant supervision		cs.CL, cs.AI, cs.LG	https://arxiv.org/pdf/1707.02657.pdf
647964	 This paper describes team Turing's submission to SemEval 2017 RumourEval: Determining rumour veracity and support for rumours (SemEval 2017 Task 8, Subtask A). Subtask A addresses the challenge of rumour stance classification, which involves identifying the attitude of Twitter users towards the truthfulness of the rumour they are discussing. Stance classification is considered to be an important step towards rumour verification, therefore performing well in this task is expected to be useful in debunking false rumours. In this work we classify a set of Twitter posts discussing rumours into either supporting, denying, questioning or commenting on the underlying rumours. We propose a LSTM-based sequential model that, through modelling the conversational structure of tweets, which achieves an accuracy of 0.784 on the RumourEval test set outperforming all other systems in Subtask A. 	1704.07221	Elena Kochkina, Maria Liakata, Isabelle Augenstein	2017-04-24		3	2	Turing at SemEval-2017 Task 8: Sequential Approach to Rumour Stance Classification with Branch-LSTM		cs.CL, cs.AI	https://arxiv.org/pdf/1704.07221.pdf
758406	 The rise in popularity and ubiquity of Twitter has made sentiment analysis of tweets an important and well-covered area of research. However, the 140 character limit imposed on tweets makes it hard to use standard linguistic methods for sentiment classification. On the other hand, what tweets lack in structure they make up with sheer volume and rich metadata. This metadata includes geolocation, temporal and author information. We hypothesize that sentiment is dependent on all these contextual factors. Different locations, times and authors have different emotional valences. In this paper, we explored this hypothesis by utilizing distant supervision to collect millions of labelled tweets from different locations, times and authors. We used this data to analyse the variation of tweet sentiments across different authors, times and locations. Once we explored and understood the relationship between these variables and sentiment, we used a Bayesian approach to combine these variables with more standard linguistic features such as n-grams to create a Twitter sentiment classifier. This combined classifier outperforms the purely linguistic classifier, showing that integrating the rich contextual information available on Twitter into sentiment classification is a promising direction of research. 	1605.05195	Soroush Vosoughi, Helen Zhou, Deb Roy	2016-05-17	10.18653/v1/W15-2904	3	4	Enhanced Twitter Sentiment Classification Using Contextual Information		cs.SI, cs.AI, cs.CL, cs.IR	https://arxiv.org/pdf/1605.05195.pdf
798320	 With the constant growth of the World Wide Web and the number of documents in different languages accordingly, the need for reliable language detection tools has increased as well. Platforms such as Twitter with predominantly short texts are becoming important information resources, which additionally imposes the need for short texts language detection algorithms. In this paper, we show how incorporating personalized user-specific information into the language detection algorithm leads to an important improvement of detection results. To choose the best algorithm for language detection for short text messages, we investigate several machine learning approaches. These approaches include the use of the well-known classifiers such as SVM and logistic regression, a dictionary based approach, and a probabilistic model based on modified Kneser-Ney smoothing. Furthermore, the extension of the probabilistic model to include additional user-specific information such as evidence accumulation per user and user interface language is explored, with the goal of improving the classification performance. The proposed approaches are evaluated on randomly collected Twitter data containing Latin as well as non-Latin alphabet languages and the quality of the obtained results is compared, followed by the selection of the best performing algorithm. This algorithm is then evaluated against two already existing general language detection tools: Chromium Compact Language Detector 2 (CLD2) and langid, where our method significantly outperforms the results achieved by both of the mentioned methods. Additionally, a preview of benefits and possible applications of having a reliable language detection algorithm is given. 	1608.08515	Ivana Balazevic, Mikio Braun, Klaus-Robert Müller	2016-08-30		3	2	Language Detection For Short Text Messages In Social Media		cs.CL, cs.AI	https://arxiv.org/pdf/1608.08515.pdf
820572	 Social messages classification is a research domain that has attracted the attention of many researchers in these last years. Indeed, the social message is different from ordinary text because it has some special characteristics like its shortness. Then the development of new approaches for the processing of the social message is now essential to make its classification more efficient. In this paper, we are mainly interested in the classification of social messages based on their spreading on online social networks (OSN). We proposed a new distance metric based on the Dynamic Time Warping distance and we use it with the probabilistic and the evidential k Nearest Neighbors (k-NN) classifiers to classify propagation networks (PrNets) of messages. The propagation network is a directed acyclic graph (DAG) that is used to record propagation traces of the message, the traversed links and their types. We tested the proposed metric with the chosen k-NN classifiers on real world propagation traces that were collected from Twitter social network and we got good classification accuracies. 	1701.07756	Siwar Jendoubi, Arnaud Martin, Ludovic Liétard, Boutheina Ben Yaghlane, Hend Ben Hadji	2017-01-26	10.1007/978-3-319-20807-7_38	5	3	Dynamic time warping distance for message propagation classification in Twitter		cs.AI, cs.SI, stat.ML	https://arxiv.org/pdf/1701.07756.pdf
869702	 Many aspects of people's lives are proven to be deeply connected to their jobs. In this paper, we first investigate the distinct characteristics of major occupation categories based on tweets. From multiple social media platforms, we gather several types of user information. From users' LinkedIn webpages, we learn their proficiencies. To overcome the ambiguity of self-reported information, a soft clustering approach is applied to extract occupations from crowd-sourced data. Eight job categories are extracted, including Marketing, Administrator, Start-up, Editor, Software Engineer, Public Relation, Office Clerk, and Designer. Meanwhile, users' posts on Twitter provide cues for understanding their linguistic styles, interests, and personalities. Our results suggest that people of different jobs have unique tendencies in certain language styles and interests. Our results also clearly reveal distinctive levels in terms of Big Five Traits for different jobs. Finally, a classifier is built to predict job types based on the features extracted from tweets. A high accuracy indicates a strong discrimination power of language features for job prediction task. 	1701.06233	Tianran Hu, Haoyuan Xiao, Thuy-vy Thi Nguyen, Jiebo Luo	2017-01-22		4	4	What the Language You Tweet Says About Your Occupation		cs.CY, cs.AI, cs.CL, cs.LG	https://arxiv.org/pdf/1701.06233.pdf
884239	 This paper introduces TwitterPaul, a system designed to make use of Social Media data to help to predict game outcomes for the 2010 FIFA World Cup tournament. To this end, we extracted over 538K mentions to football games from a large sample of tweets that occurred during the World Cup, and we classified into different types with a precision of up to 88%. The different mentions were aggregated in order to make predictions about the outcomes of the actual games. We attempt to learn which Twitter users are accurate predictors and explore several techniques in order to exploit this information to make more accurate predictions. We compare our results to strong baselines and against the betting line (prediction market) and found that the quality of extractions is more important than the quantity, suggesting that high precision methods working on a medium-sized dataset are preferable over low precision methods that use a larger amount of data. Finally, by aggregating some classes of predictions, the system performance is close to the one of the betting line. Furthermore, we believe that this domain independent framework can help to predict other sports, elections, product release dates and other future events that people talk about in social media. 	1211.6496	Naushad UzZaman, Roi Blanco, Michael Matthews	2012-11-27		3	3	TwitterPaul: Extracting and Aggregating Twitter Predictions	2012-11-30	cs.SI, cs.AI, physics.soc-ph	https://arxiv.org/pdf/1211.6496.pdf
901273	 Location tagging, also known as geotagging or geolocation, is the process of assigning geographical coordinates to input data. In this paper we present an algorithm for location tagging of textual documents. Our approach makes use of previous work in natural language processing by using a state-of-the-art part-of-speech tagger and named entity recognizer to find blocks of text which may refer to locations. A knowledge base (OpenStreatMap) is then used to find a list of possible locations for each block. Finally, one location is chosen for each block by assigning distance-based scores to each location and repeatedly selecting the location and block with the best score. We tested our geolocation algorithm with Wikipedia articles about topics with a well-defined geographical location that are geotagged by the articles' authors, where classification approaches have achieved median errors as low as 11 km, with attainable accuracy limited by the class size. Our approach achieved a 10th percentile error of 490 metres and median error of 54 kilometres on the Wikipedia dataset we used. When considering the five location tags with the greatest scores, 50% of articles were assigned at least one tag within 8.5 kilometres of the article's author-assigned true location. We also tested our approach on Twitter messages that are tagged with the location from which the message was sent. Twitter texts are challenging because they are short and unstructured and often do not contain words referring to the location they were sent from, but we obtain potentially useful results. We explain how we use the Spark framework for data analytics to collect and process our test data. In general, classification-based approaches for location tagging may be reaching their upper accuracy limit, but our precision-focused approach has high accuracy for some texts and shows significant potential for improvement overall. 	1601.05893	Shawn Brunsting, Hans De Sterck, Remco Dolman, Teun van Sprundel	2016-01-22		4	4	GeoTextTagger: High-Precision Location Tagging of Textual Documents using a Natural Language Processing Approach		cs.AI, cs.CL, cs.DB, cs.IR	https://arxiv.org/pdf/1601.05893.pdf
1081037	 Nowadays, social networks such as Twitter, Facebook and LinkedIn become increasingly popular. In fact, they introduced new habits, new ways of communication and they collect every day several information that have different sources. Most existing research works fo-cus on the analysis of homogeneous social networks, i.e. we have a single type of node and link in the network. However, in the real world, social networks offer several types of nodes and links. Hence, with a view to preserve as much information as possible, it is important to consider so-cial networks as heterogeneous and uncertain. The goal of our paper is to classify the social message based on its spreading in the network and the theory of belief functions. The proposed classifier interprets the spread of messages on the network, crossed paths and types of links. We tested our classifier on a real word network that we collected from Twitter, and our experiments show the performance of our belief classifier. 	1501.05426	Siwar Jendoubi, Arnaud Martin, Ludovic Liétard, Boutheina Ben Yaghlane	2015-01-22	10.1007/978-3-319-08855-6_8	4	3	Classification of Message Spreading in a Heterogeneous Social Network		cs.SI, cs.AI, physics.soc-ph	https://arxiv.org/pdf/1501.05426.pdf
1339738	 Social media has grown to be a crucial information source for pharmacovigilance studies where an increasing number of people post adverse reactions to medical drugs that are previously unreported. Aiming to effectively monitor various aspects of Adverse Drug Reactions (ADRs) from diversely expressed social medical posts, we propose a multi-task neural network framework that learns several tasks associated with ADR monitoring with different levels of supervisions collectively. Besides being able to correctly classify ADR posts and accurately extract ADR mentions from online posts, the proposed framework is also able to further understand reasons for which the drug is being taken, known as 'indication', from the given social media post. A coverage-based attention mechanism is adopted in our framework to help the model properly identify 'phrasal' ADRs and Indications that are attentive to multiple words in a post. Our framework is applicable in situations where limited parallel data for different pharmacovigilance tasks are available.We evaluate the proposed framework on real-world Twitter datasets, where the proposed model outperforms the state-of-the-art alternatives of each individual task consistently. 	1801.06294	Shaika Chowdhury, Chenwei Zhang, Philip S. Yu	2018-01-19	10.1145/3178876.3186053	3	3	Multi-Task Pharmacovigilance Mining from Social Media Posts	2018-02-16	cs.LG, cs.AI, cs.CL	https://arxiv.org/pdf/1801.06294.pdf
1406656	 Metadata are associated to most of the information we produce in our daily interactions and communication in the digital world. Yet, surprisingly, metadata are often still catergorized as non-sensitive. Indeed, in the past, researchers and practitioners have mainly focused on the problem of the identification of a user from the content of a message. In this paper, we use Twitter as a case study to quantify the uniqueness of the association between metadata and user identity and to understand the effectiveness of potential obfuscation strategies. More specifically, we analyze atomic fields in the metadata and systematically combine them in an effort to classify new tweets as belonging to an account using different machine learning algorithms of increasing complexity. We demonstrate that through the application of a supervised learning algorithm, we are able to identify any user in a group of 10,000 with approximately 96.7% accuracy. Moreover, if we broaden the scope of our search and consider the 10 most likely candidates we increase the accuracy of the model to 99.22%. We also found that data obfuscation is hard and ineffective for this type of data: even after perturbing 60% of the training data, it is still possible to classify users with an accuracy higher than 95%. These results have strong implications in terms of the design of metadata obfuscation strategies, for example for data set release, not only for Twitter, but, more generally, for most social media platforms. 	1803.10133	Beatrice Perez, Mirco Musolesi, Gianluca Stringhini	2018-03-27		3	3	You are your Metadata: Identification and Obfuscation of Social Media Users using Metadata Information	2018-05-14	cs.CR, cs.AI, cs.SI	https://arxiv.org/pdf/1803.10133.pdf
40506	 This paper describes our deep learning-based approach to sentiment analysis in Twitter as part of SemEval-2016 Task 4. We use a convolutional neural network to determine sentiment and participate in all subtasks, i.e. two-point, three-point, and five-point scale sentiment classification and two-point and five-point scale sentiment quantification. We achieve competitive results for two-point scale sentiment classification and quantification, ranking fifth and a close fourth (third and second by alternative metrics) respectively despite using only pre-trained embeddings that contain no sentiment information. We achieve good performance on three-point scale sentiment classification, ranking eighth out of 35, while performing poorly on five-point scale sentiment classification and quantification. An error analysis reveals that this is due to low expressiveness of the model to capture negative sentiment as well as an inability to take into account ordinal information. We propose improvements in order to address these and other issues. 	1609.02746	Sebastian Ruder, Parsa Ghaffari, John G. Breslin	2016-09-09		3	2	INSIGHT-1 at SemEval-2016 Task 4: Convolutional Neural Networks for Sentiment Classification and Quantification		cs.CL, cs.LG	https://arxiv.org/pdf/1609.02746.pdf
67753	 This paper presents the results and conclusions of our participation in the Clickbait Challenge 2017 on automatic clickbait detection in social media. We first describe linguistically-infused neural network models and identify informative representations to predict the level of clickbaiting present in Twitter posts. Our models allow to answer the question not only whether a post is a clickbait or not, but to what extent it is a clickbait post e.g., not at all, slightly, considerably, or heavily clickbaity using a score ranging from 0 to 1. We evaluate the predictive power of models trained on varied text and image representations extracted from tweets. Our best performing model that relies on the tweet text and linguistic markers of biased language extracted from the tweet and the corresponding page yields mean squared error (MSE) of 0.04, mean absolute error (MAE) of 0.16 and R2 of 0.43 on the held-out test data. For the binary classification setup (clickbait vs. non-clickbait), our model achieved F1 score of 0.69. We have not found that image representations combined with text yield significant performance improvement yet. Nevertheless, this work is the first to present preliminary analysis of objects extracted using Google Tensorflow object detection API from images in clickbait vs. non-clickbait Twitter posts. Finally, we outline several steps to improve model performance as a part of the future work. 	1710.06390	Maria Glenski, Ellyn Ayton, Dustin Arendt, Svitlana Volkova	2017-10-17		4	3	Fishing for Clickbaits in Social Images and Texts with Linguistically-Infused Neural Network Models		cs.LG, cs.CL, cs.SI	https://arxiv.org/pdf/1710.06390.pdf
98863	 The success of deep neural networks (DNNs) is heavily dependent on the availability of labeled data. However, obtaining labeled data is a big challenge in many real-world problems. In such scenarios, a DNN model can leverage labeled and unlabeled data from a related domain, but it has to deal with the shift in data distributions between the source and the target domains. In this paper, we study the problem of classifying social media posts during a crisis event (e.g., Earthquake). For that, we use labeled and unlabeled data from past similar events (e.g., Flood) and unlabeled data for the current event. We propose a novel model that performs adversarial learning based domain adaptation to deal with distribution drifts and graph based semi-supervised learning to leverage unlabeled data within a single unified deep learning framework. Our experiments with two real-world crisis datasets collected from Twitter demonstrate significant improvements over several baselines. 	1805.05151	Firoj Alam, Shafiq Joty, Muhammad Imran	2018-05-14		3	2	Domain Adaptation with Adversarial Training and Graph Embeddings		cs.LG, stat.ML	https://arxiv.org/pdf/1805.05151.pdf
141831	" We explore two techniques which use color to make sense of statistical text models. One method uses in-text annotations to illustrate a model's view of particular tokens in particular documents. Another uses a high-level, ""words-as-pixels"" graphic to display an entire corpus. Together, these methods offer both zoomed-in and zoomed-out perspectives into a model's understanding of text. We show how these interconnected methods help diagnose a classifier's poor performance on Twitter slang, and make sense of a topic model on historical political texts. "	1606.06352	Abram Handler, Su Lin Blodgett, Brendan O'Connor	2016-06-20		3	3	Visualizing textual models with in-text and word-as-pixel highlighting		stat.ML, cs.CL, cs.LG	https://arxiv.org/pdf/1606.06352.pdf
165421	 We introduce a new approach to tackle the problem of offensive language in online social media. Our approach uses unsupervised text style transfer to translate offensive sentences into non-offensive ones. We propose a new method for training encoder-decoders using non-parallel data that combines a collaborative classifier, attention and the cycle consistency loss. Experimental results on data from Twitter and Reddit show that our method outperforms a state-of-the-art text style transfer system in two out of three quantitative metrics and produces reliable non-offensive transferred sentences. 	1805.07685	Cicero Nogueira dos Santos, Igor Melnyk, Inkit Padhi	2018-05-19		3	2	Fighting Offensive Language on Social Media with Unsupervised Text Style Transfer		cs.CL, cs.LG	https://arxiv.org/pdf/1805.07685.pdf
191366	" Most past work on social network link fraud detection tries to separate genuine users from fraudsters, implicitly assuming that there is only one type of fraudulent behavior. But is this assumption true? And, in either case, what are the characteristics of such fraudulent behaviors? In this work, we set up honeypots (""dummy"" social network accounts), and buy fake followers (after careful IRB approval). We report the signs of such behaviors including oddities in local network connectivity, account attributes, and similarities and differences across fraud providers. Most valuably, we discover and characterize several types of fraud behaviors. We discuss how to leverage our insights in practice by engineering strongly performing entropy-based features and demonstrating high classification accuracy. Our contributions are (a) instrumentation: we detail our experimental setup and carefully engineered data collection process to scrape Twitter data while respecting API rate-limits, (b) observations on fraud multimodality: we analyze our honeypot fraudster ecosystem and give surprising insights into the multifaceted behaviors of these fraudster types, and (c) features: we propose novel features that give strong (>0.95 precision/recall) discriminative power on ground-truth Twitter data. "	1704.01420	Neil Shah, Hemank Lamba, Alex Beutel, Christos Faloutsos	2017-04-05		4	2	The Many Faces of Link Fraud	2017-09-11	cs.SI, cs.LG	https://arxiv.org/pdf/1704.01420.pdf
200456	 The role of social media, in particular microblogging platforms such as Twitter, as a conduit for actionable and tactical information during disasters is increasingly acknowledged. However, time-critical analysis of big crisis data on social media streams brings challenges to machine learning techniques, especially the ones that use supervised learning. The Scarcity of labeled data, particularly in the early hours of a crisis, delays the machine learning process. The current state-of-the-art classification methods require a significant amount of labeled data specific to a particular event for training plus a lot of feature engineering to achieve best results. In this work, we introduce neural network based classification methods for binary and multi-class tweet classification task. We show that neural network based models do not require any feature engineering and perform better than state-of-the-art methods. In the early hours of a disaster when no labeled data is available, our proposed method makes the best use of the out-of-event data and achieves good results. 	1608.03902	Dat Tien Nguyen, Kamela Ali Al Mannai, Shafiq Joty, Hassan Sajjad, Muhammad Imran, Prasenjit Mitra	2016-08-12		6	3	Rapid Classification of Crisis-Related Data on Social Networks using Convolutional Neural Networks		cs.CL, cs.LG, cs.SI	https://arxiv.org/pdf/1608.03902.pdf
206582	" Ensemble techniques for classification and clustering have long proven effective, yet anomaly ensembles have been barely studied. In this work, we tap into this gap and propose a new ensemble approach for anomaly mining, with application to event detection in temporal graphs. Our method aims to combine results from heterogeneous detectors with varying outputs, and leverage the evidence from multiple sources to yield better performance. However, trusting all the results may deteriorate the overall ensemble accuracy, as some detectors may fall short and provide inaccurate results depending on the nature of the data in hand. This suggests that being selective in which results to combine is vital in building effective ensembles---hence ""less is more"". In this paper we propose SELECT; an ensemble approach for anomaly mining that employs novel techniques to automatically and systematically select the results to assemble in a fully unsupervised fashion. We apply our method to event detection in temporal graphs, where SELECT successfully utilizes five base detectors and seven consensus methods under a unified ensemble framework. We provide extensive quantitative evaluation of our approach on five real-world datasets (four with ground truth), including Enron email communications, New York Times news corpus, and World Cup 2014 Twitter news feed. Thanks to its selection mechanism, SELECT yields superior performance compared to individual detectors alone, the full ensemble (naively combining all results), and an existing diversity-based ensemble. "	1501.01924	Shebuti Rayana, Leman Akoglu	2015-01-08		2	2	Less is More: Building Selective Anomaly Ensembles		cs.DB, cs.LG	https://arxiv.org/pdf/1501.01924.pdf
238103	 Tropical diseases like \textit{Chikungunya} and \textit{Zika} have come to prominence in recent years as the cause of serious, long-lasting, population-wide health problems. In large countries like Brasil, traditional disease prevention programs led by health authorities have not been particularly effective. We explore the hypothesis that monitoring and analysis of social media content streams may effectively complement such efforts. Specifically, we aim to identify selected members of the public who are likely to be sensitive to virus combat initiatives that are organised in local communities. Focusing on Twitter and on the topic of Zika, our approach involves (i) training a classifier to select topic-relevant tweets from the Twitter feed, and (ii) discovering the top users who are actively posting relevant content about the topic. We may then recommend these users as the prime candidates for direct engagement within their community. In this short paper we describe our analytical approach and prototype architecture, discuss the challenges of dealing with noisy and sparse signal, and present encouraging preliminary results. 	1703.03928	Paolo Missier, Callum McClean, Jonathan Carlton, Diego Cedrim, Leonardo Silva, Alessandro Garcia, Alexandre Plastino, Alexander Romanovsky	2017-03-11		8	2	Recruiting from the network: discovering Twitter users who can help combat Zika epidemics		cs.LG, cs.SI	https://arxiv.org/pdf/1703.03928.pdf
264858	 This paper studies users' perception regarding a controversial product, namely self-driving (autonomous) cars. To find people's opinion regarding this new technology, we used an annotated Twitter dataset, and extracted the topics in positive and negative tweets using an unsupervised, probabilistic model known as topic modeling. We later used the topics, as well as linguist and Twitter specific features to classify the sentiment of the tweets. Regarding the opinions, the result of our analysis shows that people are optimistic and excited about the future technology, but at the same time they find it dangerous and not reliable. For the classification task, we found Twitter specific features, such as hashtags as well as linguistic features such as emphatic words among top attributes in classifying the sentiment of the tweets. 	1804.04058	Rizwan Sadiq, Mohsin Khan	2018-04-05		2	4	Analyzing Self-Driving Cars on Twitter		cs.LG, cs.CL, cs.SI, stat.ML	https://arxiv.org/pdf/1804.04058.pdf
361550	 In this article, we propose a new Support Vector Machine (SVM) training algorithm based on distributed MapReduce technique. In literature, there are a lots of research that shows us SVM has highest generalization property among classification algorithms used in machine learning area. Also, SVM classifier model is not affected by correlations of the features. But SVM uses quadratic optimization techniques in its training phase. The SVM algorithm is formulated as quadratic optimization problem. Quadratic optimization problem has $O(m^3)$ time and $O(m^2)$ space complexity, where m is the training set size. The computation time of SVM training is quadratic in the number of training instances. In this reason, SVM is not a suitable classification algorithm for large scale dataset classification. To solve this training problem we developed a new distributed MapReduce method developed. Accordingly, (i) SVM algorithm is trained in distributed dataset individually; (ii) then merge all support vectors of classifier model in every trained node; and (iii) iterate these two steps until the classifier model converges to the optimal classifier function. In the implementation phase, large scale social media dataset is presented in TFxIDF matrix. The matrix is used for sentiment analysis to get polarization value. Two and three class models are created for classification method. Confusion matrices of each classification model are presented in tables. Social media messages corpus consists of 108 public and 66 private universities messages in Turkey. Twitter is used for source of corpus. Twitter user messages are collected using Twitter Streaming API. Results are shown in graphics and tables. 	1410.2686	Ferhat Özgür Çatak	2014-10-10		1	2	Polarization Measurement of High Dimensional Social Media Messages With Support Vector Machine Algorithm Using Mapreduce	2015-03-11	cs.LG, cs.CL	https://arxiv.org/pdf/1410.2686.pdf
364906	 Social media has been considered as a data source for tracking disease. However, most analyses are based on models that prioritize strong correlation with population-level disease rates over determining whether or not specific individual users are actually sick. Taking a different approach, we develop a novel system for social-media based disease detection at the individual level using a sample of professionally diagnosed individuals. Specifically, we develop a system for making an accurate influenza diagnosis based on an individual's publicly available Twitter data. We find that about half (17/35 = 48.57%) of the users in our sample that were sick explicitly discuss their disease on Twitter. By developing a meta classifier that combines text analysis, anomaly detection, and social network analysis, we are able to diagnose an individual with greater than 99% accuracy even if she does not discuss her health. 	1404.3026	Todd Bodnar, Victoria C Barclay, Nilam Ram, Conrad S Tucker, Marcel Salathé	2014-04-11	10.1145/2567948.2579272	5	3	On the Ground Validation of Online Diagnosis with Twitter and Medical Records		cs.SI, cs.CL, cs.LG	https://arxiv.org/pdf/1404.3026.pdf
381167	 Increasing popularity of Twitter in politics is subject to commercial and academic interest. To fully exploit the merits of this platform, reaching the target audience with desired political leanings is critical. This paper extends the research on inferring political orientations of Twitter users to the case of 2017 Turkish constitutional referendum. After constructing a targeted dataset of tweets, we explore several types of potential features to build accurate machine learning based predictive models. In our experiments, a three-class support vector machine (SVM) classifier trained on semantic features achieves the best accuracy score of 89.9%. Moreover, an SVM classifier trained on full-text features performs better than an SVM classifier trained on hashtags, with respective accuracy scores of 89.05% and 85.9%. Relatively high accuracy scores obtained by full-text features may point to differences in language use, which deserves further research. 	1809.05699	Kutlu Emre Yilmaz, Osman Abul	2018-09-15		2	2	Inferring Political Alignments of Twitter Users: A case study on 2017 Turkish constitutional referendum		cs.SI, cs.LG	https://arxiv.org/pdf/1809.05699.pdf
402986	 Unsupervised methods have proven effective for discriminative tasks in a single-modality scenario. In this paper, we present a multimodal framework for learning sparse representations that can capture semantic correlation between modalities. The framework can model relationships at a higher level by forcing the shared sparse representation. In particular, we propose the use of joint dictionary learning technique for sparse coding and formulate the joint representation for concision, cross-modal representations (in case of a missing modality), and union of the cross-modal representations. Given the accelerated growth of multimodal data posted on the Web such as YouTube, Wikipedia, and Twitter, learning good multimodal features is becoming increasingly important. We show that the shared representations enabled by our framework substantially improve the classification performance under both unimodal and multimodal settings. We further show how deep architectures built on the proposed framework are effective for the case of highly nonlinear correlations between modalities. The effectiveness of our approach is demonstrated experimentally in image denoising, multimedia event detection and retrieval on the TRECVID dataset (audio-video), category classification on the Wikipedia dataset (image-text), and sentiment classification on PhotoTweet (image-text). 	1511.06238	Miriam Cha, Youngjune Gwon, H. T. Kung	2015-11-19		3	3	Multimodal sparse representation learning and applications	2016-03-02	cs.LG, cs.CV, stat.ML	https://arxiv.org/pdf/1511.06238.pdf
441070	 The rise in popularity of microblogging services like Twitter has led to increased use of content annotation strategies like the hashtag. Hashtags provide users with a tagging mechanism to help organize, group, and create visibility for their posts. This is a simple idea but can be challenging for the user in practice which leads to infrequent usage. In this paper, we will investigate various methods of recommending hashtags as new posts are created to encourage more widespread adoption and usage. Hashtag recommendation comes with numerous challenges including processing huge volumes of streaming data and content which is small and noisy. We will investigate preprocessing methods to reduce noise in the data and determine an effective method of hashtag recommendation based on the popular classification algorithms. 	1502.00094	Roman Dovgopol, Matt Nohelty	2015-01-31		2	2	Twitter Hash Tag Recommendation		cs.IR, cs.LG	https://arxiv.org/pdf/1502.00094.pdf
495258	 $\textit{Fake followers}$ are those Twitter accounts specifically created to inflate the number of followers of a target account. Fake followers are dangerous for the social platform and beyond, since they may alter concepts like popularity and influence in the Twittersphere - hence impacting on economy, politics, and society. In this paper, we contribute along different dimensions. First, we review some of the most relevant existing features and rules (proposed by Academia and Media) for anomalous Twitter accounts detection. Second, we create a baseline dataset of verified human and fake follower accounts. Such baseline dataset is publicly available to the scientific community. Then, we exploit the baseline dataset to train a set of machine-learning classifiers built over the reviewed rules and features. Our results show that most of the rules proposed by Media provide unsatisfactory performance in revealing fake followers, while features proposed in the past by Academia for spam detection provide good results. Building on the most promising features, we revise the classifiers both in terms of reduction of overfitting and cost for gathering the data needed to compute the features. The final result is a novel $\textit{Class A}$ classifier, general enough to thwart overfitting, lightweight thanks to the usage of the less costly features, and still able to correctly classify more than 95% of the accounts of the original training set. We ultimately perform an information fusion-based sensitivity analysis, to assess the global sensitivity of each of the features employed by the classifier. The findings reported in this paper, other than being supported by a thorough experimental methodology and interesting on their own, also pave the way for further investigation on the novel issue of fake Twitter followers. 	1509.04098	Stefano Cresci, Roberto Di Pietro, Marinella Petrocchi, Angelo Spognardi, Maurizio Tesconi	2015-09-14	10.1016/j.dss.2015.09.003	5	3	Fame for sale: efficient detection of fake Twitter followers	2015-11-10	cs.SI, cs.CR, cs.LG	https://arxiv.org/pdf/1509.04098.pdf
595975	 Social media such as tweets are emerging as platforms contributing to situational awareness during disasters. Information shared on Twitter by both affected population (e.g., requesting assistance, warning) and those outside the impact zone (e.g., providing assistance) would help first responders, decision makers, and the public to understand the situation first-hand. Effective use of such information requires timely selection and analysis of tweets that are relevant to a particular disaster. Even though abundant tweets are promising as a data source, it is challenging to automatically identify relevant messages since tweet are short and unstructured, resulting to unsatisfactory classification performance of conventional learning-based approaches. Thus, we propose a simple yet effective algorithm to identify relevant messages based on matching keywords and hashtags, and provide a comparison between matching-based and learning-based approaches. To evaluate the two approaches, we put them into a framework specifically proposed for analyzing disaster-related tweets. Analysis results on eleven datasets with various disaster types show that our technique provides relevant tweets of higher quality and more interpretable results of sentiment analysis tasks when compared to learning approach. 	1705.02009	Hien To, Sumeet Agrawal, Seon Ho Kim, Cyrus Shahabi	2017-05-04		4	2	On Identifying Disaster-Related Tweets: Matching-based or Learning-based?		cs.IR, cs.LG	https://arxiv.org/pdf/1705.02009.pdf
622749	 Deep learning algorithms have recently produced state-of-the-art accuracy in many classification tasks, but this success is typically dependent on access to many annotated training examples. For domains without such data, an attractive alternative is to train models with light, or distant supervision. In this paper, we introduce a deep neural network for the Learning from Label Proportion (LLP) setting, in which the training data consist of bags of unlabeled instances with associated label distributions for each bag. We introduce a new regularization layer, Batch Averager, that can be appended to the last layer of any deep neural network to convert it from supervised learning to LLP. This layer can be implemented readily with existing deep learning packages. To further support domains in which the data consist of two conditionally independent feature views (e.g. image and text), we propose a co-training algorithm that iteratively generates pseudo bags and refits the deep LLP model to improve classification accuracy. We demonstrate our models on demographic attribute classification (gender and race/ethnicity), which has many applications in social media analysis, public health, and marketing. We conduct experiments to predict demographics of Twitter users based on their tweets and profile image, without requiring any user-level annotations for training. We find that the deep LLP approach outperforms baselines for both text and image features separately. Additionally, we find that co-training algorithm improves image and text classification by 4% and 8% absolute F1, respectively. Finally, an ensemble of text and image classifiers further improves the absolute F1 measure by 4% on average. 	1709.04108	Ehsan Mohammady Ardehaly, Aron Culotta	2017-09-12	10.1109/ICDMW.2017.144	2	3	Co-training for Demographic Classification Using Deep Learning from Label Proportions		cs.CV, cs.LG, stat.ML	https://arxiv.org/pdf/1709.04108.pdf
795988	 Recent work in distance metric learning has focused on learning transformations of data that best align with specified pairwise similarity and dissimilarity constraints, often supplied by a human observer. The learned transformations lead to improved retrieval, classification, and clustering algorithms due to the better adapted distance or similarity measures. Here, we address the problem of learning these transformations when the underlying constraint generation process is nonstationary. This nonstationarity can be due to changes in either the ground-truth clustering used to generate constraints or changes in the feature subspaces in which the class structure is apparent. We propose Online Convex Ensemble StrongLy Adaptive Dynamic Learning (OCELAD), a general adaptive, online approach for learning and tracking optimal metrics as they change over time that is highly robust to a variety of nonstationary behaviors in the changing metric. We apply the OCELAD framework to an ensemble of online learners. Specifically, we create a retro-initialized composite objective mirror descent (COMID) ensemble (RICE) consisting of a set of parallel COMID learners with different learning rates, and demonstrate parameter-free RICE-OCELAD metric learning on both synthetic data and a highly nonstationary Twitter dataset. We show significant performance improvements and increased robustness to nonstationary effects relative to previously proposed batch and online distance metric learning algorithms. 	1701.02804	Kristjan Greenewald, Stephen Kelley, Brandon Oselio, Alfred O. Hero	2017-01-06	10.1109/TSP.2017.2739100	4	2	Similarity Function Tracking using Pairwise Comparisons		stat.ML, cs.LG	https://arxiv.org/pdf/1701.02804.pdf
813933	 During natural or man-made disasters, humanitarian response organizations look for useful information to support their decision-making processes. Social media platforms such as Twitter have been considered as a vital source of useful information for disaster response and management. Despite advances in natural language processing techniques, processing short and informal Twitter messages is a challenging task. In this paper, we propose to use Deep Neural Network (DNN) to address two types of information needs of response organizations: 1) identifying informative tweets and 2) classifying them into topical classes. DNNs use distributed representation of words and learn the representation as well as higher level features automatically for the classification task. We propose a new online algorithm based on stochastic gradient descent to train DNNs in an online fashion during disaster situations. We test our models using a crisis-related real-world Twitter dataset. 	1610.01030	Dat Tien Nguyen, Shafiq Joty, Muhammad Imran, Hassan Sajjad, Prasenjit Mitra	2016-10-04		5	3	Applications of Online Deep Learning for Crisis Response Using Social Media Information	2016-10-05	cs.CL, cs.CY, cs.LG	https://arxiv.org/pdf/1610.01030.pdf
819797	 This paper covers the two approaches for sentiment analysis: i) lexicon based method; ii) machine learning method. We describe several techniques to implement these approaches and discuss how they can be adopted for sentiment classification of Twitter messages. We present a comparative study of different lexicon combinations and show that enhancing sentiment lexicons with emoticons, abbreviations and social-media slang expressions increases the accuracy of lexicon-based classification for Twitter. We discuss the importance of feature generation and feature selection processes for machine learning sentiment classification. To quantify the performance of the main sentiment analysis methods over Twitter we run these algorithms on a benchmark Twitter dataset from the SemEval-2013 competition, task 2-B. The results show that machine learning method based on SVM and Naive Bayes classifiers outperforms the lexicon method. We present a new ensemble method that uses a lexicon based sentiment score as input feature for the machine learning approach. The combined method proved to produce more precise classifications. We also show that employing a cost-sensitive classifier for highly unbalanced datasets yields an improvement of sentiment classification performance up to 7%. 	1507.00955	Olga Kolchyna, Tharsis T. P. Souza, Philip Treleaven, Tomaso Aste	2015-07-03		4	5	Twitter Sentiment Analysis: Lexicon Method, Machine Learning Method and Their Combination	2015-09-18	cs.CL, cs.IR, cs.LG, stat.ME, stat.ML	https://arxiv.org/pdf/1507.00955.pdf
918873	" Stance detection is the task of classifying the attitude expressed in a text towards a target such as Hillary Clinton to be ""positive"", negative"" or ""neutral"". Previous work has assumed that either the target is mentioned in the text or that training data for every target is given. This paper considers the more challenging version of this task, where targets are not always mentioned and no training data is available for the test targets. We experiment with conditional LSTM encoding, which builds a representation of the tweet that is dependent on the target, and demonstrate that it outperforms encoding the tweet and the target independently. Performance is improved further when the conditional model is augmented with bidirectional encoding. We evaluate our approach on the SemEval 2016 Task 6 Twitter Stance Detection corpus achieving performance second best only to a system trained on semi-automatically labelled tweets for the test target. When such weak supervision is added, our approach achieves state-of-the-art results. "	1606.05464	Isabelle Augenstein, Tim Rocktäschel, Andreas Vlachos, Kalina Bontcheva	2016-06-17		4	3	Stance Detection with Bidirectional Conditional Encoding	2016-09-26	cs.CL, cs.LG, cs.NE	https://arxiv.org/pdf/1606.05464.pdf
1020402	 Gang violence is a severe issue in major cities across the U.S. and recent studies [Patton et al. 2017] have found evidence of social media communications that can be linked to such violence in communities with high rates of exposure to gang activity. In this paper we partnered computer scientists with social work researchers, who have domain expertise in gang violence, to analyze how public tweets with images posted by youth who mention gang associations on Twitter can be leveraged to automatically detect psychosocial factors and conditions that could potentially assist social workers and violence outreach workers in prevention and early intervention programs. To this end, we developed a rigorous methodology for collecting and annotating tweets. We gathered 1,851 tweets and accompanying annotations related to visual concepts and the psychosocial codes: aggression, loss, and substance use. These codes are relevant to social work interventions, as they represent possible pathways to violence on social media. We compare various methods for classifying tweets into these three classes, using only the text of the tweet, only the image of the tweet, or both modalities as input to the classifier. In particular, we analyze the usefulness of mid-level visual concepts and the role of different modalities for this tweet classification task. Our experiments show that individually, text information dominates classification performance of the loss class, while image information dominates the aggression and substance use classes. Our multimodal approach provides a very promising improvement (18% relative in mean average precision) over the best single modality approach. Finally, we also illustrate the complexity of understanding social media data and elaborate on open challenges. 	1807.08465	Philipp Blandfort, Desmond Patton, William R. Frey, Svebor Karaman, Surabhi Bhargava, Fei-Tzin Lee, Siddharth Varia, Chris Kedzie, Michael B. Gaskell, Rossano Schifanella, Kathleen McKeown, Shih-Fu Chang	2018-07-23		12	3	Multimodal Social Media Analysis for Gang Violence Prevention		cs.LG, cs.CL, stat.ML	https://arxiv.org/pdf/1807.08465.pdf
1111301	 This paper formulates the problem of learning discriminative features (\textit{i.e.,} segments) from networked time series data considering the linked information among time series. For example, social network users are considered to be social sensors that continuously generate social signals (tweets) represented as a time series. The discriminative segments are often referred to as \emph{shapelets} in a time series. Extracting shapelets for time series classification has been widely studied. However, existing works on shapelet selection assume that the time series are independent and identically distributed (i.i.d.). This assumption restricts their applications to social networked time series analysis, since a user's actions can be correlated to his/her social affiliations. In this paper we propose a new Network Regularized Least Squares (NetRLS) feature selection model that combines typical time series data and user network data for analysis. Experiments on real-world networked time series Twitter and DBLP data demonstrate the performance of the proposed method. NetRLS performs better than LTS, the state-of-the-art time series feature selection approach, on real-world data. 	1612.06856	Haishuai Wang, Jia Wu, Peng Zhang, Chengqi Zhang	2016-12-20		4	1	Temporal Feature Selection on Networked Time Series	2016-12-22	cs.LG	https://arxiv.org/pdf/1612.06856.pdf
1113185	 Term weighting metrics assign weights to terms in order to discriminate the important terms from the less crucial ones. Due to this characteristic, these metrics have attracted growing attention in text classification and recently in sentiment analysis. Using the weights given by such metrics could lead to more accurate document representation which may improve the performance of the classification. While previous studies have focused on proposing or comparing different weighting metrics at two-classes document level sentiment analysis, this study propose to analyse the results given by each metric in order to find out the characteristics of good and bad weighting metrics. Therefore we present an empirical study of fifteen global supervised weighting metrics with four local weighting metrics adopted from information retrieval, we also give an analysis to understand the behavior of each metric by observing and analysing how each metric distributes the terms and deduce some characteristics which may distinguish the good and bad metrics. The evaluation has been done using Support Vector Machine on three different datasets: Twitter, restaurant and laptop reviews. 	1610.03106	Hussam Hamdan, Patrice Bellot, Frederic Bechet	2016-10-10		3	3	Supervised Term Weighting Metrics for Sentiment Analysis in Short Text		cs.CL, cs.IR, cs.LG	https://arxiv.org/pdf/1610.03106.pdf
1273654	 This paper describes our multi-view ensemble approach to SemEval-2017 Task 4 on Sentiment Analysis in Twitter, specifically, the Message Polarity Classification subtask for English (subtask A). Our system is a voting ensemble, where each base classifier is trained in a different feature space. The first space is a bag-of-words model and has a Linear SVM as base classifier. The second and third spaces are two different strategies of combining word embeddings to represent sentences and use a Linear SVM and a Logistic Regressor as base classifiers. The proposed system was ranked 18th out of 38 systems considering F1 score and 20th considering recall. 	1704.02263	Edilson A. Corrêa, Vanessa Queiroz Marinho, Leandro Borges dos Santos	2017-04-07		3	2	NILC-USP at SemEval-2017 Task 4: A Multi-view Ensemble for Twitter Sentiment Analysis		cs.CL, cs.LG	https://arxiv.org/pdf/1704.02263.pdf
1282864	" For classifying time series, a nearest-neighbor approach is widely used in practice with performance often competitive with or better than more elaborate methods such as neural networks, decision trees, and support vector machines. We develop theoretical justification for the effectiveness of nearest-neighbor-like classification of time series. Our guiding hypothesis is that in many applications, such as forecasting which topics will become trends on Twitter, there aren't actually that many prototypical time series to begin with, relative to the number of time series we have access to, e.g., topics become trends on Twitter only in a few distinct manners whereas we can collect massive amounts of Twitter data. To operationalize this hypothesis, we propose a latent source model for time series, which naturally leads to a ""weighted majority voting"" classification rule that can be approximated by a nearest-neighbor classifier. We establish nonasymptotic performance guarantees of both weighted majority voting and nearest-neighbor classification under our model accounting for how much of the time series we observe and the model complexity. Experimental results on synthetic data show weighted majority voting achieving the same misclassification rate as nearest-neighbor classification while observing less of the time series. We then use weighted majority to forecast which news topics on Twitter become trends, where we are able to detect such ""trending topics"" in advance of Twitter 79% of the time, with a mean early advantage of 1 hour and 26 minutes, a true positive rate of 95%, and a false positive rate of 4%. "	1302.3639	George H. Chen, Stanislav Nikolov, Devavrat Shah	2013-02-14		3	3	A Latent Source Model for Nonparametric Time Series Classification	2013-12-12	stat.ML, cs.LG, cs.SI	https://arxiv.org/pdf/1302.3639.pdf
1343853	 Over the past decade humans have experienced exponential growth in the use of online resources, in particular social media and microblogging websites such as Facebook, Twitter, YouTube and also mobile applications such as WhatsApp, Line, etc. Many companies have identified these resources as a rich mine of marketing knowledge. This knowledge provides valuable feedback which allows them to further develop the next generation of their product. In this paper, sentiment analysis of a product is performed by extracting tweets about that product and classifying the tweets showing it as positive and negative sentiment. The authors propose a hybrid approach which combines unsupervised learning in the form of K-means clustering to cluster the tweets and then performing supervised learning methods such as Decision Trees and Support Vector Machines for classification. 	1509.02437	Rishabh Soni, K. James Mathai	2015-09-08		2	4	Improved Twitter Sentiment Prediction through Cluster-then-Predict Model		cs.IR, cs.CL, cs.LG, cs.SI	https://arxiv.org/pdf/1509.02437.pdf
1362407	" This paper describes the participation of the team ""TwiSE"" in the SemEval 2016 challenge. Specifically, we participated in Task 4, namely ""Sentiment Analysis in Twitter"" for which we implemented sentiment classification systems for subtasks A, B, C and D. Our approach consists of two steps. In the first step, we generate and validate diverse feature sets for twitter sentiment evaluation, inspired by the work of participants of previous editions of such challenges. In the second step, we focus on the optimization of the evaluation measures of the different subtasks. To this end, we examine different learning strategies by validating them on the data provided by the task organisers. For our final submissions we used an ensemble learning approach (stacked generalization) for Subtask A and single linear models for the rest of the subtasks. In the official leaderboard we were ranked 9/35, 8/19, 1/11 and 2/14 for subtasks A, B, C and D respectively.\footnote{We make the code available for research purposes at \url{https://github.com/balikasg/SemEval2016-Twitter\_Sentiment\_Evaluation}.} "	1606.04351	Georgios Balikas, Massih-Reza Amini	2016-06-14		2	3	TwiSE at SemEval-2016 Task 4: Twitter Sentiment Classification		cs.CL, cs.IR, cs.LG	https://arxiv.org/pdf/1606.04351.pdf
1387302	 Change detection (CD) in time series data is a critical problem as it reveal changes in the underlying generative processes driving the time series. Despite having received significant attention, one important unexplored aspect is how to efficiently utilize additional correlated information to improve the detection and the understanding of changepoints. We propose hierarchical quickest change detection (HQCD), a framework that formalizes the process of incorporating additional correlated sources for early changepoint detection. The core ideas behind HQCD are rooted in the theory of quickest detection and HQCD can be regarded as its novel generalization to a hierarchical setting. The sources are classified into targets and surrogates, and HQCD leverages this structure to systematically assimilate observed data to update changepoint statistics across layers. The decision on actual changepoints are provided by minimizing the delay while still maintaining reliability bounds. In addition, HQCD also uncovers interesting relations between changes at targets from changes across surrogates. We validate HQCD for reliability and performance against several state-of-the-art methods for both synthetic dataset (known changepoints) and several real-life examples (unknown changepoints). Our experiments indicate that we gain significant robustness without loss of detection delay through HQCD. Our real-life experiments also showcase the usefulness of the hierarchical setting by connecting the surrogate sources (such as Twitter chatter) to target sources (such as Employment related protests that ultimately lead to major uprisings). 	1603.09739	Prithwish Chakraborty, Sathappan Muthiah, Ravi Tandon, Naren Ramakrishnan	2016-03-31		4	4	Hierarchical Quickest Change Detection via Surrogates		cs.LG, cs.IT, math.IT, stat.ML	https://arxiv.org/pdf/1603.09739.pdf
108209	 Social media services such as Twitter are a valuable source of information for decision support systems. Many studies have shown that this also holds for the medical domain, where Twitter is considered a viable tool for public health officials to sift through relevant information for the early detection, management, and control of epidemic outbreaks. This is possible due to the inherent capability of social media services to transmit information faster than traditional channels. However, the majority of current studies have limited their scope to the detection of common and seasonal health recurring events (e.g., Influenza-like Illness), partially due to the noisy nature of Twitter data, which makes outbreak detection and management very challenging. Within the European project M-Eco, we developed a Twitter-based Epidemic Intelligence (EI) system, which is designed to also handle a more general class of unexpected and aperiodic outbreaks. In particular, we faced three main research challenges in this endeavor: 1) dynamic classification to manage terminology evolution of Twitter messages, 2) alert generation to produce reliable outbreak alerts analyzing the (noisy) tweet time series, and 3) ranking and recommendation to support domain experts for better assessment of the generated alerts. In this paper, we empirically evaluate our proposed approach to these challenges using real-world outbreak datasets and a large collection of tweets. We validate our solution with domain experts, describe our experiences, and give a more realistic view on the benefits and issues of analyzing social media for public health. 	1611.03426	Avaré Stewart, Sara Romano, Nattiya Kanhabua, Sergio Di Martino, Wolf Siberski, Antonino Mazzeo, Wolfgang Nejdl, Ernesto Diaz-Aviles	2016-11-10		8	4	Why is it Difficult to Detect Sudden and Unexpected Epidemic Outbreaks in Twitter?		cs.CY, cs.IR, cs.SI, stat.ML	https://arxiv.org/pdf/1611.03426.pdf
144190	 In this paper we describe our attempt at producing a state-of-the-art Twitter sentiment classifier using Convolutional Neural Networks (CNNs) and Long Short Term Memory (LSTMs) networks. Our system leverages a large amount of unlabeled data to pre-train word embeddings. We then use a subset of the unlabeled data to fine tune the embeddings using distant supervision. The final CNNs and LSTMs are trained on the SemEval-2017 Twitter dataset where the embeddings are fined tuned again. To boost performances we ensemble several CNNs and LSTMs together. Our approach achieved first rank on all of the five English subtasks amongst 40 teams. 	1704.06125	Mathieu Cliche	2017-04-20		1	2	BB_twtr at SemEval-2017 Task 4: Twitter Sentiment Analysis with CNNs and LSTMs		cs.CL, stat.ML	https://arxiv.org/pdf/1704.06125.pdf
164854	 This paper describes the system developed at Amobee for the WASSA 2018 implicit emotions shared task (IEST). The goal of this task was to predict the emotion expressed by missing words in tweets without an explicit mention of those words. We developed an ensemble system consisting of language models together with LSTM-based networks containing a CNN attention mechanism. Our approach represents a novel use of language models (specifically trained on a large Twitter dataset) to predict and classify emotions. Our system reached 1st place with a macro $\text{F}_1$ score of 0.7145. 	1808.08782	Alon Rozental, Daniel Fleischer, Zohar Kelrich	2018-08-27		3	2	Amobee at IEST 2018: Transfer Learning from Language Models		cs.CL, stat.ML	https://arxiv.org/pdf/1808.08782.pdf
356590	 There is a growing demand for nonparametric conditional density estimators (CDEs) in fields such as astronomy and economics. In astronomy, for example, one can dramatically improve estimates of the parameters that dictate the evolution of the Universe by working with full conditional densities instead of regression (i.e., conditional mean) estimates. More generally, standard regression falls short in any prediction problem where the distribution of the response is more complex with multi-modality, asymmetry or heteroscedastic noise. Nevertheless, much of the work on high-dimensional inference concerns regression and classification only, whereas research on density estimation has lagged behind. Here we propose FlexCode, a fully nonparametric approach to conditional density estimation that reformulates CDE as a non-parametric orthogonal series problem where the expansion coefficients are estimated by regression. By taking such an approach, one can efficiently estimate conditional densities and not just expectations in high dimensions by drawing upon the success in high-dimensional regression. Depending on the choice of regression procedure, our method can adapt to a variety of challenging high-dimensional settings with different structures in the data (e.g., a large number of irrelevant components and nonlinear manifold structure) as well as different data types (e.g., functional data, mixed data types and sample sets). We study the theoretical and empirical performance of our proposed method, and we compare our approach with traditional conditional density estimators on simulated as well as real-world data, such as photometric galaxy data, Twitter data, and line-of-sight velocities in a galaxy cluster. 	1704.08095	Rafael Izbicki, Ann B. Lee	2017-04-26		2	2	Converting High-Dimensional Regression to High-Dimensional Conditional Density Estimation		stat.ME, stat.ML	https://arxiv.org/pdf/1704.08095.pdf
457789	 Social media has recently emerged as a premier method to disseminate information online. Through these online networks, tens of millions of individuals communicate their thoughts, personal experiences, and social ideals. We therefore explore the potential of social media to predict, even prior to onset, Major Depressive Disorder (MDD) in online personas. We employ a crowdsourced method to compile a list of Twitter users who profess to being diagnosed with depression. Using up to a year of prior social media postings, we utilize a Bag of Words approach to quantify each tweet. Lastly, we leverage several statistical classifiers to provide estimates to the risk of depression. Our work posits a new methodology for constructing our classifier by treating social as a text-classification problem, rather than a behavioral one on social media platforms. By using a corpus of 2.5M tweets, we achieved an 81% accuracy rate in classification, with a precision score of .86. We believe that this method may be helpful in developing tools that estimate the risk of an individual being depressed, can be employed by physicians, concerned individuals, and healthcare agencies to aid in diagnosis, even possibly enabling those suffering from depression to be more proactive about recovering from their mental health. 	1607.07384	Moin Nadeem	2016-07-25		1	2	Identifying Depression on Twitter		cs.SI, stat.ML	https://arxiv.org/pdf/1607.07384.pdf
741912	 While social networks can provide an ideal platform for up-to-date information from individuals across the world, it has also proved to be a place where rumours fester and accidental or deliberate misinformation often emerges. In this article, we aim to support the task of making sense from social media data, and specifically, seek to build an autonomous message-classifier that filters relevant and trustworthy information from Twitter. For our work, we collected about 100 million public tweets, including users' past tweets, from which we identified 72 rumours (41 true, 31 false). We considered over 80 trustworthiness measures including the authors' profile and past behaviour, the social network connections (graphs), and the content of tweets themselves. We ran modern machine-learning classifiers over those measures to produce trustworthiness scores at various time windows from the outbreak of the rumour. Such time-windows were key as they allowed useful insight into the progression of the rumours. From our findings, we identified that our model was significantly more accurate than similar studies in the literature. We also identified critical attributes of the data that give rise to the trustworthiness scores assigned. Finally we developed a software demonstration that provides a visual user interface to allow the user to examine the analysis. 	1611.06314	Georgios Giasemidis, Colin Singleton, Ioannis Agrafiotis, Jason R. C. Nurse, Alan Pilgrim, Chris Willis, Danica Vukadinovic Greetham	2016-11-19	10.1007/978-3-319-47880-7_12	7	2	Determining the Veracity of Rumours on Twitter		cs.SI, stat.ML	https://arxiv.org/pdf/1611.06314.pdf
1135917	 Big data trend has enforced the data-centric systems to have continuous fast data streams. In recent years, real-time analytics on stream data has formed into a new research field, which aims to answer queries about what-is-happening-now with a negligible delay. The real challenge with real-time stream data processing is that it is impossible to store instances of data, and therefore online analytical algorithms are utilized. To perform real-time analytics, pre-processing of data should be performed in a way that only a short summary of stream is stored in main memory. In addition, due to high speed of arrival, average processing time for each instance of data should be in such a way that incoming instances are not lost without being captured. Lastly, the learner needs to provide high analytical accuracy measures. Sentinel is a distributed system written in Java that aims to solve this challenge by enforcing both the processing and learning process to be done in distributed form. Sentinel is built on top of Apache Storm, a distributed computing platform. Sentinels learner, Vertical Hoeffding Tree, is a parallel decision tree-learning algorithm based on the VFDT, with ability of enabling parallel classification in distributed environments. Sentinel also uses SpaceSaving to keep a summary of the data stream and stores its summary in a synopsis data structure. Application of Sentinel on Twitter Public Stream API is shown and the results are discussed. 	1612.08543	Amir Hossein Akhavan Rahnama	2016-12-27	10.1109/CoDIT.2014.6996998	1	5	Distributed Real-Time Sentiment Analysis for Big Data Social Streams		stat.ML, cs.CL, cs.DB, cs.DC, cs.IR	https://arxiv.org/pdf/1612.08543.pdf
1375707	 This paper describes the Amobee sentiment analysis system, adapted to compete in SemEval 2017 task 4. The system consists of two parts: a supervised training of RNN models based on a Twitter sentiment treebank, and the use of feedforward NN, Naive Bayes and logistic regression classifiers to produce predictions for the different sub-tasks. The algorithm reached the 3rd place on the 5-label classification task (sub-task C). 	1705.01306	Alon Rozental, Daniel Fleischer	2017-05-03	10.18653/v1/S17-2108	2	2	Amobee at SemEval-2017 Task 4: Deep Learning System for Sentiment Detection on Twitter		cs.CL, stat.ML	https://arxiv.org/pdf/1705.01306.pdf
7455	 Social bots can affect online communication among humans. We study this phenomenon by focusing on #YaMeCanse, the most active protest hashtag in the history of Twitter in Mexico. Accounts using the hashtag are classified using the BotOrNot bot detection tool. Our preliminary analysis suggests that bots played a critical role in disrupting online communication about the protest movement. 	1609.08239	Pablo Suárez-Serrato, Margaret E. Roberts, Clayton A. Davis, Filippo Menczer	2016-09-26	10.1007/978-3-319-47874-6_19	4	2	On the influence of social bots in online protests. Preliminary findings of a Mexican case study		cs.CY, cs.SI	https://arxiv.org/pdf/1609.08239.pdf
11592	 Emoticons (e.g., :) and :( ) have been widely used in sentiment analysis and other NLP tasks as features to ma- chine learning algorithms or as entries of sentiment lexicons. In this paper, we argue that while emoticons are strong and common signals of sentiment expression on social media the relationship between emoticons and sentiment polarity are not always clear. Thus, any algorithm that deals with sentiment polarity should take emoticons into account but extreme cau- tion should be exercised in which emoticons to depend on. First, to demonstrate the prevalence of emoticons on social media, we analyzed the frequency of emoticons in a large re- cent Twitter data set. Then we carried out four analyses to examine the relationship between emoticons and sentiment polarity as well as the contexts in which emoticons are used. The first analysis surveyed a group of participants for their perceived sentiment polarity of the most frequent emoticons. The second analysis examined clustering of words and emoti- cons to better understand the meaning conveyed by the emoti- cons. The third analysis compared the sentiment polarity of microblog posts before and after emoticons were removed from the text. The last analysis tested the hypothesis that removing emoticons from text hurts sentiment classification by training two machine learning models with and without emoticons in the text respectively. The results confirms the arguments that: 1) a few emoticons are strong and reliable signals of sentiment polarity and one should take advantage of them in any senti- ment analysis; 2) a large group of the emoticons conveys com- plicated sentiment hence they should be treated with extreme caution. 	1511.02556	Hao Wang, Jorge A. Castanon	2015-11-08		2	2	Sentiment Expression via Emoticons on Social Media		cs.CL, cs.SI	https://arxiv.org/pdf/1511.02556.pdf
34642	" This work extends the set of works which deal with the popular problem of sentiment analysis in Twitter. It investigates the most popular document (""tweet"") representation methods which feed sentiment evaluation mechanisms. In particular, we study the bag-of-words, n-grams and n-gram graphs approaches and for each of them we evaluate the performance of a lexicon-based and 7 learning-based classification algorithms (namely SVM, Na\""ive Bayesian Networks, Logistic Regression, Multilayer Perceptrons, Best-First Trees, Functional Trees and C4.5) as well as their combinations, using a set of 4451 manually annotated tweets. The results demonstrate the superiority of learning-based methods and in particular of n-gram graphs approaches for predicting the sentiment of tweets. They also show that the combinatory approach has impressive effects on n-grams, raising the confidence up to 83.15% on the 5-Grams, using majority vote and a balanced dataset (equal number of positive, negative and neutral tweets for training). In the n-gram graph cases the improvement was small to none, reaching 94.52% on the 4-gram graphs, using Orthodromic distance and a threshold of 0.001. "	1505.02973	Evangelos Psomakelis, Konstantinos Tserpes, Dimosthenis Anagnostopoulos, Theodora Varvarigou	2015-05-12		4	3	Comparing methods for Twitter Sentiment Analysis		cs.CL, cs.IR, cs.SI	https://arxiv.org/pdf/1505.02973.pdf
36575	 Recent studies have shown strong correlation between social networking data and national influenza rates. We expanded upon this success to develop an automated text mining system that classifies Twitter messages in real time into six syndromic categories based on key terms from a public health ontology. 10-fold cross validation tests were used to compare Naive Bayes (NB) and Support Vector Machine (SVM) models on a corpus of 7431 Twitter messages. SVM performed better than NB on 4 out of 6 syndromes. The best performing classifiers showed moderately strong F1 scores: respiratory = 86.2 (NB); gastrointestinal = 85.4 (SVM polynomial kernel degree 2); neurological = 88.6 (SVM polynomial kernel degree 1); rash = 86.0 (SVM polynomial kernel degree 1); constitutional = 89.3 (SVM polynomial kernel degree 1); hemorrhagic = 89.9 (NB). The resulting classifiers were deployed together with an EARS C2 aberration detection algorithm in an experimental online system. 	1110.3094	Nigel Collier, Son Doan	2011-10-13		2	3	Syndromic classification of Twitter messages		cs.CL, cs.IR, cs.SI	https://arxiv.org/pdf/1110.3094.pdf
51062	 This paper introduces LABurst, a general technique for identifying key moments, or moments of high impact, in social media streams without the need for domain-specific information or seed keywords. We leverage machine learning to model temporal patterns around bursts in Twitter's unfiltered public sample stream and build a classifier to identify tokens experiencing these bursts. We show LABurst performs competitively with existing burst detection techniques while simultaneously providing insight into and detection of unanticipated moments. To demonstrate our approach's potential, we compare two baseline event-detection algorithms with our language-agnostic algorithm to detect key moments across three major sporting competitions: 2013 World Series, 2014 Super Bowl, and 2014 World Cup. Our results show LABurst outperforms a time series analysis baseline and is competitive with a domain-specific baseline even though we operate without any domain knowledge. We then go further by transferring LABurst's models learned in the sports domain to the task of identifying earthquakes in Japan and show our method detects large spikes in earthquake-related tokens within two minutes of the actual event. 	1508.00488	Cody Buntain, Jimmy Lin, Jennifer Golbeck	2015-08-03		3	1	Learning to Discover Key Moments in Social Media Streams		cs.SI	https://arxiv.org/pdf/1508.00488.pdf
82054	 Predicting investors reactions to financial and political news is important for the early detection of stock market jitters. Evidence from several recent studies suggests that online social media could improve prediction of stock market movements. However, utilizing such information to predict strong stock market fluctuations has not been explored so far. In this work, we propose a novel event detection method on Twitter, tailored to detect financial and political events that influence a specific stock market. The proposed approach applies a bursty topic detection method on a stream of tweets related to finance or politics followed by a classification process which filters-out events that do not influence the examined stock market. We train our classifier to recognise real events by using solely information about stock market volatility, without the need of manual labeling. We model Twitter events as feature vectors that encompass a rich variety of information, such as the geographical distribution of tweets, their polarity, information about their authors as well as information about bursty words associated with the event. We show that utilizing only information about tweets polarity, like most previous studies, results in wasting important information. We apply the proposed method on high-frequency intra-day data from the Greek and Spanish stock market and we show that our financial event detector successfully predicts most of the stock market jitters. 	1709.06519	Fani Tsapeli, Nikolaos Bezirgiannidis, Peter Tino, Mirco Musolesi	2017-06-19		4	1	Linking Twitter Events With Stock Market Jitters		cs.SI	https://arxiv.org/pdf/1709.06519.pdf
124635	 Social media tend to be rife with rumours while new reports are released piecemeal during breaking news. Interestingly, one can mine multiple reactions expressed by social media users in those situations, exploring their stance towards rumours, ultimately enabling the flagging of highly disputed rumours as being potentially false. In this work, we set out to develop an automated, supervised classifier that uses multi-task learning to classify the stance expressed in each individual tweet in a rumourous conversation as either supporting, denying or questioning the rumour. Using a classifier based on Gaussian Processes, and exploring its effectiveness on two datasets with very different characteristics and varying distributions of stances, we show that our approach consistently outperforms competitive baseline classifiers. Our classifier is especially effective in estimating the distribution of different types of stance associated with a given rumour, which we set forth as a desired characteristic for a rumour-tracking system that will warn both ordinary users of Twitter and professional news practitioners when a rumour is being rebutted. 	1609.01962	Michal Lukasik, Kalina Bontcheva, Trevor Cohn, Arkaitz Zubiaga, Maria Liakata, Rob Procter	2016-09-07		6	3	Using Gaussian Processes for Rumour Stance Classification in Social Media		cs.CL, cs.IR, cs.SI	https://arxiv.org/pdf/1609.01962.pdf
127453	 We explore the feasibility of automatically finding accounts that publish sensitive content on Twitter. One natural approach to this problem is to first create a list of sensitive keywords, and then identify Twitter accounts that use these words in their tweets. But such an approach may overlook sensitive accounts that are not covered by the subjective choice of keywords. In this paper, we instead explore finding sensitive accounts by examining the percentage of anonymous and identifiable followers the accounts have. This approach is motivated by an earlier study showing that sensitive accounts typically have a large percentage of anonymous followers and a small percentage of identifiable followers. To this end, we first considered the problem of automatically determining if a Twitter account is anonymous or identifiable. We find that simple techniques, such as checking for name-list membership, perform poorly. We designed a machine learning classifier that classifies accounts as anonymous or identifiable. We then classified an account as sensitive based on the percentages of anonymous and identifiable followers the account has. We applied our approach to approximately 100,000 accounts with 404 million active followers. The approach uncovered accounts that were sensitive for a diverse number of reasons. These accounts span across varied themes, including those that are not commonly proposed as sensitive or those that relate to socially stigmatized topics. To validate our approach, we applied Latent Dirichlet Allocation (LDA) topic analysis to the tweets in the detected sensitive and non-sensitive accounts. LDA showed that the sensitive and non-sensitive accounts obtained from the methodology are tweeting about distinctly different topics. Our results show that it is indeed possible to objectively identify sensitive accounts at the scale of Twitter. 	1702.00164	Sai Teja Peddinti, Keith W. Ross, Justin Cappos	2017-02-01		3	2	Mining Anonymity: Identifying Sensitive Accounts on Twitter		cs.SI, cs.CR	https://arxiv.org/pdf/1702.00164.pdf
133613	 We describe a strategy for the acquisition of training data necessary to build a social-media-driven early detection system for individuals at risk for (preventable) type 2 diabetes mellitus (T2DM). The strategy uses a game-like quiz with data and questions acquired semi-automatically from Twitter. The questions are designed to inspire participant engagement and collect relevant data to train a public-health model applied to individuals. Prior systems designed to use social media such as Twitter to predict obesity (a risk factor for T2DM) operate on entire communities such as states, counties, or cities, based on statistics gathered by government agencies. Because there is considerable variation among individuals within these groups, training data on the individual level would be more effective, but this data is difficult to acquire. The approach proposed here aims to address this issue. Our strategy has two steps. First, we trained a random forest classifier on data gathered from (public) Twitter statuses and state-level statistics with state-of-the-art accuracy. We then converted this classifier into a 20-questions-style quiz and made it available online. In doing so, we achieved high engagement with individuals that took the quiz, while also building a training set of voluntarily supplied individual-level data for future classification. 	1603.03784	Dane Bell, Daniel Fried, Luwen Huangfu, Mihai Surdeanu, Stephen Kobourov	2016-03-11		5	3	Towards using social media to identify individuals at risk for preventable chronic illness		cs.CL, cs.CY, cs.SI	https://arxiv.org/pdf/1603.03784.pdf
146929	 Cascades represent an important phenomenon across various disciplines such as sociology, economy, psychology, political science, marketing, and epidemiology. An important property of cascades is their morphology, which encompasses the structure, shape, and size. However, cascade morphology has not been rigorously characterized and modeled in prior literature. In this paper, we propose a Multi-order Markov Model for the Morphology of Cascades ($M^4C$) that can represent and quantitatively characterize the morphology of cascades with arbitrary structures, shapes, and sizes. $M^4C$ can be used in a variety of applications to classify different types of cascades. To demonstrate this, we apply it to an unexplored but important problem in online social networks -- cascade size prediction. Our evaluations using real-world Twitter data show that $M^4C$ based cascade size prediction scheme outperforms the baseline scheme based on cascade graph features such as edge growth rate, degree distribution, clustering, and diameter. $M^4C$ based cascade size prediction scheme consistently achieves more than 90% classification accuracy under different experimental scenarios. 	1302.2376	M. Zubair Shafiq, Alex X. Liu	2013-02-10		2	2	Modeling Morphology of Social Network Cascades		cs.SI, physics.soc-ph	https://arxiv.org/pdf/1302.2376.pdf
148108	 SinaWeibo is a Twitter-like social network service emerging in China in recent years. People can post weibos (microblogs) and communicate with others on it. Based on a dataset of 650 million weibos from August 2009 to January 2012 crawled from APIs of SinaWeibo, we study the hot ones that have been reposted for at least 1000 times. We find that hot weibos can be roughly classified into eight categories, i.e. Entertainment & Fashion, Hot Social Events, Leisure & Mood, Life & Health, Seeking for Help, Sales Promotion, Fengshui & Fortune and Deleted Weibos. In particular, Leisure & Mood and Hot Social Events account for almost 65% of all the hot weibos. This reflects very well the fundamental dual-structure of the current society of China: On the one hand, economy has made a great progress and quite a part of people are now living a relatively prosperous and fairly easy life. On the other hand, there still exist quite a lot of serious social problems, such as government corruptions and environmental pollutions. It is also shown that users' posting and reposting behaviors are greatly affected by their identity factors (gender, verification status, and regional location). For instance, (1) Two thirds of the hot weibos are created by male users. (2) Although verified users account for only 0.1% in SinaWeibo, 46.5% of the hot weibos are contributed by them. Very interestingly, 39.2% are written by SPA users. A more or less pathetic fact is that only 14.4% of the hot weibos are created by grassroots (individual users that are neither SPA nor verified). (3) Users from different areas of China have distinct posting and reposting behaviors which usually reflect very their local cultures. Homophily is also examined for people's reposting behaviors. 	1304.4682	Yuan Li, Haoyu Gao, Mingmin Yang, Wanqiu Guan, Haixin Ma, Weining Qian, Zhigang Cao, Xiaoguang Yang	2013-04-17		8	3	What are Chinese Talking about in Hot Weibos?	2013-05-10	cs.SI, cs.CY, physics.soc-ph	https://arxiv.org/pdf/1304.4682.pdf
148217	 The impact of social media and its growing association with the sharing of ideas and propagation of messages remains vital in everyday communication. Twitter is one effective platform for the dissemination of news and stories about recent events happening around the world. It has a continually growing database currently adopted by over 300 million users. In this paper we propose a novel grid-based approach employing supervised Multinomial Naive Bayes while extracting geographic entities from relevant user descriptions metadata which gives a spatial indication of the user location. To the best of our knowledge our approach is the first to make location inference from tweets using geo-enriched grid-based classification. Our approach performs better than existing baselines achieving more than 57% accuracy at city-level granularity. In addition we present a novel framework for content-based estimation of user locations by specifying levels of granularity required in pre-defined location grids. 	1701.03855	Oluwaseun Ajao, Deepak P, Jun Hong	2017-01-13		3	2	Location Inference from Tweets using Grid-based Classification		cs.IR, cs.SI	https://arxiv.org/pdf/1701.03855.pdf
154871	 As microblogging services like Twitter are becoming more and more influential in today's globalised world, its facets like sentiment analysis are being extensively studied. We are no longer constrained by our own opinion. Others opinions and sentiments play a huge role in shaping our perspective. In this paper, we build on previous works on Twitter sentiment analysis using Distant Supervision. The existing approach requires huge computation resource for analysing large number of tweets. In this paper, we propose techniques to speed up the computation process for sentiment analysis. We use tweet subjectivity to select the right training samples. We also introduce the concept of EFWS (Effective Word Score) of a tweet that is derived from polarity scores of frequently used words, which is an additional heuristic that can be used to speed up the sentiment classification with standard machine learning algorithms. We performed our experiments using 1.6 million tweets. Experimental evaluations show that our proposed technique is more efficient and has higher accuracy compared to previously proposed methods. We achieve overall accuracies of around 80% (EFWS heuristic gives an accuracy around 85%) on a training dataset of 100K tweets, which is half the size of the dataset used for the baseline model. The accuracy of our proposed model is 2-3% higher than the baseline model, and the model effectively trains at twice the speed of the baseline model. 	1701.03051	Tapan Sahni, Chinmay Chandak, Naveen Reddy Chedeti, Manish Singh	2017-01-11		4	3	Efficient Twitter Sentiment Classification using Subjective Distant Supervision		cs.SI, cs.CL, cs.IR	https://arxiv.org/pdf/1701.03051.pdf
172470	 Microblogging websites, especially Twitter have become an important means of communication, in today's time. Often these services have been found to be faster than conventional news services. With millions of users, a need was felt to classify users based on ambient metadata associated with their user accounts. We particularly look at the effectiveness of the profile description field in order to carry out the task of user classification. Our results show that such metadata can be an effective feature for any classification task. 	1407.8499	Chirag Nagpal, Khushboo Singhal	2014-07-31		2	2	Twitter User Classification using Ambient Metadata		cs.SI, cs.IR	https://arxiv.org/pdf/1407.8499.pdf
177415	" Background: Twitter has become the ""wild-west"" of marketing and promotional strategies for advertisement agencies. Electronic cigarettes have been heavily marketed across Twitter feeds, offering discounts, ""kid-friendly"" flavors, algorithmically generated false testimonials, and free samples. Methods:All electronic cigarette keyword related tweets from a 10% sample of Twitter spanning January 2012 through December 2014 (approximately 850,000 total tweets) were identified and categorized as Automated or Organic by combining a keyword classification and a machine trained Human Detection algorithm. A sentiment analysis using Hedonometrics was performed on Organic tweets to quantify the change in consumer sentiments over time. Commercialized tweets were topically categorized with key phrasal pattern matching. Results:The overwhelming majority (80%) of tweets were classified as automated or promotional in nature. The majority of these tweets were coded as commercialized (83.65% in 2013), up to 33% of which offered discounts or free samples and appeared on over a billion twitter feeds as impressions. The positivity of Organic (human) classified tweets has decreased over time (5.84 in 2013 to 5.77 in 2014) due to a relative increase in the negative words ban,tobacco,doesn't,drug,against,poison,tax and a relative decrease in the positive words like haha,good,cool. Automated tweets are more positive than organic (6.17 versus 5.84) due to a relative increase in the marketing words best,win,buy,sale,health,discount and a relative decrease in negative words like bad, hate, stupid, don't. Conclusions:Due to the youth presence on Twitter and the clinical uncertainty of the long term health complications of electronic cigarette consumption, the protection of public health warrants scrutiny and potential regulation of social media marketing. "	1508.01843	Eric M. Clark, Chris A. Jones, Jake Ryland Williams, Allison N. Kurti, Michell Craig Nortotsky, Christopher M. Danforth, Peter Sheridan Dodds	2015-08-07		7	1	Vaporous Marketing: Uncovering Pervasive Electronic Cigarette Advertisements on Twitter	2016-03-05	cs.SI	https://arxiv.org/pdf/1508.01843.pdf
245103	 Microblogs are increasingly exploited for predicting prices and traded volumes of stocks in financial markets. However, it has been demonstrated that much of the content shared in microblogging platforms is created and publicized by bots and spammers. Yet, the presence (or lack thereof) and the impact of fake stock microblogs has never systematically been investigated before. Here, we study 9M tweets related to stocks of the 5 main financial markets in the US. By comparing tweets with financial data from Google Finance, we highlight important characteristics of Twitter stock microblogs. More importantly, we uncover a malicious practice - referred to as cashtag piggybacking - perpetrated by coordinated groups of bots and likely aimed at promoting low-value stocks by exploiting the popularity of high-value ones. Among the findings of our study is that as much as 71% of the authors of suspicious financial tweets are classified as bots by a state-of-the-art spambot detection algorithm. Furthermore, 37% of them were suspended by Twitter a few months after our investigation. Our results call for the adoption of spam and bot detection techniques in all studies and applications that exploit user-generated content for predicting the stock market. 	1804.04406	Stefano Cresci, Fabrizio Lillo, Daniele Regoli, Serena Tardelli, Maurizio Tesconi	2018-04-12		5	2	Cashtag piggybacking: uncovering spam and bot activity in stock microblogs on Twitter	2018-07-17	cs.SI, cs.CR	https://arxiv.org/pdf/1804.04406.pdf
265692	 Most previous work related to tweet classification have focused on identifying a given tweet as a spam, or to classify a Twitter user account as a spammer or a bot. In most cases the tweet classification has taken place offline, on a pre-collected dataset of tweets. In this paper we present an \emph{on-the-fly} approach to classify each newly downloaded tweet as \emph{autogenerated} or not. We define an autogenerated tweet (AGT) as a tweet where all or parts of the natural language content is generated automatically by a bot or other type of program. Our on-the-fly approach makes use of two classifiers. The first classifies a tweet solely based on the twitter text and the tweet metadata that comes with every tweet. It is used for tweets posted by unknown users with no available tweet history. An unknown user also triggers a batch job to start downloading the missing user timeline information. The second classifier is used for tweets posted by a user where the user timeline is downloaded and available. Initially, it will be the first classifier that handles most of the tweets. This will gradually change and after an initialization phase where we download historic data for the most active users, we reach a state where the second classifier handles a vast majority of all the tweets. A simulation using our on-the-fly detection mechanism indicates that we can handle Twitter streams with up to 68,000 unique users each day. The bottleneck is the time required to download new user timelines. The AGT detection is very accurate. In a set of 5,000 tweets we correctly classified about 98\% of all AGTs using a subject-wise cross-validation. 	1802.01197	Jonas Lundberg, Jonas Nordqvist, Antonio Matosevic	2018-02-04		3	1	On-the-fly Detection of Autogenerated Tweets		cs.SI	https://arxiv.org/pdf/1802.01197.pdf
294866	 Twitter as a micro-blogging platform rose to instant fame mainly due to its minimalist features that allow seamless communication between users. As the conversations grew thick and faster, a placeholder feature called as Hashtags became important as it captured the themes behind the tweets. Prior studies have investigated the conversation dynamics, inter-play with other media platforms and communication patterns between users for specific event-based hashtags such as the #Occupy movement. Commonplace hashtags which are used on a daily basis have been largely ignored due to their seemingly innocuous presence in tweets and also due to the lack of connection with real-world events. However, it can be postulated that utility of these hashtags is the main reason behind their continued usage. This study is aimed at understanding the rationale behind the usage of a particular type of commonplace hashtags - location hashtags such as country and city name hashtags. Tweets with the hashtag #singapore were extracted for one week duration. Manual and automatic tweet classification was performed along with social network analysis, to identify the underlying themes. Seven themes were identified. Findings indicate that the hashtag is prominent in tweets about local events, local news, users current location and landmark related information sharing. Users who share content from social media sites such as Instagram make use of the hashtag in a more prominent way when compared to users who post textual content. News agencies, commercial bodies and celebrities make use of the hashtag more than common individuals. Overall, the results show the non-conversational nature of the hashtag. The findings are to be validated with other country names and cross-validated with hashtag data from other social media platforms. 	1601.04135	Aravind Sesagiri Raamkumar	2016-01-16		1	1	Whats in a Country Name - Twitter Hashtag Analysis of #singapore		cs.SI	https://arxiv.org/pdf/1601.04135.pdf
307618	 Many works related to Twitter aim at characterizing its users in some way: role on the service (spammers, bots, organizations, etc.), nature of the user (socio-professional category, age, etc.), topics of interest , and others. However, for a given user classification problem, it is very difficult to select a set of appropriate features, because the many features described in the literature are very heterogeneous, with name overlaps and collisions, and numerous very close variants. In this article, we review a wide range of such features. In order to present a clear state-of-the-art description, we unify their names, definitions and relationships, and we propose a new, neutral, typology. We then illustrate the interest of our review by applying a selection of these features to the offline influence detection problem. This task consists in identifying users which are influential in real-life, based on their Twitter account and related data. We show that most features deemed efficient to predict online influence, such as the numbers of retweets and followers, are not relevant to this problem. However, We propose several content-based approaches to label Twitter users as Influencers or not. We also rank them according to a predicted influence level. Our proposals are evaluated over the CLEF RepLab 2014 dataset, and outmatch state-of-the-art methods. 	1509.06585	Jean-Valère Cossu, Vincent Labatut, Nicolas Dugué	2015-09-22	10.1007/s13278-016-0329-x	3	2	A Review of Features for the Discrimination of Twitter Users: Application to the Prediction of Offline Influence	2016-07-29	cs.CL, cs.SI	https://arxiv.org/pdf/1509.06585.pdf
310283	 This paper employs deep learning in detecting the traffic accident from social media data. First, we thoroughly investigate the 1-year over 3 million tweet contents in two metropolitan areas: Northern Virginia and New York City. Our results show that paired tokens can capture the association rules inherent in the accident-related tweets and further increase the accuracy of the traffic accident detection. Second, two deep learning methods: Deep Belief Network (DBN) and Long Short-Term Memory (LSTM) are investigated and implemented on the extracted token. Results show that DBN can obtain an overall accuracy of 85% with about 44 individual token features and 17 paired token features. The classification results from DBN outperform those of Support Vector Machines (SVMs) and supervised Latent Dirichlet allocation (sLDA). Finally, to validate this study, we compare the accident-related tweets with both the traffic accident log on freeways and traffic data on local roads from 15,000 loop detectors. It is found that nearly 66% of the accident-related tweets can be located by the accident log and more than 80% of them can be tied to nearby abnormal traffic data. Several important issues of using Twitter to detect traffic accidents have been brought up by the comparison including the location and time bias, as well as the characteristics of influential users and hashtags. 	1801.01528	Zhenhua Zhang, Qing Heb, Jing Gao, Ming Ni	2018-01-04		4	2	A deep learning approach for detecting traffic accidents from social media data		cs.SI, stat.OT	https://arxiv.org/pdf/1801.01528.pdf
322331	 When evaluating the cause of one's popularity on Twitter, one thing is considered to be the main driver: Many tweets. There is debate about the kind of tweet one should publish, but little beyond tweets. Of particular interest is the information provided by each Twitter user's profile page. One of the features are the given names on those profiles. Studies on psychology and economics identified correlations of the first name to, e.g., one's school marks or chances of getting a job interview in the US. Therefore, we are interested in the influence of those profile information on the follower count. We addressed this question by analyzing the profiles of about 6 Million Twitter users. All profiles are separated into three groups: Users that have a first name, English words, or neither of both in their name field. The assumption is that names and words influence the discoverability of a user and subsequently his/her follower count. We propose a classifier that labels users who will increase their follower count within a month by applying different models based on the user's group. The classifiers are evaluated with the area under the receiver operator curve score and achieves a score above 0.800. 	1705.03214	Juergen Mueller, Gerd Stumme	2017-05-09	10.1145/3091478.3091490	2	2	Predicting Rising Follower Counts on Twitter Using Profile Information		cs.IR, cs.SI	https://arxiv.org/pdf/1705.03214.pdf
324889	 Opinion mining and demographic attribute inference have many applications in social science. In this paper, we propose models to infer daily joint probabilities of multiple latent attributes from Twitter data, such as political sentiment and demographic attributes. Since it is costly and time-consuming to annotate data for traditional supervised classification, we instead propose scalable Learning from Label Proportions (LLP) models for demographic and opinion inference using U.S. Census, national and state political polls, and Cook partisan voting index as population level data. In LLP classification settings, the training data is divided into a set of unlabeled bags, where only the label distribution in of each bag is known, removing the requirement of instance-level annotations. Our proposed LLP model, Weighted Label Regularization (WLR), provides a scalable generalization of prior work on label regularization to support weights for samples inside bags, which is applicable in this setting where bags are arranged hierarchically (e.g., county-level bags are nested inside of state-level bags). We apply our model to Twitter data collected in the year leading up to the 2016 U.S. presidential election, producing estimates of the relationships among political sentiment and demographics over time and place. We find that our approach closely tracks traditional polling data stratified by demographic category, resulting in error reductions of 28-44% over baseline approaches. We also provide descriptive evaluations showing how the model may be used to estimate interactions among many variables and to identify linguistic temporal variation, capabilities which are typically not feasible using traditional polling methods. 	1708.08000	Ehsan Mohammady Ardehaly, Aron Culotta	2017-08-26	10.1109/ICDM.2017.84	2	1	Mining the Demographics of Political Sentiment from Twitter Using Learning from Label Proportions		cs.SI	https://arxiv.org/pdf/1708.08000.pdf
350523	 The problem of ideology detection is to study the latent (political) placement for people, which is traditionally studied on politicians according to their voting behaviors. Recently, more and more studies begin to address the ideology detection problem for ordinary users based on their online behaviors that can be captured by social media, e.g., Twitter. As far as we are concerned, however, the vast majority of the existing methods on ideology detection on social media have oversimplified the problem as a binary classification problem (i.e., liberal vs. conservative). Moreover, though social links can play a critical role in deciding one's ideology, most of the existing work ignores the heterogeneous types of links in social media. In this paper we propose to detect \emph{numerical} ideology positions for Twitter users, according to their \emph{follow}, \emph{mention}, and \emph{retweet} links to a selected set of politicians. A unified probabilistic model is proposed that can (1) explain the reasons why links are built among people in terms of their ideology, (2) integrate heterogeneous types of links together in determining people's ideology, and (3) automatically learn the quality of each type of links in deciding one's ideology. Experiments have demonstrated the advantages of our model in terms of both ranking and political leaning classification accuracy. It is shown that (1) using multiple types of links is better than using any single type of links alone to determine one's ideology, and (2) our model is even more superior than baselines when dealing with people that are sparsely linked in one type of links. We also show that the detected ideology for Twitter users aligns with our intuition quite well. 	1612.08207	Yupeng Gu, Ting Chen, Yizhou Sun, Bingyu Wang	2016-12-24		4	1	Ideology Detection for Twitter Users with Heterogeneous Types of Links		cs.SI	https://arxiv.org/pdf/1612.08207.pdf
381274	 We investigate the influence of fake and traditional, fact-based, news outlets on Twitter during the 2016 US presidential election. Using a comprehensive dataset of 171 million tweets covering the five months preceding election day, we identify 30 million tweets, sent by 2.2 million users, which are classified as spreading fake and extremely biased news, based on a list of news outlets curated from independent fact-checking organizations, and traditional news from right to left. We find that 29% of these tweets disseminate fake or extremely biased news. We fully characterize the networks of users spreading fake and traditional news and find the most influential users. Contrary to traditional news, where influencers are mainly journalists or news outlets with verified Twitter accounts, e.g. @FoxNews and @CNN, the majority of fake news influencers have unverified or deleted accounts. In particular, accounts with seemingly deceiving profiles are found among the top fake and extremely biased influencers. We find that the three top influencers spreading (i.e. re-tweeting) fake news websites are @PrisonPlanet, @RealAlexJones and @zerohedge and re-tweeting extremely bias news websites are @realDonaldTrump, @DailyCaller and @BreitbartNews. To understand how fake news influenced Twitter opinion during the presidential election, we perform a Granger-causality test between the time series of activity of influencers and the supporters of each presidential candidate: Trump and Clinton. Two different news spreading mechanisms are revealed: (i) The influencers spreading traditional center and left leaning news largely determine (Granger-cause) the opinion of the Clinton supporters. (ii) Remarkably, this causality is reversed for the fake news: the opinion of Trump supporters largely Granger-causes the dynamics of influencers spreading fake and extremely biased news. 	1803.08491	Alexandre Bovet, Hernan A. Makse	2018-03-22		2	3	Influence of fake news in Twitter during the 2016 US presidential election		cs.SI, cs.CY, physics.soc-ph	https://arxiv.org/pdf/1803.08491.pdf
385021	 The popularity of social media platforms such as Twitter has led to the proliferation of automated bots, creating both opportunities and challenges in information dissemination, user engagements, and quality of services. Past works on profiling bots had been focused largely on malicious bots, with the assumption that these bots should be removed. In this work, however, we find many bots that are benign, and propose a new, broader categorization of bots based on their behaviors. This includes broadcast, consumption, and spam bots. To facilitate comprehensive analyses of bots and how they compare to human accounts, we develop a systematic profiling framework that includes a rich set of features and classifier bank. We conduct extensive experiments to evaluate the performances of different classifiers under varying time windows, identify the key features of bots, and infer about bots in a larger Twitter population. Our analysis encompasses more than 159K bot and human (non-bot) accounts in Twitter. The results provide interesting insights on the behavioral traits of both benign and malicious bots. 	1609.00543	Richard Jayadi Oentaryo, Arinto Murdopo, Philips Kokoh Prasetyo, Ee-Peng Lim	2016-09-02	10.1007/978-3-319-47880-7_6	4	2	On Profiling Bots in Social Media		cs.SI, cs.IR	https://arxiv.org/pdf/1609.00543.pdf
387915	" An important challenge in the process of tracking and detecting the dissemination of misinformation is to understand the political gap between people that engage with the so called ""fake news"". A possible factor responsible for this gap is opinion polarization, which may prompt the general public to classify content that they disagree or want to discredit as fake. In this work, we study the relationship between political polarization and content reported by Twitter users as related to ""fake news"". We investigate how polarization may create distinct narratives on what misinformation actually is. We perform our study based on two datasets collected from Twitter. The first dataset contains tweets about US politics in general, from which we compute the degree of polarization of each user towards the Republican and Democratic Party. In the second dataset, we collect tweets and URLs that co-occurred with ""fake news"" related keywords and hashtags, such as #FakeNews and #AlternativeFact, as well as reactions towards such tweets and URLs. We then analyze the relationship between polarization and what is perceived as misinformation, and whether users are designating information that they disagree as fake. Our results show an increase in the polarization of users and URLs associated with fake-news keywords and hashtags, when compared to information not labeled as ""fake news"". We discuss the impact of our findings on the challenges of tracking ""fake news"" in the ongoing battle against misinformation. "	1706.05924	Manoel Horta Ribeiro, Pedro H. Calais, Virgílio A. F. Almeida, Wagner Meira	2017-06-19		4	2	"""Everything I Disagree With is #FakeNews"": Correlating Political Polarization and Spread of Misinformation"	2017-07-17	cs.SI, cs.CY	https://arxiv.org/pdf/1706.05924.pdf
387939	 Sybil attacks are becoming increasingly widespread and pose a significant threat to online social systems; a single adversary can inject multiple colluding identities in the system to compromise security and privacy. Recent works have leveraged social network-based trust relationships to defend against Sybil attacks. However, existing defenses are based on oversimplified assumptions about network structure, which do not necessarily hold in real-world social networks. Recognizing these limitations, we propose SybilFuse, a defense-in-depth framework for Sybil detection when the oversimplified assumptions are relaxed. SybilFuse adopts a collective classification approach by first training local classifiers to compute local trust scores for nodes and edges, and then propagating the local scores through the global network structure via weighted random walk and loopy belief propagation mechanisms. We evaluate our framework on both synthetic and real-world network topologies, including a large-scale, labeled Twitter network comprising 20M nodes and 265M edges, and demonstrate that SybilFuse outperforms state-of-the-art approaches significantly. In particular, SybilFuse achieves 98% of Sybil coverage among top-ranked nodes. 	1803.06772	Peng Gao, Binghui Wang, Neil Zhenqiang Gong, Sanjeev R. Kulkarni, Kurt Thomas, Prateek Mittal	2018-03-18		6	2	SybilFuse: Combining Local Attributes with Global Structure to Perform Robust Sybil Detection	2018-06-06	cs.CR, cs.SI	https://arxiv.org/pdf/1803.06772.pdf
388610	 Social media expose millions of users every day to information campaigns --- some emerging organically from grassroots activity, others sustained by advertising or other coordinated efforts. These campaigns contribute to the shaping of collective opinions. While most information campaigns are benign, some may be deployed for nefarious purposes. It is therefore important to be able to detect whether a meme is being artificially promoted at the very moment it becomes wildly popular. This problem has important social implications and poses numerous technical challenges. As a first step, here we focus on discriminating between trending memes that are either organic or promoted by means of advertisement. The classification is not trivial: ads cause bursts of attention that can be easily mistaken for those of organic trends. We designed a machine learning framework to classify memes that have been labeled as trending on Twitter.After trending, we can rely on a large volume of activity data. Early detection, occurring immediately at trending time, is a more challenging problem due to the minimal volume of activity data that is available prior to trending.Our supervised learning framework exploits hundreds of time-varying features to capture changing network and diffusion patterns, content and sentiment information, timing signals, and user meta-data. We explore different methods for encoding feature time series. Using millions of tweets containing trending hashtags, we achieve 75% AUC score for early detection, increasing to above 95% after trending. We evaluate the robustness of the algorithms by introducing random temporal shifts on the trend time series. Feature selection analysis reveals that content cues provide consistently useful signals; user features are more informative for early detection, while network and timing features are more helpful once more data is available. 	1703.07518	Onur Varol, Emilio Ferrara, Filippo Menczer, Alessandro Flammini	2017-03-22		4	1	Early Detection of Promoted Campaigns on Social Media		cs.SI	https://arxiv.org/pdf/1703.07518.pdf
389822	" Within a fairly short amount of time, the Islamic State of Iraq and Syria (ISIS) has managed to put large swaths of land in Syria and Iraq under their control. To many observers, the sheer speed at which this ""state"" was established was dumbfounding. To better understand the roots of this organization and its supporters we present a study using data from Twitter. We start by collecting large amounts of Arabic tweets referring to ISIS and classify them into pro-ISIS and anti-ISIS. This classification turns out to be easily done simply using the name variants used to refer to the organization: the full name and the description as ""state"" is associated with support, whereas abbreviations usually indicate opposition. We then ""go back in time"" by analyzing the historic timelines of both users supporting and opposing and look at their pre-ISIS period to gain insights into the antecedents of support. To achieve this, we build a classifier using pre-ISIS data to ""predict"", in retrospect, who will support or oppose the group. The key story that emerges is one of frustration with failed Arab Spring revolutions. ISIS supporters largely differ from ISIS opposition in that they refer a lot more to Arab Spring uprisings that failed. We also find temporal patterns in the support and opposition which seems to be linked to major news, such as reported territorial gains, reports on gruesome acts of violence, and reports on airstrikes and foreign intervention. "	1503.02401	Walid Magdy, Kareem Darwish, Ingmar Weber	2015-03-09		3	2	#FailedRevolutions: Using Twitter to Study the Antecedents of ISIS Support		cs.SI, physics.soc-ph	https://arxiv.org/pdf/1503.02401.pdf
394458	 This paper studies the relation between activity on Twitter and sales. While research exists into the relation between Tweets and movie and book sales, this paper shows that the same relations do not hold for products that receive less attention on social media. For such products, classification of Tweets is far more important to determine a relation. Also, for such products advanced statistical relations, in addition to correlation, are required to relate Twitter activity and sales. In a case study that involves Tweets and sales from a company in four countries, the paper shows how, by classifying Tweets, such relations can be identified. In particular, the paper shows evidence that positive Tweets by persons (as opposed to companies) can be used to forecast sales and that peaks in positive Tweets by persons are strongly related to an increase in sales. These results can be used to improve sales forecasts and to increase sales in marketing campaigns. 	1503.04599	Remco Dijkman, Panagiotis Ipeirotis, Freek Aertsen, Roy van Helden	2015-03-16		4	1	Using Twitter to Predict Sales: A Case Study		cs.SI	https://arxiv.org/pdf/1503.04599.pdf
398218	 Twitter as a new form of social media potentially contains useful information that opens new opportunities for content analysis on tweets. This paper examines the predictive power of Twitter regarding the US presidential election of 2012. For this study, we analyzed 32 million tweets regarding the US presidential election by employing a combination of machine learning techniques. We devised an advanced classifier for sentiment analysis in order to increase the accuracy of Twitter content analysis. We carried out our analysis by comparing Twitter results with traditional opinion polls. In addition, we used the Latent Dirichlet Allocation model to extract the underlying topical structure from the selected tweets. Our results show that we can determine the popularity of candidates by running sentiment analysis. We can also uncover candidates popularities in the US states by running the sentiment analysis algorithm on geo-tagged tweets. To the best of our knowledge, no previous work in the field has presented a systematic analysis of a considerable number of tweets employing a combination of analysis techniques by which we conducted this study. Thus, our results aptly suggest that Twitter as a well-known social medium is a valid source in predicting future events such as elections. This implies that understanding public opinions and trends via social media in turn allows us to propose a cost- and time-effective way not only for spreading and sharing information, but also for predicting future events. 	1407.0622	Kazem Jahanbakhsh, Yumi Moon	2014-07-01		2	3	The Predictive Power of Social Media: On the Predictability of U.S. Presidential Elections using Twitter		cs.SI, cs.CY, physics.soc-ph	https://arxiv.org/pdf/1407.0622.pdf
429892	 Online social media (OSM) has a enormous influence in today's world. Some individuals view OSM as fertile ground for abuse and use it to disseminate misinformation and political propaganda, slander competitors, and spread spam. The crowdturfing industry employs large numbers of bots and human workers to manipulate OSM and misrepresent public opinion. The detection of online discussion topics manipulated by OSM \emph{abusers} is an emerging issue attracting significant attention. In this paper, we propose an approach for quantifying the authenticity of online discussions based on the similarity of OSM accounts participating in the discussion to known abusers and legitimate accounts. Our method uses several similarity functions for the analysis and classification of OSM accounts. The proposed methods are demonstrated using Twitter data collected for this study and previously published \emph{Arabic honeypot dataset}. The former includes manually labeled accounts and abusers who participated in crowdturfing platforms. Evaluation of the topic's authenticity, derived from account similarity functions, shows that the suggested approach is effective for discriminating between topics that were strongly promoted by abusers and topics that attracted authentic public interest. 	1708.02763	Aviad Elyashar, Jorge Bendahan, Rami Puzis	2017-08-09		3	1	Has the Online Discussion Been Manipulated? Quantifying Online Discussion Authenticity within Online Social Media	2018-01-04	cs.SI	https://arxiv.org/pdf/1708.02763.pdf
436467	 In this paper, we study the implications of the commonplace assumption that most social media studies make with respect to the nature of message shares (such as retweets) as a predominantly positive interaction. By analyzing two large longitudinal Brazilian Twitter datasets containing 5 years of conversations on two polarizing topics - Politics and Sports - we empirically demonstrate that groups holding antagonistic views can actually retweet each other more often than they retweet other groups. We show that assuming retweets as endorsement interactions can lead to misleading conclusions with respect to the level of antagonism among social communities, and that this apparent paradox is explained in part by the use of retweets to quote the original content creator out of the message's original temporal context, for humor and criticism purposes. As a consequence, messages diffused on online media can have their polarity reversed over time, what poses challenges for social and computer scientists aiming to classify and track opinion groups on online media. On the other hand, we found that the time users take to retweet a message after it has been originally posted can be a useful signal to infer antagonism in social platforms, and that surges of out-of-context retweets correlate with sentiment drifts triggered by real-world events. We also discuss how such evidences can be embedded in sentiment analysis models. 	1703.03895	Pedro Calais Guerra, Roberto C. S. N. P. Souza, Renato M. Assunção, Wagner Meira	2017-03-10		4	1	Antagonism also Flows through Retweets: The Impact of Out-of-Context Quotes in Opinion Polarization Analysis		cs.SI	https://arxiv.org/pdf/1703.03895.pdf
441114	 Gun related violence is a complex issue and accounts for a large proportion of violent incidents. In the research reported in this paper, we set out to investigate the pro-gun and anti-gun sentiments expressed on a social media platform, namely Twitter, in response to the 2012 Sandy Hook Elementary School shooting in Connecticut, USA. Machine learning techniques are applied to classify a data corpus of over 700,000 tweets. The sentiments are captured using a public sentiment score that considers the volume of tweets as well as population. A web-based interactive tool is developed to visualise the sentiments and is available at http://www.gunsontwitter.com. The key findings from this research are: (i) There are elevated rates of both pro-gun and anti-gun sentiments on the day of the shooting. Surprisingly, the pro-gun sentiment remains high for a number of days following the event but the anti-gun sentiment quickly falls to pre-event levels. (ii) There is a different public response from each state, with the highest pro-gun sentiment not coming from those with highest gun ownership levels but rather from California, Texas and New York. 	1609.00536	Nan Wang, Blesson Varghese, Peter D. Donnelly	2016-09-02		3	1	A Machine Learning Analysis of Twitter Sentiment to the Sandy Hook Shootings		cs.SI	https://arxiv.org/pdf/1609.00536.pdf
459260	 In recent years, bullying and aggression against users on social media have grown significantly, causing serious consequences to victims of all demographics. In particular, cyberbullying affects more than half of young social media users worldwide, and has also led to teenage suicides, prompted by prolonged and/or coordinated digital harassment. Nonetheless, tools and technologies for understanding and mitigating it are scarce and mostly ineffective. In this paper, we present a principled and scalable approach to detect bullying and aggressive behavior on Twitter. We propose a robust methodology for extracting text, user, and network-based attributes, studying the properties of cyberbullies and aggressors, and what features distinguish them from regular users. We find that bully users post less, participate in fewer online communities, and are less popular than normal users, while aggressors are quite popular and tend to include more negativity in their posts. We evaluate our methodology using a corpus of 1.6M tweets posted over 3 months, and show that machine learning classification algorithms can accurately detect users exhibiting bullying and aggressive behavior, achieving over 90% AUC. 	1702.06877	Despoina Chatzakou, Nicolas Kourtellis, Jeremy Blackburn, Emiliano De Cristofaro, Gianluca Stringhini, Athena Vakali	2017-02-22		6	2	Mean Birds: Detecting Aggression and Bullying on Twitter	2017-05-12	cs.CY, cs.SI	https://arxiv.org/pdf/1702.06877.pdf
468243	" During sudden onset crisis events, the presence of spam, rumors and fake content on Twitter reduces the value of information contained on its messages (or ""tweets""). A possible solution to this problem is to use machine learning to automatically evaluate the credibility of a tweet, i.e. whether a person would deem the tweet believable or trustworthy. This has been often framed and studied as a supervised classification problem in an off-line (post-hoc) setting. In this paper, we present a semi-supervised ranking model for scoring tweets according to their credibility. This model is used in TweetCred, a real-time system that assigns a credibility score to tweets in a user's timeline. TweetCred, available as a browser plug-in, was installed and used by 1,127 Twitter users within a span of three months. During this period, the credibility score for about 5.4 million tweets was computed, allowing us to evaluate TweetCred in terms of response time, effectiveness and usability. To the best of our knowledge, this is the first research work to develop a real-time system for credibility on Twitter, and to evaluate it on a user base of this size. "	1405.5490	Aditi Gupta, Ponnurangam Kumaraguru, Carlos Castillo, Patrick Meier	2014-05-21		4	3	TweetCred: Real-Time Credibility Assessment of Content on Twitter	2015-01-30	cs.CR, cs.SI, physics.soc-ph	https://arxiv.org/pdf/1405.5490.pdf
469111	 Social media are becoming an increasingly important source of information about the public mood regarding issues such as elections, Brexit, stock market, etc. In this paper we focus on sentiment classification of Twitter data. Construction of sentiment classifiers is a standard text mining task, but here we address the question of how to properly evaluate them as there is no settled way to do so. Sentiment classes are ordered and unbalanced, and Twitter produces a stream of time-ordered data. The problem we address concerns the procedures used to obtain reliable estimates of performance measures, and whether the temporal ordering of the training and test data matters. We collected a large set of 1.5 million tweets in 13 European languages. We created 138 sentiment models and out-of-sample datasets, which are used as a gold standard for evaluations. The corresponding 138 in-sample datasets are used to empirically compare six different estimation procedures: three variants of cross-validation, and three variants of sequential validation (where test set always follows the training set). We find no significant difference between the best cross-validation and sequential validation. However, we observe that all cross-validation variants tend to overestimate the performance, while the sequential methods tend to underestimate it. Standard cross-validation with random selection of examples is significantly worse than the blocked cross-validation, and should not be used to evaluate classifiers in time-ordered data scenarios. 	1803.05160	Igor Mozetič, Luis Torgo, Vitor Cerqueira, Jasmina Smailović	2018-03-14	10.1371/journal.pone.0194317	4	3	How to evaluate sentiment classifiers for Twitter time-ordered data?		cs.CL, cs.IR, cs.SI	https://arxiv.org/pdf/1803.05160.pdf
481362	 Online social media are complementing and in some cases replacing person-to-person social interaction and redefining the diffusion of information. In particular, microblogs have become crucial grounds on which public relations, marketing, and political battles are fought. We introduce an extensible framework that will enable the real-time analysis of meme diffusion in social media by mining, visualizing, mapping, classifying, and modeling massive streams of public microblogging events. We describe a Web service that leverages this framework to track political memes in Twitter and help detect astroturfing, smear campaigns, and other misinformation in the context of U.S. political elections. We present some cases of abusive behaviors uncovered by our service. Finally, we discuss promising preliminary results on the detection of suspicious memes via supervised learning based on features extracted from the topology of the diffusion networks, sentiment analysis, and crowdsourced annotations. 	1011.3768	Jacob Ratkiewicz, Michael Conover, Mark Meiss, Bruno Gonçalves, Snehal Patil, Alessandro Flammini, Filippo Menczer	2010-11-16	10.1145/1963192.1963301	7	2	Detecting and Tracking the Spread of Astroturf Memes in Microblog Streams		cs.SI, cs.CY	https://arxiv.org/pdf/1011.3768.pdf
481488	 With the advent of online social media, phishers have started using social networks like Twitter, Facebook, and Foursquare to spread phishing scams. Twitter is an immensely popular micro-blogging network where people post short messages of 140 characters called tweets. It has over 100 million active users who post about 200 million tweets everyday. Phishers have started using Twitter as a medium to spread phishing because of this vast information dissemination. Further, it is difficult to detect phishing on Twitter unlike emails because of the quick spread of phishing links in the network, short size of the content, and use of URL obfuscation to shorten the URL. Our technique, PhishAri, detects phishing on Twitter in realtime. We use Twitter specific features along with URL features to detect whether a tweet posted with a URL is phishing or not. Some of the Twitter specific features we use are tweet content and its characteristics like length, hashtags, and mentions. Other Twitter features used are the characteristics of the Twitter user posting the tweet such as age of the account, number of tweets, and the follower-followee ratio. These Twitter specific features coupled with URL based features prove to be a strong mechanism to detect phishing tweets. We use machine learning classification techniques and detect phishing tweets with an accuracy of 92.52%. We have deployed our system for end-users by providing an easy to use Chrome browser extension which works in realtime and classifies a tweet as phishing or safe. We show that we are able to detect phishing tweets at zero hour with high accuracy which is much faster than public blacklists and as well as Twitter's own defense mechanism to detect malicious content. To the best of our knowledge, this is the first realtime, comprehensive and usable system to detect phishing on Twitter. 	1301.6899	Anupama Aggarwal, Ashwin Rajadesingan, Ponnurangam Kumaraguru	2013-01-29		3	2	PhishAri: Automatic Realtime Phishing Detection on Twitter		cs.SI, physics.soc-ph	https://arxiv.org/pdf/1301.6899.pdf
498174	 Much attention has been given to the task of gender inference of Twitter users. Although names are strong gender indicators, the names of Twitter users are rarely used as a feature; probably due to the high number of ill-formed names, which cannot be found in any name dictionary. Instead of relying solely on a name database, we propose a novel name classifier. Our approach extracts characteristics from the user names and uses those in order to assign the names to a gender. This enables us to classify international first names as well as ill-formed names. 	1606.05467	Juergen Mueller, Gerd Stumme	2016-06-17	10.1145/2955129.2955182	2	2	Gender Inference using Statistical Name Characteristics in Twitter	2016-07-01	cs.CL, cs.SI	https://arxiv.org/pdf/1606.05467.pdf
507264	 The topical stance detection problem addresses detecting the stance of the text content with respect to a given topic: whether the sentiment of the given text content is in FAVOR of (positive), is AGAINST (negative), or is NONE (neutral) towards the given topic. Using the concept of attention, we develop a two-phase solution. In the first phase, we classify subjectivity - whether a given tweet is neutral or subjective with respect to the given topic. In the second phase, we classify sentiment of the subjective tweets (ignoring the neutral tweets) - whether a given subjective tweet has a FAVOR or AGAINST stance towards the topic. We propose a Long Short-Term memory (LSTM) based deep neural network for each phase, and embed attention at each of the phases. On the SemEval 2016 stance detection Twitter task dataset, we obtain a best-case macro F-score of 68.84% and a best-case accuracy of 60.2%, outperforming the existing deep learning based solutions. Our framework, T-PAN, is the first in the topical stance detection literature, that uses deep learning within a two-phase architecture. 	1801.03032	Kuntal Dey, Ritvik Shrivastava, Saroj Kaushik	2018-01-09		3	3	Topical Stance Detection for Twitter: A Two-Phase LSTM Model Using Attention		cs.CL, cs.IR, cs.SI	https://arxiv.org/pdf/1801.03032.pdf
515203	 Detecting and preventing outbreaks of mosquito-borne diseases such as Dengue and Zika in Brasil and other tropical regions has long been a priority for governments in affected areas. Streaming social media content, such as Twitter, is increasingly being used for health vigilance applications such as flu detection. However, previous work has not addressed the complexity of drastic seasonal changes on Twitter content across multiple epidemic outbreaks. In order to address this gap, this paper contrasts two complementary approaches to detecting Twitter content that is relevant for Dengue outbreak detection, namely supervised classification and unsupervised clustering using topic modelling. Each approach has benefits and shortcomings. Our classifier achieves a prediction accuracy of about 80\% based on a small training set of about 1,000 instances, but the need for manual annotation makes it hard to track seasonal changes in the nature of the epidemics, such as the emergence of new types of virus in certain geographical locations. In contrast, LDA-based topic modelling scales well, generating cohesive and well-separated clusters from larger samples. While clusters can be easily re-generated following changes in epidemics, however, this approach makes it hard to clearly segregate relevant tweets into well-defined clusters. 	1605.00968	Paolo Missier, Alexander Romanovsky, Tudor Miu, Atinder Pal, Michael Daniilakis, Alessandro Garcia, Diego Cedrim, Leonardo da Silva Sousa	2016-05-03		8	1	Tracking Dengue Epidemics using Twitter Content Classification and Topic Modelling		cs.SI	https://arxiv.org/pdf/1605.00968.pdf
536645	 Twitter is often referred to as a backchannel for conferences. While the main conference takes place in a physical setting, attendees and virtual attendees socialize, introduce new ideas or broadcast information by microblogging on Twitter. In this paper we analyze the scholars' Twitter use in 16 Computer Science conferences over a timespan of five years. Our primary finding is that over the years there are increasing differences with respect to conversation use and information use in Twitter. We studied the interaction network between users to understand whether assumptions about the structure of the conversations hold over time and between different types of interactions, such as retweets, replies, and mentions. While `people come and people go', we want to understand what keeps people stay with the conference on Twitter. By casting the problem to a classification task, we find different factors that contribute to the continuing participation of users to the online Twitter conference activity. These results have implications for research communities to implement strategies for continuous and active participation among members. 	1403.7772	Xidao Wen, Yu-Ru Lin, Christoph Trattner, Denis Parra	2014-03-30		4	3	Twitter in Academic Conferences: Usage, Networking and Participation over Time		cs.SI, cs.CY, physics.soc-ph	https://arxiv.org/pdf/1403.7772.pdf
542611	 Currency trading (Forex) is the largest world market in terms of volume. We analyze trading and tweeting about the EUR-USD currency pair over a period of three years. First, a large number of tweets were manually labeled, and a Twitter stance classification model is constructed. The model then classifies all the tweets by the trading stance signal: buy, hold, or sell (EUR vs. USD). The Twitter stance is compared to the actual currency rates by applying the event study methodology, well-known in financial economics. It turns out that there are large differences in Twitter stance distribution and potential trading returns between the four groups of Twitter users: trading robots, spammers, trading companies, and individual traders. Additionally, we observe attempts of reputation manipulation by post festum removal of tweets with poor predictions, and deleting/reposting of identical tweets to increase the visibility without tainting one's Twitter timeline. 	1804.02233	Igor Mozetič, Peter Gabrovšek, Petra Kralj Novak	2018-04-06		3	4	Forex trading and Twitter: Spam, bots, and reputation manipulation	2018-04-16	cs.SI, cs.CL, cs.CY, econ.TH	https://arxiv.org/pdf/1804.02233.pdf
548431	 Digital traces of conversations in micro-blogging platforms and OSNs provide information about user opinion with a high degree of resolution. These information sources can be exploited to under- stand and monitor collective behaviors. In this work, we focus on polarization classes, i.e., those topics that require the user to side exclusively with one position. The proposed method provides an iterative classification of users and keywords: first, polarized users are identified, then polarized keywords are discovered by monitoring the activities of previously classified users. This method thus allows tracking users and topics over time. We report several experiments conducted on two Twitter datasets during political election time-frames. We measure the user classification accuracy on a golden set of users, and analyze the relevance of the extracted keywords for the ongoing political discussion. 	1610.08686	Mauro Coletto, Claudio Lucchese, Salvatore Orlando, Raffaele Perego	2016-10-27		4	1	Polarized User and Topic Tracking in Twitter		cs.SI	https://arxiv.org/pdf/1610.08686.pdf
550408	 Nationality identification unlocks important demographic information, with many applications in biomedical and sociological research. Existing name-based nationality classifiers use name substrings as features and are trained on small, unrepresentative sets of labeled names, typically extracted from Wikipedia. As a result, these methods achieve limited performance and cannot support fine-grained classification. We exploit the phenomena of homophily in communication patterns to learn name embeddings, a new representation that encodes gender, ethnicity, and nationality which is readily applicable to building classifiers and other systems. Through our analysis of 57M contact lists from a major Internet company, we are able to design a fine-grained nationality classifier covering 39 groups representing over 90% of the world population. In an evaluation against other published systems over 13 common classes, our F1 score (0.795) is substantial better than our closest competitor Ethnea (0.580). To the best of our knowledge, this is the most accurate, fine-grained nationality classifier available. As a social media application, we apply our classifiers to the followers of major Twitter celebrities over six different domains. We demonstrate stark differences in the ethnicities of the followers of Trump and Obama, and in the sports and entertainments favored by different groups. Finally, we identify an anomalous political figure whose presumably inflated following appears largely incapable of reading the language he posts in. 	1708.07903	Junting Ye, Shuchu Han, Yifan Hu, Baris Coskun, Meizhu Liu, Hong Qin, Steven Skiena	2017-08-25		7	2	Nationality Classification Using Name Embeddings		cs.SI, cs.CL	https://arxiv.org/pdf/1708.07903.pdf
562292	 Increasing evidence suggests that a growing amount of social media content is generated by autonomous entities known as social bots. In this work we present a framework to detect such entities on Twitter. We leverage more than a thousand features extracted from public data and meta-data about users: friends, tweet content and sentiment, network patterns, and activity time series. We benchmark the classification framework by using a publicly available dataset of Twitter bots. This training data is enriched by a manually annotated collection of active Twitter users that include both humans and bots of varying sophistication. Our models yield high accuracy and agreement with each other and can detect bots of different nature. Our estimates suggest that between 9% and 15% of active Twitter accounts are bots. Characterizing ties among accounts, we observe that simple bots tend to interact with bots that exhibit more human-like behaviors. Analysis of content flows reveals retweet and mention strategies adopted by bots to interact with different target groups. Using clustering analysis, we characterize several subclasses of accounts, including spammers, self promoters, and accounts that post content from connected applications. 	1703.03107	Onur Varol, Emilio Ferrara, Clayton A. Davis, Filippo Menczer, Alessandro Flammini	2017-03-08		5	1	Online Human-Bot Interactions: Detection, Estimation, and Characterization	2017-03-27	cs.SI	https://arxiv.org/pdf/1703.03107.pdf
576216	 Twitter is used for a variety of reasons, including information dissemination, marketing, political organizing and to spread propaganda, spamming, promotion, conversations, and so on. Characterizing these activities and categorizing associated user generated content is a challenging task. We present a information-theoretic approach to classification of user activity on Twitter. We focus on tweets that contain embedded URLs and study their collective `retweeting' dynamics. We identify two features, time-interval and user entropy, which we use to classify retweeting activity. We achieve good separation of different activities using just these two features and are able to categorize content based on the collective user response it generates. We have identified five distinct categories of retweeting activity on Twitter: automatic/robotic activity, newsworthy information dissemination, advertising and promotion, campaigns, and parasitic advertisement. In the course of our investigations, we have shown how Twitter can be exploited for promotional and spam-like activities. The content-independent, entropy-based activity classification method is computationally efficient, scalable and robust to sampling and missing data. It has many applications, including automatic spam-detection, trend identification, trust management, user-modeling, social search and content classification on online social media. 	1106.0346	Rumi Ghosh, Tawan Surachawala, Kristina Lerman	2011-06-01		3	2	Entropy-based Classification of 'Retweeting' Activity on Twitter		cs.SI, cs.CY	https://arxiv.org/pdf/1106.0346.pdf
594619	 Recent research has shown a substantial active presence of bots in online social networks (OSNs). In this paper we utilise our past work on studying bots (Stweeler) to comparatively analyse the usage and impact of bots and humans on Twitter, one of the largest OSNs in the world. We collect a large-scale Twitter dataset and define various metrics based on tweet metadata. We divide and filter the dataset in four popularity groups in terms of number of followers. Using a human annotation task we assign 'bot' and 'human' ground-truth labels to the dataset, and compare the annotations against an online bot detection tool for evaluation. We then ask a series of questions to discern important behavioural bot and human characteristics using metrics within and among four popularity groups. From the comparative analysis we draw important differences as well as surprising similarities between the two entities, thus paving the way for reliable classification of automated political infiltration, advertisement campaigns, and general bot detection. 	1704.01508	Zafar Gilani, Reza Farahbakhsh, Gareth Tyson, Liang Wang, Jon Crowcroft	2017-04-05		5	1	An in-depth characterisation of Bots and Humans on Twitter		cs.SI	https://arxiv.org/pdf/1704.01508.pdf
608699	 Breaking news leads to situations of fast-paced reporting in social media, producing all kinds of updates related to news stories, albeit with the caveat that some of those early updates tend to be rumours, i.e., information with an unverified status at the time of posting. Flagging information that is unverified can be helpful to avoid the spread of information that may turn out to be false. Detection of rumours can also feed a rumour tracking system that ultimately determines their veracity. In this paper we introduce a novel approach to rumour detection that learns from the sequential dynamics of reporting during breaking news in social media to detect rumours in new stories. Using Twitter datasets collected during five breaking news stories, we experiment with Conditional Random Fields as a sequential classifier that leverages context learnt during an event for rumour detection, which we compare with the state-of-the-art rumour detection system as well as other baselines. In contrast to existing work, our classifier does not need to observe tweets querying a piece of information to deem it a rumour, but instead we detect rumours from the tweet alone by exploiting context learnt during the event. Our classifier achieves competitive performance, beating the state-of-the-art classifier that relies on querying tweets with improved precision and recall, as well as outperforming our best baseline with nearly 40% improvement in terms of F1 score. The scale and diversity of our experiments reinforces the generalisability of our classifier. 	1610.07363	Arkaitz Zubiaga, Maria Liakata, Rob Procter	2016-10-24		3	3	Learning Reporting Dynamics during Breaking News for Rumour Detection in Social Media		cs.CL, cs.IR, cs.SI	https://arxiv.org/pdf/1610.07363.pdf
629780	 To deal with the sheer volume of information and gain competitive advantage, the news industry has started to explore and invest in news automation. In this paper, we present Reuters Tracer, a system that automates end-to-end news production using Twitter data. It is capable of detecting, classifying, annotating, and disseminating news in real time for Reuters journalists without manual intervention. In contrast to other similar systems, Tracer is topic and domain agnostic. It has a bottom-up approach to news detection, and does not rely on a predefined set of sources or subjects. Instead, it identifies emerging conversations from 12+ million tweets per day and selects those that are news-like. Then, it contextualizes each story by adding a summary and a topic to it, estimating its newsworthiness, veracity, novelty, and scope, and geotags it. Designing algorithms to generate news that meets the standards of Reuters journalists in accuracy and timeliness is quite challenging. But Tracer is able to achieve competitive precision, recall, timeliness, and veracity on news detection and delivery. In this paper, we reveal our key algorithm designs and evaluations that helped us achieve this goal, and lessons learned along the way. 	1711.04068	Xiaomo Liu, Armineh Nourbakhsh, Quanzhi Li, Sameena Shah, Robert Martin, John Duprey	2017-11-10		6	1	Reuters Tracer: Toward Automated News Production Using Large Scale Social Media Data		cs.SI	https://arxiv.org/pdf/1711.04068.pdf
658669	 In the widely used message platform Twitter, about 2% of the tweets contains the geographical location through exact GPS coordinates (latitude and longitude). Knowing the location of a tweet is useful for many data analytics questions. This research is looking at the determination of a location for tweets that do not contain GPS coordinates. An accuracy of 82% was achieved using a Naive Bayes model trained on features such as the users' timezone, the user's language, and the parsed user location. The classifier performs well on active Twitter countries such as the Netherlands and United Kingdom. An analysis of errors made by the classifier shows that mistakes were made due to limited information and shared properties between countries such as shared timezone. A feature analysis was performed in order to see the effect of different features. The features timezone and parsed user location were the most informative features. 	1508.02483	Han van der Veen, Djoerd Hiemstra, Tijs van den Broek, Michel Ehrenhard, Ariana Need	2015-08-11		5	1	Determine the User Country of a Tweet		cs.SI	https://arxiv.org/pdf/1508.02483.pdf
663027	 Background: Micro-blogging services such as Twitter offer the potential to crowdsource epidemics in real-time. However, Twitter posts ('tweets') are often ambiguous and reactive to media trends. In order to ground user messages in epidemic response we focused on tracking reports of self-protective behaviour such as avoiding public gatherings or increased sanitation as the basis for further risk analysis. Results: We created guidelines for tagging self protective behaviour based on Jones and Salath\'e (2009)'s behaviour response survey. Applying the guidelines to a corpus of 5283 Twitter messages related to influenza like illness showed a high level of inter-annotator agreement (kappa 0.86). We employed supervised learning using unigrams, bigrams and regular expressions as features with two supervised classifiers (SVM and Naive Bayes) to classify tweets into 4 self-reported protective behaviour categories plus a self-reported diagnosis. In addition to classification performance we report moderately strong Spearman's Rho correlation by comparing classifier output against WHO/NREVSS laboratory data for A(H1N1) in the USA during the 2009-2010 influenza season. Conclusions: The study adds to evidence supporting a high degree of correlation between pre-diagnostic social media signals and diagnostic influenza case data, pointing the way towards low cost sensor networks. We believe that the signals we have modelled may be applicable to a wide range of diseases. 	1110.3089	Nigel Collier, Nguyen Truong Son, Ngoc Mai Nguyen	2011-10-13	10.1186/2041-1480-2-S5-S9	3	3	OMG U got flu? Analysis of shared health messages for bio-surveillance		cs.CL, cs.IR, cs.SI	https://arxiv.org/pdf/1110.3089.pdf
677410	 The link between affect, defined as the capacity for sentimental arousal on the part of a message, and virality, defined as the probability that it be sent along, is of significant theoretical and practical importance, e.g. for viral marketing. A quantitative study of emailing of articles from the NY Times finds a strong link between positive affect and virality, and, based on psychological theories it is concluded that this relation is universally valid. The conclusion appears to be in contrast with classic theory of diffusion in news media emphasizing negative affect as promoting propagation. In this paper we explore the apparent paradox in a quantitative analysis of information diffusion on Twitter. Twitter is interesting in this context as it has been shown to present both the characteristics social and news media. The basic measure of virality in Twitter is the probability of retweet. Twitter is different from email in that retweeting does not depend on pre-existing social relations, but often occur among strangers, thus in this respect Twitter may be more similar to traditional news media. We therefore hypothesize that negative news content is more likely to be retweeted, while for non-news tweets positive sentiments support virality. To test the hypothesis we analyze three corpora: A complete sample of tweets about the COP15 climate summit, a random sample of tweets, and a general text corpus including news. The latter allows us to train a classifier that can distinguish tweets that carry news and non-news information. We present evidence that negative sentiment enhances virality in the news segment, but not in the non-news segment. We conclude that the relation between affect and virality is more complex than expected based on the findings of Berger and Milkman (2010), in short 'if you want to be cited: Sweet talk your friends or serve bad news to the public'. 	1101.0510	Lars Kai Hansen, Adam Arvidsson, Finn Årup Nielsen, Elanor Colleoni, Michael Etter	2011-01-03		5	3	Good Friends, Bad News - Affect and Virality in Twitter		cs.SI, cs.CL, physics.soc-ph	https://arxiv.org/pdf/1101.0510.pdf
682290	 The growing incidents of counterfeiting and associated economic and health consequences necessitate the development of active surveillance systems capable of producing timely and reliable information for all stake holders in the anti-counterfeiting fight. User generated content from social media platforms can provide early clues about product allergies, adverse events and product counterfeiting. This paper reports a work in progresswith contributions including: the development of a framework for gathering and analyzing the views and experiences of users of drug and cosmetic products using machine learning, text mining and sentiment analysis, the application of the proposed framework on Facebook comments and data from Twitter for brand analysis, and the description of how to develop a product safety lexicon and training data for modeling a machine learning classifier for drug and cosmetic product sentiment prediction. The initial brand and product comparison results signify the usefulness of text mining and sentiment analysis on social media data while the use of machine learning classifier for predicting the sentiment orientation provides a useful tool for users, product manufacturers, regulatory and enforcement agencies to monitor brand or product sentiment trends in order to act in the event of sudden or significant rise in negative sentiment. 	1510.05301	Haruna Isah, Daniel Neagu, Paul Trundle	2015-10-18	10.1109/UKCI.2014.6930158	3	2	Social Media Analysis for Product Safety using Text Mining and Sentiment Analysis		cs.SI, cs.IR	https://arxiv.org/pdf/1510.05301.pdf
687568	 We present a new algorithm for inferring the home location of Twitter users at different granularities, including city, state, time zone or geographic region, using the content of users tweets and their tweeting behavior. Unlike existing approaches, our algorithm uses an ensemble of statistical and heuristic classifiers to predict locations and makes use of a geographic gazetteer dictionary to identify place-name entities. We find that a hierarchical classification approach, where time zone, state or geographic region is predicted first and city is predicted next, can improve prediction accuracy. We have also analyzed movement variations of Twitter users, built a classifier to predict whether a user was travelling in a certain period of time and use that to further improve the location detection accuracy. Experimental evidence suggests that our algorithm works well in practice and outperforms the best existing algorithms for predicting the home location of Twitter users. 	1403.2345	Jalal Mahmud, Jeffrey Nichols, Clemens Drews	2014-03-07		3	3	Home Location Identification of Twitter Users		cs.SI, cs.CL, cs.CY	https://arxiv.org/pdf/1403.2345.pdf
695345	 Internet is fast becoming critically important to commerce, industry and individuals. Search Engine (SE) is the most vital component for communication network and also used for discover information for users or people. Search engine optimization (SEO) is the process that is mostly used to increasing traffic from free, organic or natural listings on search engines and also helps to increase website ranking. It includes techniques like link building, directory submission, classified submission etc. but SMO, on the other hand, is the process of promoting your website on social media platforms. It includes techniques like RSS feeds, social news and bookmarking sites, video and blogging sites, as well as social networking sites, such as Facebook, Twitter, Google+, Tumblr, Pinterest, Instagram etc.Social media optimization is becoming increasingly important for search engine optimization, as search engines are increasingly utilizing the recommendations of users of social networks to rank pages in the search engine result pages. Since it is more difficult to tip the influence the search engines in this way. Social Media Optimization (SMO) may also use to generate traffic on a website, promote your business at the center of social marketing place and increase ranking. 	1408.0332	Dushyant Vaghela	2014-08-01		1	1	Social Media Impact on Website Ranking		cs.SI	https://arxiv.org/pdf/1408.0332.pdf
698015	 Existence of spam URLs over emails and Online Social Media (OSM) has become a growing phenomenon. To counter the dissemination issues associated with long complex URLs in emails and character limit imposed on various OSM (like Twitter), the concept of URL shortening gained a lot of traction. URL shorteners take as input a long URL and give a short URL with the same landing page in return. With its immense popularity over time, it has become a prime target for the attackers giving them an advantage to conceal malicious content. Bitly, a leading service in this domain is being exploited heavily to carry out phishing attacks, work from home scams, pornographic content propagation, etc. This imposes additional performance pressure on Bitly and other URL shorteners to be able to detect and take a timely action against the illegitimate content. In this study, we analyzed a dataset marked as suspicious by Bitly in the month of October 2013 to highlight some ground issues in their spam detection mechanism. In addition, we identified some short URL based features and coupled them with two domain specific features to classify a Bitly URL as malicious / benign and achieved a maximum accuracy of 86.41%. To the best of our knowledge, this is the first large scale study to highlight the issues with Bitly's spam detection policies and proposing a suitable countermeasure. 	1405.1511	Neha Gupta, Ponnurangam Kumaraguru	2014-05-07		2	2	Exploration of gaps in Bitly's spam detection and relevant counter measures		cs.SI, cs.IR	https://arxiv.org/pdf/1405.1511.pdf
733840	 The problem associated with the propagation of fake news continues to grow at an alarming scale. This trend has generated much interest from politics to academia and industry alike. We propose a framework that detects and classifies fake news messages from Twitter posts using hybrid of convolutional neural networks and long-short term recurrent neural network models. The proposed work using this deep learning approach achieves 82% accuracy. Our approach intuitively identifies relevant features associated with fake news stories without previous knowledge of the domain. 	1806.11316	Oluwaseun Ajao, Deepayan Bhowmik, Shahrzad Zargari	2018-06-29	10.1145/3217804.3217917	3	2	Fake News Identification on Twitter with Hybrid CNN and RNN Models		cs.SI, cs.CL	https://arxiv.org/pdf/1806.11316.pdf
735557	 Research shows that various social media platforms on Internet such as Twitter, Tumblr (micro-blogging websites), Facebook (a popular social networking website), YouTube (largest video sharing and hosting website), Blogs and discussion forums are being misused by extremist groups for spreading their beliefs and ideologies, promoting radicalization, recruiting members and creating online virtual communities sharing a common agenda. Popular microblogging websites such as Twitter are being used as a real-time platform for information sharing and communication during planning and mobilization if civil unrest related events. Applying social media intelligence for predicting and identifying online radicalization and civil unrest oriented threats is an area that has attracted several researchers' attention over past 10 years. There are several algorithms, techniques and tools that have been proposed in existing literature to counter and combat cyber-extremism and predicting protest related events in much advance. In this paper, we conduct a literature review of all these existing techniques and do a comprehensive analysis to understand state-of-the-art, trends and research gaps. We present a one class classification approach to collect scholarly articles targeting the topics and subtopics of our research scope. We perform characterization, classification and an in-depth meta analysis meta-anlaysis of about 100 conference and journal papers to gain a better understanding of existing literature. 	1511.06858	Swati Agarwal, Ashish Sureka	2015-11-21		2	2	Applying Social Media Intelligence for Predicting and Identifying On-line Radicalization and Civil Unrest Oriented Threats		cs.CY, cs.SI	https://arxiv.org/pdf/1511.06858.pdf
735782	 This year (2014) in the month of May, the tenure of the 15th Lok Sabha was to end and the elections to the 543 parliamentary seats were to be held. A whooping $5 billion were spent on these elections, which made us stand second only to the US Presidential elections in terms of money spent. Swelling number of Internet users and Online Social Media (OSM) users could effect 3-4% of urban population votes as per a report of IAMAI (Internet & Mobile Association of India). Our count of tweets related to elections from September 2013 to May 2014, was close to 18.21 million. We analyzed the complete dataset and found that the activity on Twitter peaked during important events. It was evident from our data that the political behavior of the politicians affected their followers count. Yet another aim of our work was to find an efficient way to classify the political orientation of the users on Twitter. We used four different techniques: two were based on the content of the tweets, one on the user based features and another based on community detection algorithm on the retweet and user mention networks. We found that the community detection algorithm worked best. We built a portal to show the analysis of the tweets of the last 24 hours. To the best of our knowledge, this is the first academic pursuit to analyze the elections data and classify the users in the India General Elections 2014. 	1406.5059	Abhishek Bhola	2014-06-19		1	2	Twitter and Polls: Analyzing and estimating political orientation of Twitter users in India General #Elections2014		cs.SI, physics.soc-ph	https://arxiv.org/pdf/1406.5059.pdf
740361	 Being a matter of cognition, user interests should be apt to classification independent of the language of users, social network and content of interest itself. To prove it, we analyze a collection of English and Russian Twitter and Vkontakte community pages by interests of their followers. First, we create a model of Major Interests (MaIs) with the help of expert analysis and then classify a set of pages using machine learning algorithms (SVM, Neural Network, Naive Bayes, and some other). We take three interest domains that are typical of both English and Russian-speaking communities: football, rock music, vegetarianism. The results of classification show a greater correlation between Russian-Vkontakte and Russian-Twitter pages while English-Twitterpages appear to provide the highest score. 	1707.05481	Elena Mikhalkova, Nadezhda Ganzherli, Yuri Karyakin	2017-07-18		3	3	A Comparative Analysis of Social Network Pages by Interests of Their Followers	2017-10-17	cs.CL, cs.IR, cs.SI	https://arxiv.org/pdf/1707.05481.pdf
762111	 With the growing popularity and usage of online social media services, people now have accounts (some times several) on multiple and diverse services like Facebook, LinkedIn, Twitter and YouTube. Publicly available information can be used to create a digital footprint of any user using these social media services. Generating such digital footprints can be very useful for personalization, profile management, detecting malicious behavior of users. A very important application of analyzing users' online digital footprints is to protect users from potential privacy and security risks arising from the huge publicly available user information. We extracted information about user identities on different social networks through Social Graph API, FriendFeed, and Profilactic; we collated our own dataset to create the digital footprints of the users. We used username, display name, description, location, profile image, and number of connections to generate the digital footprints of the user. We applied context specific techniques (e.g. Jaro Winkler similarity, Wordnet based ontologies) to measure the similarity of the user profiles on different social networks. We specifically focused on Twitter and LinkedIn. In this paper, we present the analysis and results from applying automated classifiers for disambiguating profiles belonging to the same user from different social networks. UserID and Name were found to be the most discriminative features for disambiguating user profiles. Using the most promising set of features and similarity metrics, we achieved accuracy, precision and recall of 98%, 99%, and 96%, respectively. 	1301.6870	Anshu Malhotra, Luam Totti, Wagner Meira, Ponnurangam Kumaraguru, Virgilio Almeida	2013-01-29		5	1	Studying User Footprints in Different Online Social Networks		cs.SI	https://arxiv.org/pdf/1301.6870.pdf
776379	" We tackle the challenge of topic classification of tweets in the context of analyzing a large collection of curated streams by news outlets and other organizations to deliver relevant content to users. Our approach is novel in applying distant supervision based on semi-automatically identifying curated streams that are topically focused (for example, on politics, entertainment, or sports). These streams provide a source of labeled data to train topic classifiers that can then be applied to categorize tweets from more topically-diffuse streams. Experiments on both noisy labels and human ground-truth judgments demonstrate that our approach yields good topic classifiers essentially ""for free"", and that topic classifiers trained in this manner are able to dynamically adjust for topic drift as news on Twitter evolves. "	1704.06726	Salman Mohammed, Nimesh Ghelani, Jimmy Lin	2017-04-21		3	2	Distant Supervision for Topic Classification of Tweets in Curated Streams		cs.IR, cs.SI	https://arxiv.org/pdf/1704.06726.pdf
777124	 Smell has a huge influence over how we perceive places. Despite its importance, smell has been crucially overlooked by urban planners and scientists alike, not least because it is difficult to record and analyze at scale. One of the authors of this paper has ventured out in the urban world and conducted smellwalks in a variety of cities: participants were exposed to a range of different smellscapes and asked to record their experiences. As a result, smell-related words have been collected and classified, creating the first dictionary for urban smell. Here we explore the possibility of using social media data to reliably map the smells of entire cities. To this end, for both Barcelona and London, we collect geo-referenced picture tags from Flickr and Instagram, and geo-referenced tweets from Twitter. We match those tags and tweets with the words in the smell dictionary. We find that smell-related words are best classified in ten categories. We also find that specific categories (e.g., industry, transport, cleaning) correlate with governmental air quality indicators, adding validity to our study. 	1505.06851	Daniele Quercia, Rossano Schifanella, Luca Maria Aiello, Kate McLean	2015-05-26		4	2	Smelly Maps: The Digital Life of Urban Smellscapes		cs.SI, cs.CY	https://arxiv.org/pdf/1505.06851.pdf
779254	 Online Social Networks (OSNs), such as Facebook and Twitter, have become an integral part of our daily lives. There are hundreds of OSNs, each with its own focus in that each offers particular services and functionalities. Recent studies show that many OSN users create several accounts on multiple OSNs using the same or different personal information. Collecting all the available data of an individual from several OSNs and fusing it into a single profile can be useful for many purposes. In this paper, we introduce novel machine learning based methods for solving Entity Resolution (ER), a problem for matching user profiles across multiple OSNs. The presented methods are able to match between two user profiles from two different OSNs based on supervised learning techniques, which use features extracted from each one of the user profiles. By using the extracted features and supervised learning techniques, we developed classifiers which can perform entity matching between two profiles for the following scenarios: (a) matching entities across two OSNs; (b) searching for a user by similar name; and (c) de-anonymizing a user's identity. The constructed classifiers were tested by using data collected from two popular OSNs, Facebook and Xing. We then evaluated the classifiers' performances using various evaluation measures, such as true and false positive rates, accuracy, and the Area Under the receiver operator Curve (AUC). The constructed classifiers were evaluated and their classification performance measured by AUC was quite remarkable, with an AUC of up to 0.982 and an accuracy of up to 95.9% in identifying user profiles across two OSNs. 	1410.6717	Olga Peled, Michael Fire, Lior Rokach, Yuval Elovici	2014-10-24		4	1	Matching Entities Across Online Social Networks	2014-11-04	cs.SI	https://arxiv.org/pdf/1410.6717.pdf
795355	 This project addresses the problem of sentiment analysis in twitter; that is classifying tweets according to the sentiment expressed in them: positive, negative or neutral. Twitter is an online micro-blogging and social-networking platform which allows users to write short status updates of maximum length 140 characters. It is a rapidly expanding service with over 200 million registered users - out of which 100 million are active users and half of them log on twitter on a daily basis - generating nearly 250 million tweets per day. Due to this large amount of usage we hope to achieve a reflection of public sentiment by analysing the sentiments expressed in the tweets. Analysing the public sentiment is important for many applications such as firms trying to find out the response of their products in the market, predicting political elections and predicting socioeconomic phenomena like stock exchange. The aim of this project is to develop a functional classifier for accurate and automatic sentiment classification of an unknown tweet stream. 	1509.04219	Afroze Ibrahim Baqapuri	2015-09-14		1	3	Twitter Sentiment Analysis		cs.CL, cs.IR, cs.SI	https://arxiv.org/pdf/1509.04219.pdf
842958	 In the last decade we have witnessed the explosive growth of online social networking services (SNSs) such as Facebook, Twitter, RenRen and LinkedIn. While SNSs provide diverse benefits for example, forstering interpersonal relationships, community formations and news propagation, they also attracted uninvited nuiance. Spammers abuse SNSs as vehicles to spread spams rapidly and widely. Spams, unsolicited or inappropriate messages, significantly impair the credibility and reliability of services. Therefore, detecting spammers has become an urgent and critical issue in SNSs. This paper deals with Follow spam in Twitter. Instead of spreading annoying messages to the public, a spammer follows (subscribes to) legitimate users, and followed a legitimate user. Based on the assumption that the online relationships of spammers are different from those of legitimate users, we proposed classification schemes that detect follow spammers. Particularly, we focused on cascaded social relations and devised two schemes, TSP-Filtering and SS-Filtering, each of which utilizes Triad Significance Profile (TSP) and Social status (SS) in a two-hop subnetwork centered at each other. We also propose an emsemble technique, Cascaded-Filtering, that combine both TSP and SS properties. Our experiments on real Twitter datasets demonstrated that the proposed three approaches are very practical. The proposed schemes are scalable because instead of analyzing the whole network, they inspect user-centered two hop social networks. Our performance study showed that proposed methods yield significantly better performance than prior scheme in terms of true positives and false positives. 	1605.00448	Sihyun Jeong, Giseop Noh, Hayoung Oh, Chong-kwon Kim	2016-05-02		4	2	Follow Spam Detection based on Cascaded Social Information		cs.SI, cs.IR	https://arxiv.org/pdf/1605.00448.pdf
844043	 Sentiment classification is widely used for product reviews and in online social media such as forums, Twitter, and blogs. However, the problem of classifying the sentiment of user comments on news sites has not been addressed yet. News sites cover a wide range of domains including politics, sports, technology, and entertainment, in contrast to other online social sites such as forums and review sites, which are specific to a particular domain. A user associated with a news site is likely to post comments on diverse topics (e.g., politics, smartphones, and sports) or diverse entities (e.g., Obama, iPhone, or Google). Classifying the sentiment of users tied to various entities may help obtain a holistic view of their personality, which could be useful in applications such as online advertising, content personalization, and political campaign planning. In this paper, we formulate the problem of entity-specific sentiment classification of comments posted on news articles in Yahoo News and propose novel features that are specific to news comments. Experimental results show that our models outperform state-of-the-art baselines. 	1506.03775	Prakhar Biyani, Cornelia Caragea, Narayan Bhamidipati	2015-06-11		3	3	Entity-Specific Sentiment Classification of Yahoo News Comments		cs.CL, cs.IR, cs.SI	https://arxiv.org/pdf/1506.03775.pdf
859011	 Text actionability detection is the problem of classifying user authored natural language text, according to whether it can be acted upon by a responding agent. In this paper, we propose a supervised learning framework for domain-aware, large-scale actionability classification of social media messages. We derive lexicons, perform an in-depth analysis for over 25 text based features, and explore strategies to handle domains that have limited training data. We apply these methods to over 46 million messages spanning 75 companies and 35 languages, from both Facebook and Twitter. The models achieve an aggregate population-weighted F measure of 0.78 and accuracy of 0.74, with values of over 0.9 in some cases. 	1511.00722	Nemanja Spasojevic, Adithya Rao	2015-11-02		2	2	Identifying Actionable Messages on Social Media		cs.IR, cs.SI	https://arxiv.org/pdf/1511.00722.pdf
861102	" Social media offers a unique window into attitudes like racism and homophobia, exposure to which are important, hard to measure and understudied social determinants of health. However, individual geo-located observations from social media are noisy and geographically inconsistent. Existing areas by which exposures are measured, like Zip codes, average over irrelevant administratively-defined boundaries. Hence, in order to enable studies of online social environmental measures like attitudes on social media and their possible relationship to health outcomes, first there is a need for a method to define the collective, underlying degree of social media attitudes by region. To address this, we create the Socio-spatial-Self organizing map, ""SS-SOM"" pipeline to best identify regions by their latent social attitude from Twitter posts. SS-SOMs use neural embedding for text-classification, and augment traditional SOMs to generate a controlled number of non-overlapping, topologically-constrained and topically-similar clusters. We find that not only are SS-SOMs robust to missing data, the exposure of a cohort of men who are susceptible to multiple racism and homophobia-linked health outcomes, changes by up to 42% using SS-SOM measures as compared to using Zip code-based measures. "	1803.09002	Kunal Relia, Mohammad Akbari, Dustin Duncan, Rumi Chunara	2018-03-23	10.1145/3274414	4	2	Socio-spatial Self-organizing Maps: Using Social Media to Assess Relevant Geographies for Exposure to Social Processes	2018-09-04	cs.SI, cs.CY	https://arxiv.org/pdf/1803.09002.pdf
862655	 Directionality is a crucial ingredient in many complex networks in which information, energy or influence are transmitted. In such directed networks, analysing flows (and not only the strength of connections) is crucial to reveal important features of the network that might go undetected if the orientation of connections is ignored. We showcase here a flow-based approach for community detection in networks through the study of the network of the most influential Twitter users during the 2011 riots in England. Firstly, we use directed Markov Stability to extract descriptions of the network at different levels of coarseness in terms of interest communities, i.e., groups of nodes within which flows of information are contained and reinforced. Such interest communities reveal user groupings according to location, profession, employer, and topic. The study of flows also allows us to generate an interest distance, which affords a personalised view of the attention in the network as viewed from the vantage point of any given user. Secondly, we analyse the profiles of incoming and outgoing long-range flows with a combined approach of role-based similarity and the novel relaxed minimum spanning tree algorithm to reveal that the users in the network can be classified into five roles. These flow roles go beyond the standard leader/follower dichotomy and differ from classifications based on regular/structural equivalence. We then show that the interest communities fall into distinct informational organigrams characterised by a different mix of user roles reflecting the quality of dialogue within them. Our generic framework can be used to provide insight into how flows are generated, distributed, preserved and consumed in directed networks. 	1311.6785	Mariano Beguerisse-Díaz, Guillermo Garduño-Hernández, Borislav Vangelov, Sophia N. Yaliraki, Mauricio Barahona	2013-11-26	10.1098/rsif.2014.0940	5	2	Interest communities and flow roles in directed networks: the Twitter network of the UK riots	2014-10-08	physics.soc-ph, cs.SI	https://arxiv.org/pdf/1311.6785.pdf
866719	" Our goal is to determine the structural differences between different categories of networks and to use these differences to predict the network category. Existing work on this topic has looked at social networks such as Facebook, Twitter, co-author networks etc. We, instead, focus on a novel data set that we have assembled from a variety of sources, including law-enforcement agencies, financial institutions, commercial database providers and other similar organizations. The data set comprises networks of ""persons of interest"" with each network belonging to different categories such as suspected terrorists, convicted individuals etc. We demonstrate that such ""anti-social"" networks are qualitatively different from the usual social networks and that new techniques are required to identify and learn features of such networks for the purposes of prediction and classification. We propose Cliqster, a new generative Bernoulli process-based model for unweighted networks. The generating probabilities are the result of a decomposition which reflects a network's community structure. Using a maximum likelihood solution for the network inference leads to a least-squares problem. By solving this problem, we are able to present an efficient algorithm for transforming the network to a new space which is both concise and discriminative. This new space preserves the identity of the network as much as possible. Our algorithm is interpretable and intuitive. Finally, by comparing our research against the baseline method (SVD) and against a state-of-the-art Graphlet algorithm, we show the strength of our algorithm in discriminating between different categories of networks. "	1510.01374	Saber Shokat Fadaee, Mehrdad Farajtabar, Ravi Sundaram, Javed A. Aslam, Nikos Passas	2015-10-05	10.1007/s13278-015-0302-0	5	1	On The Network You Keep: Analyzing Persons of Interest using Cliqster		cs.SI	https://arxiv.org/pdf/1510.01374.pdf
874741	 Manual annotations are a prerequisite for many applications of machine learning. However, weaknesses in the annotation process itself are easy to overlook. In particular, scholars often choose what information to give to annotators without examining these decisions empirically. For subjective tasks such as sentiment analysis, sarcasm, and stance detection, such choices can impact results. Here, for the task of political stance detection on Twitter, we show that providing too little context can result in noisy and uncertain annotations, whereas providing too strong a context may cause it to outweigh other signals. To characterize and reduce these biases, we develop ConStance, a general model for reasoning about annotations across information conditions. Given conflicting labels produced by multiple annotators seeing the same instances with different contexts, ConStance simultaneously estimates gold standard labels and also learns a classifier for new instances. We show that the classifier learned by ConStance outperforms a variety of baselines at predicting political stance, while the model's interpretable parameters shed light on the effects of each context. 	1708.06309	Kenneth Joseph, Lisa Friedland, William Hobbs, Oren Tsur, David Lazer	2017-08-21		5	1	ConStance: Modeling Annotation Contexts to Improve Stance Classification		cs.SI	https://arxiv.org/pdf/1708.06309.pdf
879553	 Microblogging platforms such as Twitter provide active communication channels during mass convergence and emergency events such as earthquakes, typhoons. During the sudden onset of a crisis situation, affected people post useful information on Twitter that can be used for situational awareness and other humanitarian disaster response efforts, if processed timely and effectively. Processing social media information pose multiple challenges such as parsing noisy, brief and informal messages, learning information categories from the incoming stream of messages and classifying them into different classes among others. One of the basic necessities of many of these tasks is the availability of data, in particular human-annotated data. In this paper, we present human-annotated Twitter corpora collected during 19 different crises that took place between 2013 and 2015. To demonstrate the utility of the annotations, we train machine learning classifiers. Moreover, we publish first largest word2vec word embeddings trained on 52 million crisis-related tweets. To deal with tweets language issues, we present human-annotated normalized lexical resources for different lexical variations. 	1605.05894	Muhammad Imran, Prasenjit Mitra, Carlos Castillo	2016-05-19		3	3	Twitter as a Lifeline: Human-annotated Twitter Corpora for NLP of Crisis-related Messages	2016-05-31	cs.CL, cs.CY, cs.SI	https://arxiv.org/pdf/1605.05894.pdf
912512	" Social media's unfettered access has made it an important venue for health discussion and a resource for patients and their loved ones. However, the quality of the information available, as well as the motivations of its posters, has been questioned. This work examines the individuals on social media that are posting questionable health-related information, and in particular promoting cancer treatments which have been shown to be ineffective (making it a kind of misinformation, willful or not). Using a multi-stage user selection process, we study 4,212 Twitter users who have posted about one of 139 such ""treatments"", and compare them to a baseline of users generally interested in cancer. Considering features capturing user attributes, writing style, and sentiment, we build a classifier which is able to identify users prone to propagate such misinformation at an accuracy of over 90%, providing a potential tool for public health officials to identify such individuals for preventive intervention. "	1809.00557	Amira Ghenai, Yelena Mejova	2018-09-03		2	2	Fake Cures: User-centric Modeling of Health Misinformation in Social Media		cs.CY, cs.SI	https://arxiv.org/pdf/1809.00557.pdf
917031	 Working adults spend nearly one third of their daily time at their jobs. In this paper, we study job-related social media discourse from a community of users. We use both crowdsourcing and local expertise to train a classifier to detect job-related messages on Twitter. Additionally, we analyze the linguistic differences in a job-related corpus of tweets between individual users vs. commercial accounts. The volumes of job-related tweets from individual users indicate that people use Twitter with distinct monthly, daily, and hourly patterns. We further show that the moods associated with jobs, positive and negative, have unique diurnal rhythms. 	1511.04805	Tong Liu, Christopher M. Homan, Cecilia Ovesdotter Alm, Ann Marie White, Megan C. Lytle-Flint, Henry A. Kautz	2015-11-15		6	1	Job-related discourse on social media		cs.SI	https://arxiv.org/pdf/1511.04805.pdf
934044	 While social media has become an empowering agent to individual voices and freedom of expression, it also facilitates anti-social behaviors including online harassment, cyberbullying, and hate speech. In this paper, we present the first comparative study of hate speech instigators and target users on Twitter. Through a multi-step classification process, we curate a comprehensive hate speech dataset capturing various types of hate. We study the distinctive characteristics of hate instigators and targets in terms of their profile self-presentation, activities, and online visibility. We find that hate instigators target more popular and high profile Twitter users, and that participating in hate speech can result in greater online visibility. We conduct a personality analysis of hate instigators and targets and show that both groups have eccentric personality facets that differ from the general Twitter population. Our results advance the state of the art of understanding online hate speech engagement. 	1804.04649	Mai ElSherief, Shirin Nilizadeh, Dana Nguyen, Giovanni Vigna, Elizabeth Belding	2018-04-12		5	2	Peer to Peer Hate: Hate Speech Instigators and Their Targets		cs.SI, cs.CY	https://arxiv.org/pdf/1804.04649.pdf
938344	 Until recently, social media was seen to promote democratic discourse on social and political issues. However, this powerful communication platform has come under scrutiny for allowing hostile actors to exploit online discussions in an attempt to manipulate public opinion. A case in point is the ongoing U.S. Congress' investigation of Russian interference in the 2016 U.S. election campaign, with Russia accused of using trolls (malicious accounts created to manipulate) and bots to spread misinformation and politically biased information. In this study, we explore the effects of this manipulation campaign, taking a closer look at users who re-shared the posts produced on Twitter by the Russian troll accounts publicly disclosed by U.S. Congress investigation. We collected a dataset with over 43 million election-related posts shared on Twitter between September 16 and October 21, 2016, by about 5.7 million distinct users. This dataset included accounts associated with the identified Russian trolls. We use label propagation to infer the ideology of all users based on the news sources they shared. This method enables us to classify a large number of users as liberal or conservative with precision and recall above 90%. Conservatives retweeted Russian trolls about 31 times more often than liberals and produced 36x more tweets. Additionally, most retweets of troll content originated from two Southern states: Tennessee and Texas. Using state-of-the-art bot detection techniques, we estimated that about 4.9% and 6.2% of liberal and conservative users respectively were bots. Text analysis on the content shared by trolls reveals that they had a mostly conservative, pro-Trump agenda. Although an ideologically broad swath of Twitter users was exposed to Russian Trolls in the period leading up to the 2016 U.S. Presidential election, it was mainly conservatives who helped amplify their message. 	1802.04291	Adam Badawy, Emilio Ferrara, Kristina Lerman	2018-02-12		3	2	Analyzing the Digital Traces of Political Manipulation: The 2016 Russian Interference Twitter Campaign		cs.SI, cs.CY	https://arxiv.org/pdf/1802.04291.pdf
941118	 Microblogging websites like Twitter have been shown to be immensely useful for spreading information on a global scale within seconds. The detrimental effect, however, of such platforms is that misinformation and rumors are also as likely to spread on the network as credible, verified information. From a public health standpoint, the spread of misinformation creates unnecessary panic for the public. We recently witnessed several such scenarios during the outbreak of Ebola in 2014 [14, 1]. In order to effectively counter the medical misinformation in a timely manner, our goal here is to study the nature of such misinformation and rumors in the United States during fall 2014 when a handful of Ebola cases were confirmed in North America. It is a well known convention on Twitter to use hashtags to give context to a Twitter message (a tweet). In this study, we collected approximately 47M tweets from the Twitter streaming API related to Ebola. Based on hashtags, we propose a method to classify the tweets into two sets: credible and speculative. We analyze these two sets and study how they differ in terms of a number of features extracted from the Twitter API. In conclusion, we infer several interesting differences between the two sets. We outline further potential directions to using this material for monitoring and separating speculative tweets from credible ones, to enable improved public health information. 	1508.02079	Janani Kalyanam, Sumithra Velupillai, Son Doan, Mike Conway, Gert Lanckriet	2015-08-09		5	2	Facts and Fabrications about Ebola: A Twitter Based Study		cs.SI, cs.CY	https://arxiv.org/pdf/1508.02079.pdf
966604	" We present the Civique system for emergency detection in urban areas by monitoring micro blogs like Tweets. The system detects emergency related events, and classifies them into appropriate categories like ""fire"", ""accident"", ""earthquake"", etc. We demonstrate our ideas by classifying Twitter posts in real time, visualizing the ongoing event on a map interface and alerting users with options to contact relevant authorities, both online and offline. We evaluate our classifiers for both the steps, i.e., emergency detection and categorization, and obtain F-scores exceeding 70% and 90%, respectively. We demonstrate Civique using a web interface and on an Android application, in realtime, and show its use for both tweet detection and visualization. "	1610.04377	Diptesh Kanojia, Vishwajeet Kumar, Krithi Ramamritham	2016-10-14		3	3	Civique: Using Social Media to Detect Urban Emergencies		cs.CL, cs.CY, cs.SI	https://arxiv.org/pdf/1610.04377.pdf
990612	 The popularity of Twitter for information discovery, coupled with the automatic shortening of URLs to save space, given the 140 character limit, provides cyber criminals with an opportunity to obfuscate the URL of a malicious Web page within a tweet. Once the URL is obfuscated the cyber criminal can lure a user to click on it with enticing text and images before carrying out a cyber attack using a malicious Web server. This is known as a drive-by- download. In a drive-by-download a user's computer system is infected while interacting with the malicious endpoint, often without them being made aware, the attack has taken place. An attacker can gain control of the system by exploiting unpatched system vulnerabilities and this form of attack currently represents one of the most common methods employed. In this paper, we build a machine learning model using machine activity data and tweet meta data to move beyond post-execution classification of such URLs as malicious, to predict a URL will be malicious with 99.2% F-measure (using 10-fold cross validation) and 83.98% (using an unseen test set) at 1 second into the interaction with the URL. Thus providing a basis from which to kill the connection to the server before an attack has completed and proactively blocking and preventing an attack, rather than reacting and repairing at a later date. 	1708.05831	Amir Javed, Pete Burnap, Omer Rana	2017-08-19		3	1	Real Time Prediction of Drive by Download Attacks on Twitter		cs.SI	https://arxiv.org/pdf/1708.05831.pdf
1001423	 Polarization in American politics has been extensively documented and analyzed for decades, and the phenomenon became all the more apparent during the 2016 presidential election, where Trump and Clinton depicted two radically different pictures of America. Inspired by this gaping polarization and the extensive utilization of Twitter during the 2016 presidential campaign, in this paper we take the first step in measuring polarization in social media and we attempt to predict individuals' Twitter following behavior through analyzing ones' everyday tweets, profile images and posted pictures. As such, we treat polarization as a classification problem and study to what extent Trump followers and Clinton followers on Twitter can be distinguished, which in turn serves as a metric of polarization in general. We apply LSTM to processing tweet features and we extract visual features using the VGG neural network. Integrating these two sets of features boosts the overall performance. We are able to achieve an accuracy of 69%, suggesting that the high degree of polarization recorded in the literature has started to manifest itself in social media as well. 	1711.00617	Yu Wang, Yang Feng, Zhe Hong, Ryan Berger, Jiebo Luo	2017-11-02	10.1007/978-3-319-67217-5_27	5	1	How Polarized Have We Become? A Multimodal Classification of Trump Followers and Clinton Followers		cs.SI	https://arxiv.org/pdf/1711.00617.pdf
1005212	 Rumour stance classification, the task that determines if each tweet in a collection discussing a rumour is supporting, denying, questioning or simply commenting on the rumour, has been attracting substantial interest. Here we introduce a novel approach that makes use of the sequence of transitions observed in tree-structured conversation threads in Twitter. The conversation threads are formed by harvesting users' replies to one another, which results in a nested tree-like structure. Previous work addressing the stance classification task has treated each tweet as a separate unit. Here we analyse tweets by virtue of their position in a sequence and test two sequential classifiers, Linear-Chain CRF and Tree CRF, each of which makes different assumptions about the conversational structure. We experiment with eight Twitter datasets, collected during breaking news, and show that exploiting the sequential structure of Twitter conversations achieves significant improvements over the non-sequential methods. Our work is the first to model Twitter conversations as a tree structure in this manner, introducing a novel way of tackling NLP tasks on Twitter conversations. 	1609.09028	Arkaitz Zubiaga, Elena Kochkina, Maria Liakata, Rob Procter, Michal Lukasik	2016-09-28		5	2	Stance Classification in Rumours as a Sequential Task Exploiting the Tree Structure of Social Media Conversations	2016-10-11	cs.CL, cs.SI	https://arxiv.org/pdf/1609.09028.pdf
1005831	 With the rise of social media as an important channel for the debate and discussion of public affairs, online social networks such as Twitter have become important platforms for public information and engagement by policy makers. To communicate effectively through Twitter, policy makers need to understand how influence and interest propagate within its network of users. In this chapter we use graph-theoretic methods to analyse the Twitter debate surrounding NHS England's controversial care.data scheme. Directionality is a crucial feature of the Twitter social graph - information flows from the followed to the followers - but is often ignored in social network analyses; our methods are based on the behaviour of dynamic processes on the network and can be applied naturally to directed networks. We uncover robust communities of users and show that these communities reflect how information flows through the Twitter network. We are also able to classify users by their differing roles in directing the flow of information through the network. Our methods and results will be useful to policy makers who would like to use Twitter effectively as a communication medium. 	1508.03165	B. Amor, S. Vuik, R. Callahan, A. Darzi, S. N. Yaliraki, M. Barahona	2015-08-13		6	2	Community detection and role identification in directed networks: understanding the Twitter network of the care.data debate		cs.SI, physics.soc-ph	https://arxiv.org/pdf/1508.03165.pdf
1009238	 Gang affiliates have joined the masses who use social media to share thoughts and actions publicly. Interestingly, they use this public medium to express recent illegal actions, to intimidate others, and to share outrageous images and statements. Agencies able to unearth these profiles may thus be able to anticipate, stop, or hasten the investigation of gang-related crimes. This paper investigates the use of word embeddings to help identify gang members on Twitter. Building on our previous work, we generate word embeddings that translate what Twitter users post in their profile descriptions, tweets, profile images, and linked YouTube content to a real vector format amenable for machine learning classification. Our experimental results show that pre-trained word embeddings can boost the accuracy of supervised learning algorithms trained over gang members social media posts. 	1610.08597	Sanjaya Wijeratne, Lakshika Balasuriya, Derek Doran, Amit Sheth	2016-10-26		4	4	Word Embeddings to Enhance Twitter Gang Member Profile Identification		cs.SI, cs.CL, cs.CY, cs.IR	https://arxiv.org/pdf/1610.08597.pdf
1009550	 During time-critical situations such as natural disasters, rapid classification of data posted on social networks by affected people is useful for humanitarian organizations to gain situational awareness and to plan response efforts. However, the scarcity of labeled data in the early hours of a crisis hinders machine learning tasks thus delays crisis response. In this work, we propose to use an inductive semi-supervised technique to utilize unlabeled data, which is often abundant at the onset of a crisis event, along with fewer labeled data. Specif- ically, we adopt a graph-based deep learning framework to learn an inductive semi-supervised model. We use two real-world crisis datasets from Twitter to evaluate the proposed approach. Our results show significant improvements using unlabeled data as compared to only using labeled data. 	1805.06289	Firoj Alam, Shafiq Joty, Muhammad Imran	2018-05-02		3	4	Graph Based Semi-supervised Learning with Convolution Neural Networks to Classify Crisis Related Tweets		cs.CY, cs.CL, cs.IR, cs.SI	https://arxiv.org/pdf/1805.06289.pdf
1018913	 The first objective towards the effective use of microblogging services such as Twitter for situational awareness during the emerging disasters is discovery of the disaster-related postings. Given the wide range of possible disasters, using a pre-selected set of disaster-related keywords for the discovery is suboptimal. An alternative that we focus on in this work is to train a classifier using a small set of labeled postings that are becoming available as a disaster is emerging. Our hypothesis is that utilizing large quantities of historical microblogs could improve the quality of classification, as compared to training a classifier only on the labeled data. We propose to use unlabeled microblogs to cluster words into a limited number of clusters and use the word clusters as features for classification. To evaluate the proposed semi-supervised approach, we used Twitter data from 6 different disasters. Our results indicate that when the number of labeled tweets is 100 or less, the proposed approach is superior to the standard classification based on the bag or words feature representation. Our results also reveal that the choice of the unlabeled corpus, the choice of word clustering algorithm, and the choice of hyperparameters can have a significant impact on the classification accuracy. 	1610.03750	Shanshan Zhang, Slobodan Vucetic	2016-10-12		2	2	Semi-supervised Discovery of Informative Tweets During the Emerging Disasters		cs.CL, cs.SI	https://arxiv.org/pdf/1610.03750.pdf
1023554	" We present an in-depth study of co-following on Twitter based on the observation that two Twitter users whose followers have similar friends are also similar, even though they might not share any direct links or a single mutual follower. We show how this observation contributes to (i) a better understanding of language-agnostic user classification on Twitter, (ii) eliciting opportunities for Computational Social Science, and (iii) improving online marketing by identifying cross-selling opportunities. We start with a machine learning problem of predicting a user's preference among two alternative choices of Twitter friends. We show that co-following information provides strong signals for diverse classification tasks and that these signals persist even when (i) the most discriminative features are removed and (ii) only relatively ""sparse"" users with fewer than 152 but more than 43 Twitter friends are considered. Going beyond mere classification performance optimization, we present applications of our methodology to Computational Social Science. Here we confirm stereotypes such as that the country singer Kenny Chesney (@kennychesney) is more popular among @GOP followers, whereas Lady Gaga (@ladygaga) enjoys more support from @TheDemocrats followers. In the domain of marketing we give evidence that celebrity endorsement is reflected in co-following and we demonstrate how our methodology can be used to reveal the audience similarities between Apple and Puma and, less obviously, between Nike and Coca-Cola. Concerning a user's popularity we find a statistically significant connection between having a more ""average"" followership and having more followers than direct rivals. Interestingly, a \emph{larger} audience also seems to be linked to a \emph{less diverse} audience in terms of their co-following. "	1407.0791	Venkata Rama Kiran Garimella, Ingmar Weber	2014-07-03		2	2	Co-Following on Twitter		cs.SI, physics.soc-ph	https://arxiv.org/pdf/1407.0791.pdf
1061105	 Gender is playing an important role in the 2016 U.S. presidential election, especially with Hillary Clinton becoming the first female presidential nominee and Donald Trump being frequently accused of sexism. In this paper, we introduce computer vision to the study of gender politics and present an image-driven method that can measure the effects of gender in an accurate and timely manner. We first collect all the profile images of the candidates' Twitter followers. Then we train a convolutional neural network using images that contain gender labels. Lastly, we classify all the follower and unfollower images. Through two case studies, one on the `woman card' controversy and one on Sanders followers, we demonstrate how gender is informing the 2016 presidential election. Our framework of analysis can be readily generalized to other case studies and elections. 	1611.02806	Yu Wang, Yang Feng, Xiyang Zhang, Jiebo Luo	2016-11-08		4	1	Gender Politics in the 2016 U.S. Presidential Election: A Computer Vision Approach		cs.SI	https://arxiv.org/pdf/1611.02806.pdf
1071408	 Social media are being increasingly used for health promotion, yet the landscape of users, messages and interactions in such fora is poorly understood. Studies of social media and diabetes have focused mostly on patients, or public agencies addressing it, but have not looked broadly at all the participants or the diversity of content they contribute. We study Twitter conversations about diabetes through the systematic analysis of 2.5 million tweets collected over 8 months and the interactions between their authors. We address three questions: (1) what themes arise in these tweets?, (2) who are the most influential users?, (3) which type of users contribute to which themes? We answer these questions using a mixed-methods approach, integrating techniques from anthropology, network science and information retrieval such as thematic coding, temporal network analysis, and community and topic detection. Diabetes-related tweets fall within broad thematic groups: health information, news, social interaction, and commercial. At the same time, humorous messages and references to popular culture appear consistently, more than any other type of tweet. We classify authors according to their temporal 'hub' and 'authority' scores. Whereas the hub landscape is diffuse and fluid over time, top authorities are highly persistent across time and comprise bloggers, advocacy groups and NGOs related to diabetes, as well as for-profit entities without specific diabetes expertise. Top authorities fall into seven interest communities as derived from their Twitter follower network. Our findings have implications for public health professionals and policy makers who seek to use social media as an engagement tool and to inform policy design. 	1508.05764	Mariano Beguerisse-Díaz, Amy K. McLennan, Guillermo Garduño-Hernández, Mauricio Barahona, Stanley J. Ulijaszek	2015-08-24	10.1177/2055207616688841	5	4	The 'who' and 'what' of #diabetes on Twitter	2017-01-30	physics.soc-ph, cs.CY, cs.IR, cs.SI	https://arxiv.org/pdf/1508.05764.pdf
1078431	 Compounding of natural language units is a very common phenomena. In this paper, we show, for the first time, that Twitter hashtags which, could be considered as correlates of such linguistic units, undergo compounding. We identify reasons for this compounding and propose a prediction model that can identify with 77.07% accuracy if a pair of hashtags compounding in the near future (i.e., 2 months after compounding) shall become popular. At longer times T = 6, 10 months the accuracies are 77.52% and 79.13% respectively. This technique has strong implications to trending hashtag recommendation since newly formed hashtag compounds can be recommended early, even before the compounding has taken place. Further, humans can predict compounds with an overall accuracy of only 48.7% (treated as baseline). Notably, while humans can discriminate the relatively easier cases, the automatic framework is successful in classifying the relatively harder cases. 	1510.00249	Suman Kalyan Maity, Ritvik Saraf, Animesh Mukherjee	2015-10-01	10.1145/2818048.2820019	3	1	#Bieber + #Blast = #BieberBlast: Early Prediction of Popular Hashtag Compounds		cs.SI	https://arxiv.org/pdf/1510.00249.pdf
1087558	 During the 2016 US elections Twitter experienced unprecedented levels of propaganda and fake news through the collaboration of bots and hired persons, the ramifications of which are still being debated. This work proposes an approach to identify the presence of organized behavior in tweets. The Random Forest, Support Vector Machine, and Logistic Regression algorithms are each used to train a model with a data set of 850 records consisting of 299 features extracted from tweets gathered during the 2016 US presidential election. The features represent user and temporal synchronization characteristics to capture coordinated behavior. These models are trained to classify tweet sets among the categories: organic vs organized, political vs non-political, and pro-Trump vs pro-Hillary vs neither. The random forest algorithm performs better with greater than 95% average accuracy and f-measure scores for each category. The most valuable features for classification are identified as user based features, with media use and marking tweets as favorite to be the most dominant. 	1711.10720	Erdem Beğenilmiş, Suzan Üsküdarlı	2017-11-29		2	1	Organized Behavior Classification of Tweet Sets using Supervised Learning Methods		cs.SI	https://arxiv.org/pdf/1711.10720.pdf
1119307	 In the era of data-driven journalism, data analytics can deliver tools to support journalists in connecting to new and developing news stories, e.g., as echoed in micro-blogs such as Twitter, the new citizen-driven media. In this paper, we propose a framework for tracking and automatically connecting news articles to Twitter conversations as captured by Twitter hashtags. For example, such a system could alert journalists about news that get a lot of Twitter reaction, so that they can investigate those conversations for new developments in the story, promote their article to a set of interested consumers, or discover general sentiment towards the story. Mapping articles to appropriate hashtags is nevertheless very challenging, due to different language styles used in articles versus tweets, the streaming aspect of news and tweets, as well as the user behavior when marking certain tweet-terms as hashtags. As a case-study, we continuously track the RSS feeds of Irish Times news articles and a focused Twitter stream over a two months period, and present a system that assigns hashtags to each article, based on its Twitter echo. We propose a machine learning approach for classifying and ranking article-hashtag pairs. Our empirical study shows that our system delivers high precision for this task. 	1405.3117	Bichen Shi, Georgiana Ifrim, Neil Hurley	2014-05-13		3	3	Be In The Know: Connecting News Articles to Relevant Twitter Conversations		cs.SI, cs.IR, physics.soc-ph	https://arxiv.org/pdf/1405.3117.pdf
1149294	 Most existing techniques for spam detection on Twitter aim to identify and block users who post spam tweets. In this paper, we propose a Semi-Supervised Spam Detection (S3D) framework for spam detection at tweet-level. The proposed framework consists of two main modules: spam detection module operating in real-time mode, and model update module operating in batch mode. The spam detection module consists of four light-weight detectors: (i) blacklisted domain detector to label tweets containing blacklisted URLs, (ii) near-duplicate detector to label tweets that are near-duplicates of confidently pre-labeled tweets, (iii) reliable ham detector to label tweets that are posted by trusted users and that do not contain spammy words, and (iv) multi-classifier based detector labels the remaining tweets. The information required by the detection module are updated in batch mode based on the tweets that are labeled in the previous time window. Experiments on a large scale dataset show that the framework adaptively learns patterns of new spam activities and maintain good accuracy for spam detection in a tweet stream. 	1702.01032	Surendra Sedhai, Aixin Sun	2017-02-01		2	3	Semi-Supervised Spam Detection in Twitter Stream		cs.IR, cs.CR, cs.SI	https://arxiv.org/pdf/1702.01032.pdf
1151064	 Pinboard on Pinterest is an emerging media to engage online social media users, on which users post online images for specific topics. Regardless of its significance, there is little previous work specifically to facilitate information discovery based on pinboards. This paper proposes a novel pinboard recommendation system for Twitter users. In order to associate contents from the two social media platforms, we propose to use MultiLabel classification to map Twitter user followees to pinboard topics and visual diversification to recommend pinboards given user interested topics. A preliminary experiment on a dataset with 2000 users validated our proposed system. 	1509.00511	Xitong Yang, Yuncheng Li, Jiebo Luo	2015-09-01	10.1145/2733373.2806375	3	2	Pinterest Board Recommendation for Twitter Users		cs.SI, cs.MM	https://arxiv.org/pdf/1509.00511.pdf
1154354	 Multiple studies in the past have analyzed the role and dynamics of the Twitter social network during real world events. However, little work has explored the content of other social media services, or compared content across two networks during real world events. We believe that social media platforms like Facebook also play a vital role in disseminating information on the Internet during real world events. In this work, we study and characterize the content posted on the world's biggest social network, Facebook, and present a comparative analysis of Facebook and Twitter content posted during 16 real world events. Contrary to existing notion that Facebook is used mostly as a private network, our findings reveal that more than 30% of public content that was present on Facebook during these events, was also present on Twitter. We then performed qualitative analysis on the content spread by the most active users during these events, and found that over 10% of the most active users on both networks post spam content. We used stylometric features from Facebook posts and tweet text to classify this spam content, and were able to achieve an accuracy of over 99% for Facebook, and over 98% for Twitter. This work is aimed at providing researchers with an overview of Facebook content during real world events, and serve as basis for more in-depth exploration of its various aspects like information quality, and credibility during real world events. 	1405.4820	Prateek Dewan, Ponnurangam Kumaraguru	2014-05-19		2	2	It Doesn't Break Just on Twitter. Characterizing Facebook content During Real World Events		cs.SI, physics.soc-ph	https://arxiv.org/pdf/1405.4820.pdf
1173656	 From a crowded field with 17 candidates, Hillary Clinton and Donald Trump have emerged as the two front-runners in the 2016 U.S. presidential campaign. The two candidates each boast more than 5 million followers on Twitter, and at the same time both have witnessed hundreds of thousands of people leave their camps. In this paper we attempt to characterize individuals who have left Hillary Clinton and Donald Trump between September 2015 and March 2016. Our study focuses on three dimensions of social demographics: social capital, gender, and age. Within each camp, we compare the characteristics of the current followers with former followers, i.e., individuals who have left since September 2015. We use the number of followers to measure social capital, and profile images to infer gender and age. For classifying gender, we train a convolutional neural network (CNN). For age, we use the Face++ API. Our study shows that for both candidates followers with more social capital are more likely to leave (or switch camps). For both candidates females make up a larger presence among unfollowers than among current followers. Somewhat surprisingly, the effect is particularly pronounced for Clinton. Lastly, middle-aged individuals are more likely to leave Trump, and the young are more likely to leave Hillary Clinton. 	1604.07103	Yu Wang, Yuncheng Li, Quanzeng You, Xiyang Zhang, Richard Niemi, Jiebo Luo	2016-04-24		6	1	Voting with Feet: Who are Leaving Hillary Clinton and Donald Trump?	2016-04-25	cs.SI	https://arxiv.org/pdf/1604.07103.pdf
1185422	 The social networking era has left us with little privacy. The details of the social network users are published on Social Networking sites. Vulnerability has reached new heights due to the overpowering effects of social networking. The sites like Facebook, Twitter are having a huge set of users who publish their files, comments, messages in other users walls. These messages and comments could be of any nature. Even friends could post a comment that would harm a persons integrity. Thus there has to be a system which will monitor the messages and comments that are posted on the walls. If the messages are found to be neutral (does not have any harmful content), then it can be published. If the messages are found to have non-neutral content in them, then these messages would be blocked by the social network manager. The messages that are non-neutral would be of sexual, offensive, hatred, pun intended nature. Thus the social network manager can classify content as neutral and non-neutral and notify the user if there seems to be messages of non-neutral behavior. 	1604.01833	Jinju Joby P, Jyothi Korra	2016-04-06		2	1	System for Filtering Messages on Social Media Content		cs.SI	https://arxiv.org/pdf/1604.01833.pdf
1190434	 Online social networks are known to be demographically biased. Currently there are questions about what degree of representativity of the physical population they have, and how population biases impact user-generated content. In this paper we focus on centralism, a problem affecting Chile. Assuming that local differences exist in a country, in terms of vocabulary, we built a methodology based on the vector space model to find distinctive content from different locations, and use it to create classifiers to predict whether the content of a micro-post is related to a particular location, having in mind a geographically diverse selection of micro-posts. We evaluate them in a case study where we analyze the virtual population of Chile that participated in the Twitter social network during an event of national relevance: the municipal (local governments) elections held in 2012. We observe that the participating virtual population is spatially representative of the physical population, implying that there is centralism in Twitter. Our classifiers out-perform a non geographically-diverse baseline at the regional level, and have the same accuracy at a provincial level. However, our approach makes assumptions that need to be tested in multi-thematic and more general datasets. We leave this for future work. 	1309.1785	Eduardo Graells-Garrido, Barbara Poblete	2013-09-06	10.1145/2535597.2535611	2	2	#Santiago is not #Chile, or is it? A Model to Normalize Social Media Impact		cs.SI, physics.soc-ph	https://arxiv.org/pdf/1309.1785.pdf
1199614	 In recent times, social media sites such as Twitter have been extensively used for debating politics and public policies. These debates span millions of tweets and numerous topics of public importance. Thus, it is imperative that this vast trove of data is tapped in order to gain insights into public opinion especially on hotly contested issues such as abortion, gun reforms etc. Thus, in our work, we aim to gauge users' stance on such topics in Twitter. We propose ReLP, a semi-supervised framework using a retweet-based label propagation algorithm coupled with a supervised classifier to identify users with differing opinions. In particular, our framework is designed such that it can be easily adopted to different domains with little human supervision while still producing excellent accuracy 	1402.7143	Ashwin Rajadesingan, Huan Liu	2014-02-28		2	3	Identifying Users with Opposing Opinions in Twitter Debates	2014-04-10	cs.SI, cs.CY, physics.soc-ph	https://arxiv.org/pdf/1402.7143.pdf
1200562	 Network embedding, which aims to learn low-dimensional representations of nodes, has been used for various graph related tasks including visualization, link prediction and node classification. Most existing embedding methods rely solely on network structure. However, in practice we often have auxiliary information about the nodes and/or their interactions, e.g., content of scientific papers in co-authorship networks, or topics of communication in Twitter mention networks. Here we propose a novel embedding method that uses both network structure and edge attributes to learn better network representations. Our method jointly minimizes the reconstruction error for higher-order node neighborhood, social roles and edge attributes using a deep architecture that can adequately capture highly non-linear interactions. We demonstrate the efficacy of our model over existing state-of-the-art methods on a variety of real-world networks including collaboration networks, and social networks. We also observe that using edge attributes to inform network embedding yields better performance in downstream tasks such as link prediction and node classification. 	1805.03280	Palash Goyal, Homa Hosseinmardi, Emilio Ferrara, Aram Galstyan	2018-05-08		4	1	Capturing Edge Attributes via Network Embedding	2018-05-22	cs.SI	https://arxiv.org/pdf/1805.03280.pdf
1207123	" This study provides a methodological framework for the computer to classify tweets according to variables of the Theory of Planned Behavior. We present a sequential process of automated text analysis which combined supervised approach and unsupervised approach in order to make the computer to detect one of TPB variables in each tweet. We conducted Latent Dirichlet Allocation (LDA), Nearest Neighbor, and then assessed ""typicality"" of newly labeled tweets in order to predict classification boundary. Furthermore, this study reports findings from a content analysis of suicide-related tweets which identify traits of information environment in Twitter. Consistent with extant literature about suicide coverage, the findings demonstrate that tweets often contain information which prompt perceived behavior control of committing suicide, while rarely provided deterring information on suicide. We conclude by highlighting implications for methodological advances and empirical theory studies. "	1808.08331	Joon-Mo Park, Chul-joo Lee, Yunseok Jang	2018-08-24		3	1	Theory-Driven Automated Content Analysis of Suicidal Tweets : Using Typicality-Based Classification for LDA Dataset		cs.SI	https://arxiv.org/pdf/1808.08331.pdf
1220806	 Centrality is one of the most studied concepts in social network analysis. There is a huge literature regarding centrality measures, as ways to identify the most relevant users in a social network. The challenge is to find measures that can be computed efficiently, and that can be able to classify the users according to relevance criteria as close as possible to reality. We address this problem in the context of the Twitter network, an online social networking service with millions of users and an impressive flow of messages that are published and spread daily by interactions between users. Twitter has different types of users, but the greatest utility lies in finding the most influential ones. The purpose of this article is to collect and classify the different Twitter influence measures that exist so far in literature. These measures are very diverse. Some are based on simple metrics provided by the Twitter API, while others are based on complex mathematical models. Several measures are based on the PageRank algorithm, traditionally used to rank the websites on the Internet. Some others consider the timeline of publication, others the content of the messages, some are focused on specific topics, and others try to make predictions. We consider all these aspects, and some additional ones. Furthermore, we include measures of activity and popularity, the traditional mechanisms to correlate measures, and some important aspects of computational complexity for this particular context. 	1508.07951	Fabián Riquelme, Pablo González-Cantergiani	2015-08-31	10.1016/j.ipm.2016.04.003	2	1	Measuring user influence on Twitter: A survey	2016-04-24	cs.SI	https://arxiv.org/pdf/1508.07951.pdf
1224683	 The utility of Twitter data as a medium to support population-level mental health monitoring is not well understood. In an effort to better understand the predictive power of supervised machine learning classifiers and the influence of feature sets for efficiently classifying depression-related tweets on a large-scale, we conducted two feature study experiments. In the first experiment, we assessed the contribution of feature groups such as lexical information (e.g., unigrams) and emotions (e.g., strongly negative) using a feature ablation study. In the second experiment, we determined the percentile of top ranked features that produced the optimal classification performance by applying a three-step feature elimination approach. In the first experiment, we observed that lexical features are critical for identifying depressive symptoms, specifically for depressed mood (-35 points) and for disturbed sleep (-43 points). In the second experiment, we observed that the optimal F1-score performance of top ranked features in percentiles variably ranged across classes e.g., fatigue or loss of energy (5th percentile, 288 features) to depressed mood (55th percentile, 3,168 features) suggesting there is no consistent count of features for predicting depressive-related tweets. We conclude that simple lexical features and reduced feature sets can produce comparable results to larger feature sets. 	1701.08229	Danielle Mowery, Craig Bryan, Mike Conway	2017-01-27		3	4	Feature Studies to Inform the Classification of Depressive Symptoms from Twitter Data for Population Health		cs.IR, cs.CL, cs.CY, cs.SI	https://arxiv.org/pdf/1701.08229.pdf
1233552	 Speech acts are a way to conceptualize speech as action. This holds true for communication on any platform, including social media platforms such as Twitter. In this paper, we explored speech act recognition on Twitter by treating it as a multi-class classification problem. We created a taxonomy of six speech acts for Twitter and proposed a set of semantic and syntactic features. We trained and tested a logistic regression classifier using a data set of manually labelled tweets. Our method achieved a state-of-the-art performance with an average F1 score of more than $0.70$. We also explored classifiers with three different granularities (Twitter-wide, type-specific and topic-specific) in order to find the right balance between generalization and overfitting for our task. 	1605.05156	Soroush Vosoughi, Deb Roy	2016-05-17		2	2	Tweet Acts: A Speech Act Classifier for Twitter		cs.CL, cs.SI	https://arxiv.org/pdf/1605.05156.pdf
1238182	 On-line social networks publish information on a high volume of real-world events almost instantly, becoming a primary source for breaking news. Some of these real-world events can end up having a very strong impact on on-line social networks. The effect of such events can be analyzed from several perspectives, one of them being the intensity and characteristics of the collective activity that it produces in the social platform. We research 5,234 real-world news events encompassing 43 million messages discussed on the Twitter microblogging service for approximately 1 year. We show empirically that exogenous news events naturally create collective patterns of bursty behavior in combination with long periods of inactivity in the network. This type of behavior agrees with other patterns previously observed in other types of natural collective phenomena, as well as in individual human communications. In addition, we propose a methodology to classify news events according to the different levels of intensity in activity that they produce. In particular, we analyze the most highly active events and observe a consistent and strikingly different collective reaction from users when they are exposed to such events. This reaction is independent of an event's reach and scope. We further observe that extremely high-activity events have characteristics that are quite distinguishable at the beginning stages of their outbreak. This allows us to predict with high precision, the top 8% of events that will have the most impact in the social network by just using the first 5% of the information of an event's lifetime evolution. This strongly implies that high-activity events are naturally prioritized collectively by the social network, engaging users early on, way before they are brought to the mainstream audience. 	1511.01830	Janani Kalyanam, Mauricio Quezada, Barbara Poblete, Gert Lanckriet	2015-11-05	10.1371/journal.pone.0166694	4	2	Prediction and Characterization of High-Activity Events in Social Media Triggered by Real-World News	2016-10-10	cs.SI, physics.soc-ph	https://arxiv.org/pdf/1511.01830.pdf
1308282	 People use microblogging platforms like Twitter to involve with other users for a wide range of interests and practices. Twitter profiles run by different types of users such as humans, bots, spammers, businesses and professionals. This research work identifies six broad classes of Twitter users, and employs a supervised machine learning approach which uses a comprehensive set of features to classify users into the identified classes. For this purpose, we exploit users' profile and tweeting behavior information. We evaluate our approach by performing 10-fold cross validation using manually annotated 716 different Twitter profiles. High classification accuracy (measured using AUC, and precision, recall) reveals the significance of the proposed approach. 	1406.1335	Muhammad Moeen Uddin, Muhammad Imran, Hassan Sajjad	2014-06-05		3	2	Understanding Types of Users on Twitter		cs.SI, cs.CY	https://arxiv.org/pdf/1406.1335.pdf
1319488	 Online Radicalization (also called Cyber-Terrorism or Extremism or Cyber-Racism or Cyber- Hate) is widespread and has become a major and growing concern to the society, governments and law enforcement agencies around the world. Research shows that various platforms on the Internet (low barrier to publish content, allows anonymity, provides exposure to millions of users and a potential of a very quick and widespread diffusion of message) such as YouTube (a popular video sharing website), Twitter (an online micro-blogging service), Facebook (a popular social networking website), online discussion forums and blogosphere are being misused for malicious intent. Such platforms are being used to form hate groups, racist communities, spread extremist agenda, incite anger or violence, promote radicalization, recruit members and create virtual organi- zations and communities. Automatic detection of online radicalization is a technically challenging problem because of the vast amount of the data, unstructured and noisy user-generated content, dynamically changing content and adversary behavior. There are several solutions proposed in the literature aiming to combat and counter cyber-hate and cyber-extremism. In this survey, we review solutions to detect and analyze online radicalization. We review 40 papers published at 12 venues from June 2003 to November 2011. We present a novel classification scheme to classify these papers. We analyze these techniques, perform trend analysis, discuss limitations of existing techniques and find out research gaps. 	1301.4916	Denzil Correa, Ashish Sureka	2013-01-21		2	3	Solutions to Detect and Analyze Online Radicalization : A Survey		cs.IR, cs.SI, physics.soc-ph	https://arxiv.org/pdf/1301.4916.pdf
1334213	 During recent years the online social networks (in particular Twitter) have become an important alternative information channel to traditional media during natural disasters, but the amount and diversity of messages poses the challenge of information overload to end users. The goal of our research is to develop an automatic classifier of tweets to feed a mobile application that reduces the difficulties that citizens face to get relevant information during natural disasters. In this paper, we present in detail the process to build a classifier that filters tweets relevant and non-relevant to an earthquake. By using a dataset from the Chilean earthquake of 2010, we first build and validate a ground truth, and then we contribute by presenting in detail the effect of class imbalance and dimensionality reduction over 5 classifiers. We show how the performance of these models is affected by these variables, providing important considerations at the moment of building these systems. 	1503.05784	Alfredo Cobo, Denis Parra, Jaime Navón	2015-03-18		3	1	Identifying Relevant Messages in a Twitter-based Citizen Channel for Natural Disaster Situations		cs.SI	https://arxiv.org/pdf/1503.05784.pdf
1345297	 Social spam produces a great amount of noise on social media services such as Twitter, which reduces the signal-to-noise ratio that both end users and data mining applications observe. Existing techniques on social spam detection have focused primarily on the identification of spam accounts by using extensive historical and network-based data. In this paper we focus on the detection of spam tweets, which optimises the amount of data that needs to be gathered by relying only on tweet-inherent features. This enables the application of the spam detection system to a large set of tweets in a timely fashion, potentially applicable in a real-time or near real-time setting. Using two large hand-labelled datasets of tweets containing spam, we study the suitability of five classification algorithms and four different feature sets to the social spam detection task. Our results show that, by using the limited set of features readily available in a tweet, we can achieve encouraging results which are competitive when compared against existing spammer detection systems that make use of additional, costly user features. Our study is the first that attempts at generalising conclusions on the optimal classifiers and sets of features for social spam detection over different datasets. 	1503.07405	Bo Wang, Arkaitz Zubiaga, Maria Liakata, Rob Procter	2015-03-25		4	2	Making the Most of Tweet-Inherent Features for Social Spam Detection on Twitter		cs.IR, cs.SI	https://arxiv.org/pdf/1503.07405.pdf
1358584	 Sybil detection in social networks is a basic security research problem. Structure-based methods have been shown to be promising at detecting Sybils. Existing structure-based methods can be classified into Random Walk (RW)-based methods and Loop Belief Propagation (LBP)-based methods. RW-based methods cannot leverage labeled Sybils and labeled benign users simultaneously, which limits their detection accuracy, and/or they are not robust to noisy labels. LBP-based methods are not scalable and cannot guarantee convergence. In this work, we propose SybilSCAR, a novel structure-based method to detect Sybils in social networks. SybilSCAR is Scalable, Convergent, Accurate, and Robust to label noise. We first propose a framework to unify RW-based and LBP-based methods. Under our framework, these methods can be viewed as iteratively applying a (different) local rule to every user, which propagates label information among a social graph. Second, we design a new local rule, which SybilSCAR iteratively applies to every user to detect Sybils. We compare SybilSCAR with state-of-the-art RW-based and LBP-based methods theoretically and empirically. Theoretically, we show that, with proper parameter settings, SybilSCAR has a tighter asymptotical bound on the number of Sybils that are falsely accepted into a social network than existing structure-based methods. Empirically, we perform evaluation using both social networks with synthesized Sybils and a large-scale Twitter dataset (41.7M nodes and 1.2B edges) with real Sybils. Our results show that 1) SybilSCAR is substantially more accurate and more robust to label noise than state-of-the-art RW-based methods; 2) SybilSCAR is more accurate and one order of magnitude more scalable than state-of-the-art LBP-based methods. 	1803.04321	Binghui Wang, Jinyuan Jia, Le Zhang, Neil Zhenqiang Gong	2018-03-12		4	2	Structure-based Sybil Detection in Social Networks via Local Rule-based Propagation		cs.CR, cs.SI	https://arxiv.org/pdf/1803.04321.pdf
1358898	" Socialbots, or non-human/algorithmic social media users, have recently been documented as competing for information dissemination and disruption on online social networks. Here we investigate the influence of socialbots in Mexican Twitter in regards to the ""Tanhuato"" human rights abuse report. We analyze the applicability of the BotOrNot API to generalize from English to Spanish tweets and propose adaptations for Spanish-speaking bot detection. We then use text and sentiment analysis to compare the differences between bot and human tweets. Our analysis shows that bots actually aided in information proliferation among human users. This suggests that taxonomies classifying bots should include non-adversarial roles as well. Our study contributes to the understanding of different behaviors and intentions of automated accounts observed in empirical online social network data. Since this type of analysis is seldom performed in languages different from English, the proposed techniques we employ here are also useful for other non-English corpora. "	1710.11346	E. Velázquez, M. Yazdani, P. Suárez-Serrato	2017-10-31		3	2	Socialbots supporting human rights		cs.CY, cs.SI	https://arxiv.org/pdf/1710.11346.pdf
1363598	 In this paper, we propose a data-driven method to measure the impact of the 'woman card' exchange between Hillary Clinton and Donald Trump. Building from a unique dataset of the two candidates' Twitter followers, we first examine the transition dynamics of the two candidates' Twitter followers one week before the exchange and one week after. Then we train a convolutional neural network to classify the gender of the followers and unfollowers, and study how women in particular are reacting to the 'woman card' exchange. Our study suggests that the 'woman card' comment has made women more likely to follow Hillary Clinton, less likely to unfollow her and that it has apparently not affected the gender composition of Trump followers. 	1605.05401	Yu Wang, Yang Feng, Yuncheng Li, Xiyang Zhang, Richard Niemi, Jiebo Luo	2016-05-17		6	1	Pricing the Woman Card: Gender Politics between Hillary Clinton and Donald Trump		cs.SI	https://arxiv.org/pdf/1605.05401.pdf
1403847	 In this paper, we study the likelihood of Bernie Sanders supporters voting for Donald Trump instead of Hillary Clinton. Building from a unique time-series dataset of the three candidates' Twitter followers, which we make public here, we first study the proportion of Sanders followers who simultaneously follow Trump (but not Clinton) and how this evolves over time. Then we train a convolutional neural network to classify the gender of Sanders followers, and study whether men are more likely to jump ship for Trump than women. Our study shows that between March and May an increasing proportion of Sanders followers are following Trump (but not Clinton). The proportion of Sanders followers who follow Clinton but not Trump has actually decreased. Equally important, our study suggests that the jumping ship behavior will be affected by gender and that men are more likely to switch to Trump than women. 	1605.09473	Yu Wang, Yang Feng, Xiyang Zhang, Richard Niemi, Jiebo Luo	2016-05-30		5	1	Will Sanders Supporters Jump Ship for Trump? Fine-grained Analysis of Twitter Followers		cs.SI	https://arxiv.org/pdf/1605.09473.pdf
1404176	 The advent of the era of Big Data has allowed many researchers to dig into various socio-technical systems, including social media platforms. In particular, these systems have provided them with certain verifiable means to look into certain aspects of human behavior. In this work, we are specifically interested in the behavior of individuals on social media platforms---how they handle the information they get, and how they share it. We look into Twitter to understand the dynamics behind the users' posting activities---tweets and retweets---zooming in on topics that peaked in popularity. Three mechanisms are considered: endogenous stimuli, exogenous stimuli, and a mechanism that dictates the decay of interest of the population in a topic. We propose a model involving two parameters $\eta^\star$ and $\lambda$ describing the tweeting behaviour of users, which allow us to reconstruct the findings of Lehmann et al. (2012) on the temporal profiles of popular Twitter hashtags. With this model, we are able to accurately reproduce the temporal profile of user engagements on Twitter. Furthermore, we introduce an alternative in classifying the collective activities on the socio-technical system based on the model. 	1508.07097	Hoai Nguyen Huynh, Erika Fille Legara, Christopher Monterola	2015-08-28	10.1145/2700171.2791029	3	4	A Dynamical Model of Twitter Activity Profiles		cs.SI, cs.CY, cs.HC, physics.soc-ph	https://arxiv.org/pdf/1508.07097.pdf
1414628	 Over the past couple of years, clicking and posting selfies has become a popular trend. However, since March 2014, 127 people have died and many have been injured while trying to click a selfie. Researchers have studied selfies for understanding the psychology of the authors, and understanding its effect on social media platforms. In this work, we perform a comprehensive analysis of the selfie-related casualties and infer various reasons behind these deaths. We use inferences from incidents and from our understanding of the features, we create a system to make people more aware of the dangerous situations in which these selfies are taken. We use a combination of text-based, image-based and location-based features to classify a particular selfie as dangerous or not. Our method ran on 3,155 annotated selfies collected on Twitter gave 73% accuracy. Individually the image-based features were the most informative for the prediction task. The combination of image-based and location-based features resulted in the best accuracy. We have made our code and dataset available at http://labs.precog.iiitd.edu.in/killfie. 	1611.01911	Hemank Lamba, Varun Bharadhwaj, Mayank Vachher, Divyansh Agarwal, Megha Arora, Ponnurangam Kumaraguru	2016-11-07		6	2	Me, Myself and My Killfie: Characterizing and Preventing Selfie Deaths	2016-11-11	cs.SI, cs.CY	https://arxiv.org/pdf/1611.01911.pdf
1426915	 Most street gang members use Twitter to intimidate others, to present outrageous images and statements to the world, and to share recent illegal activities. Their tweets may thus be useful to law enforcement agencies to discover clues about recent crimes or to anticipate ones that may occur. Finding these posts, however, requires a method to discover gang member Twitter profiles. This is a challenging task since gang members represent a very small population of the 320 million Twitter users. This paper studies the problem of automatically finding gang members on Twitter. It outlines a process to curate one of the largest sets of verifiable gang member profiles that have ever been studied. A review of these profiles establishes differences in the language, images, YouTube links, and emojis gang members use compared to the rest of the Twitter population. Features from this review are used to train a series of supervised classifiers. Our classifier achieves a promising F1 score with a low false positive rate. 	1610.09516	Lakshika Balasuriya, Sanjaya Wijeratne, Derek Doran, Amit Sheth	2016-10-29		4	4	Finding Street Gang Members on Twitter		cs.SI, cs.CL, cs.CY, cs.IR	https://arxiv.org/pdf/1610.09516.pdf
1436896	 With 93% of pro-marijuana population in US favoring legalization of medical marijuana, high expectations of a greater return for Marijuana stocks, and public actively sharing information about medical, recreational and business aspects related to marijuana, it is no surprise that marijuana culture is thriving on Twitter. After the legalization of marijuana for recreational and medical purposes in 29 states, there has been a dramatic increase in the volume of drug-related communication on Twitter. Specifically, Twitter accounts have been established for promotional and informational purposes, some prominent among them being American Ganja, Medical Marijuana Exchange, and Cannabis Now. Identification and characterization of different user types can allow us to conduct more fine-grained spatiotemporal analysis to identify dominant or emerging topics in the echo chambers of marijuana-related communities on Twitter. In this research, we mainly focus on classifying Twitter accounts created and run by ordinary users, retailers, and informed agencies. Classifying user accounts by type can enable better capturing and highlighting of aspects such as trending topics, business profiling of marijuana companies, and state-specific marijuana policymaking. Furthermore, type-based analysis can provide more profound understanding and reliable assessment of the implications of marijuana-related communications. We developed a comprehensive approach to classifying users by their types on Twitter through contextualization of their marijuana-related conversations. We accomplished this using compositional multiview embedding synthesized from People, Content, and Network views achieving 8% improvement over the empirical baseline. 	1806.06813	Ugur Kursuncu, Manas Gaur, Usha Lokala, Anurag Illendula, Krishnaprasad Thirunarayan, Raminta Daniulaityte, Amit Sheth, I. Budak Arpinar	2018-06-18		8	1	"""What's ur type?"" Contextualized Classification of User Types in Marijuana-related Communications using Compositional Multiview Embedding"		cs.SI	https://arxiv.org/pdf/1806.06813.pdf
1550	 We present a study of the relationship between gender, linguistic style, and social networks, using a novel corpus of 14,000 Twitter users. Prior quantitative work on gender often treats this social variable as a female/male binary; we argue for a more nuanced approach. By clustering Twitter users, we find a natural decomposition of the dataset into various styles and topical interests. Many clusters have strong gender orientations, but their use of linguistic resources sometimes directly conflicts with the population-level language statistics. We view these clusters as a more accurate reflection of the multifaceted nature of gendered language styles. Previous corpus-based work has also had little to say about individuals whose linguistic styles defy population-level gender patterns. To identify such individuals, we train a statistical classifier, and measure the classifier confidence for each individual in the dataset. Examining individuals whose language does not match the classifier's model for their gender, we find that they have social networks that include significantly fewer same-gender social connections and that, in general, social network homophily is correlated with the use of same-gender language markers. Pairing computational methods and social theory thus offers a new perspective on how gender emerges as individuals position themselves relative to audiences, topics, and mainstream gender norms. 	1210.4567	David Bamman, Jacob Eisenstein, Tyler Schnoebelen	2012-10-16	10.1111/josl.12080	3	1	Gender identity and lexical variation in social media	2014-05-12	cs.CL	https://arxiv.org/pdf/1210.4567.pdf
15525	 Automatic abusive language detection is a difficult but important task for online social media. Our research explores a two-step approach of performing classification on abusive language and then classifying into specific types and compares it with one-step approach of doing one multi-class classification for detecting sexist and racist languages. With a public English Twitter corpus of 20 thousand tweets in the type of sexism and racism, our approach shows a promising performance of 0.827 F-measure by using HybridCNN in one-step and 0.824 F-measure by using logistic regression in two-steps. 	1706.01206	Ji Ho Park, Pascale Fung	2017-06-05		2	1	One-step and Two-step Classification for Abusive Language Detection on Twitter		cs.CL	https://arxiv.org/pdf/1706.01206.pdf
21613	 Commercial establishments like restaurants, service centres and retailers have several sources of customer feedback about products and services, most of which need not be as structured as rated reviews provided by services like Yelp, or Amazon, in terms of sentiment conveyed. For instance, Amazon provides a fine-grained score on a numeric scale for product reviews. Some sources, however, like social media (Twitter, Facebook), mailing lists (Google Groups) and forums (Quora) contain text data that is much more voluminous, but unstructured and unlabelled. It might be in the best interests of a business establishment to assess the general sentiment towards their brand on these platforms as well. This text could be pipelined into a system with a built-in prediction model, with the objective of generating real-time graphs on opinion and sentiment trends. Although such tasks like the one described about have been explored with respect to document classification problems in the past, the implementation described in this paper, by virtue of learning a continuous function rather than a discrete one, offers a lot more depth of insight as compared to document classification approaches. This study aims to explore the validity of such a continuous function predicting model to quantify sentiment about an entity, without the additional overhead of manual labelling, and computational preprocessing & feature extraction. This research project also aims to design and implement a re-usable document regression pipeline as a framework, Rapid-Rate, that can be used to predict document scores in real-time. 	1703.08088	Vineet John	2017-03-23	10.13140/RG.2.2.32385.04966	1	1	Rapid-Rate: A Framework for Semi-supervised Real-time Sentiment Trend Detection in Unstructured Big Data	2017-03-24	cs.CL	https://arxiv.org/pdf/1703.08088.pdf
43566	" This is the proposal for RumourEval-2019, which will run in early 2019 as part of that year's SemEval event. Since the first RumourEval shared task in 2017, interest in automated claim validation has greatly increased, as the dangers of ""fake news"" have become a mainstream concern. Yet automated support for rumour checking remains in its infancy. For this reason, it is important that a shared task in this area continues to provide a focus for effort, which is likely to increase. We therefore propose a continuation in which the veracity of further rumours is determined, and as previously, supportive of this goal, tweets discussing them are classified according to the stance they take regarding the rumour. Scope is extended compared with the first RumourEval, in that the dataset is substantially expanded to include Reddit as well as Twitter data, and additional languages are also included. "	1809.06683	Genevieve Gorrell, Kalina Bontcheva, Leon Derczynski, Elena Kochkina, Maria Liakata, Arkaitz Zubiaga	2018-09-18		6	1	RumourEval 2019: Determining Rumour Veracity and Support for Rumours		cs.CL	https://arxiv.org/pdf/1809.06683.pdf
75622	" Millions of users share their experiences on social media sites, such as Twitter, which in turn generate valuable data for public health monitoring, digital epidemiology, and other analyses of population health at global scale. The first, critical, task for these applications is classifying whether a personal health event was mentioned, which we call the (PHM) problem. This task is challenging for many reasons, including typically short length of social media posts, inventive spelling and lexicons, and figurative language, including hyperbole using diseases like ""heart attack"" or ""cancer"" for emphasis, and not as a health self-report. This problem is even more challenging for rarely reported, or frequent but ambiguously expressed conditions, such as ""stroke"". To address this problem, we propose a general, robust method for detecting PHMs in social media, which we call WESPAD, that combines lexical, syntactic, word embedding-based, and context-based features. WESPAD is able to generalize from few examples by automatically distorting the word embedding space to most effectively detect the true health mentions. Unlike previously proposed state-of-the-art supervised and deep-learning techniques, WESPAD requires relatively little training data, which makes it possible to adapt, with minimal effort, to each new disease and condition. We evaluate WESPAD on both an established publicly available Flu detection benchmark, and on a new dataset that we have constructed with mentions of multiple health conditions. Our experiments show that WESPAD outperforms the baselines and state-of-the-art methods, especially in cases when the number and proportion of true health mentions in the training data is small. "	1802.09130	Payam Karisani, Eugene Agichtein	2018-02-25		2	1	Did You Really Just Have a Heart Attack? Towards Robust Detection of Personal Health Mentions in Social Media	2018-03-03	cs.CL	https://arxiv.org/pdf/1802.09130.pdf
171268	 Twitter, a popular social media outlet, has evolved into a vast source of linguistic data, rich with opinion, sentiment, and discussion. Due to the increasing popularity of Twitter, its perceived potential for exerting social influence has led to the rise of a diverse community of automatons, commonly referred to as bots. These inorganic and semi-organic Twitter entities can range from the benevolent (e.g., weather-update bots, help-wanted-alert bots) to the malevolent (e.g., spamming messages, advertisements, or radical opinions). Existing detection algorithms typically leverage meta-data (time between tweets, number of followers, etc.) to identify robotic accounts. Here, we present a powerful classification scheme that exclusively uses the natural language text from organic users to provide a criterion for identifying accounts posting automated messages. Since the classifier operates on text alone, it is flexible and may be applied to any textual data beyond the Twitter-sphere. 	1505.04342	Eric M. Clark, Jake Ryland Williams, Chris A. Jones, Richard A. Galbraith, Christopher M. Danforth, Peter Sheridan Dodds	2015-05-16		6	1	Sifting Robotic from Organic Text: A Natural Language Approach for Detecting Automation on Twitter	2016-06-14	cs.CL	https://arxiv.org/pdf/1505.04342.pdf
175428	 The rapid expansion in the usage of social media networking sites leads to a huge amount of unprocessed user generated data which can be used for text mining. Author profiling is the problem of automatically determining profiling aspects like the author's gender and age group through a text is gaining much popularity in computational linguistics. Most of the past research in author profiling is concentrated on English texts \cite{1,2}. However many users often change the language while posting on social media which is called code-mixing, and it develops some challenges in the field of text classification and author profiling like variations in spelling, non-grammatical structure and transliteration \cite{3}. There are very few English-Hindi code-mixed annotated datasets of social media content present online \cite{4}. In this paper, we analyze the task of author's gender prediction in code-mixed content and present a corpus of English-Hindi texts collected from Twitter which is annotated with author's gender. We also explore language identification of every word in this corpus. We present a supervised classification baseline system which uses various machine learning algorithms to identify the gender of an author using a text, based on character and word level features. 	1806.05600	Ankush Khandelwal, Sahil Swami, Syed Sarfaraz Akhtar, Manish Shrivastava	2018-06-14		4	1	Gender Prediction in English-Hindi Code-Mixed Social Media Content : Corpus and Baseline System		cs.CL	https://arxiv.org/pdf/1806.05600.pdf
202744	 In the last couple decades, social network services like Twitter have generated large volumes of data about users and their interests, providing meaningful business intelligence so organizations can better understand and engage their customers. All businesses want to know who is promoting their products, who is complaining about them, and how are these opinions bringing or diminishing value to a company. Companies want to be able to identify their high-value customers and quantify the value each user brings. Many businesses use social media metrics to calculate the user contribution score, which enables them to quantify the value that influential users bring on social media, so the businesses can offer them more differentiated services. However, the score calculation can be refined to provide a better illustration of a user's contribution. Using Microsoft Azure as a case study, we conducted Twitter sentiment analysis to develop a machine learning classification model that identifies tweet contents and sentiments most illustrative of positive-value user contribution. Using data mining and AI-powered cognitive tools, we analyzed factors of social influence and specifically, promotional language in the developer community. Our predictive model was a combination of a traditional supervised machine learning algorithm and a custom-developed natural language model for identifying promotional tweets, that identifies a product-specific promotion on Twitter with a 90% accuracy rate. 	1711.11081	Angela Lin	2017-11-10		1	1	Improved Twitter Sentiment Analysis Using Naive Bayes and Custom Language Model		cs.CL	https://arxiv.org/pdf/1711.11081.pdf
219737	 We analyze over 500 million Twitter messages from an eight month period and find that tracking a small number of flu-related keywords allows us to forecast future influenza rates with high accuracy, obtaining a 95% correlation with national health statistics. We then analyze the robustness of this approach to spurious keyword matches, and we propose a document classification component to filter these misleading messages. We find that this document classifier can reduce error rates by over half in simulated false alarm experiments, though more research is needed to develop methods that are robust in cases of extremely high noise. 	1007.4748	Aron Culotta	2010-07-27		1	2	Detecting influenza outbreaks by analyzing Twitter messages		cs.IR, cs.CL	https://arxiv.org/pdf/1007.4748.pdf
229551	" We study feature selection as a means to optimize the baseline clickbait detector employed at the Clickbait Challenge 2017. The challenge's task is to score the ""clickbaitiness"" of a given Twitter tweet on a scale from 0 (no clickbait) to 1 (strong clickbait). Unlike most other approaches submitted to the challenge, the baseline approach is based on manual feature engineering and does not compete out of the box with many of the deep learning-based approaches. We show that scaling up feature selection efforts to heuristically identify better-performing feature subsets catapults the performance of the baseline classifier to second rank overall, beating 12 other competing approaches and improving over the baseline performance by 20%. This demonstrates that traditional classification approaches can still keep up with deep learning on this task. "	1802.01191	Matti Wiegmann, Michael Völske, Benno Stein, Matthias Hagen, Martin Potthast	2018-02-04		5	1	Heuristic Feature Selection for Clickbait Detection		cs.CL	https://arxiv.org/pdf/1802.01191.pdf
235668	 Mining social media messages such as tweets, articles, and Facebook posts for health and drug related information has received significant interest in pharmacovigilance research. Social media sites (e.g., Twitter), have been used for monitoring drug abuse, adverse reactions of drug usage and analyzing expression of sentiments related to drugs. Most of these studies are based on aggregated results from a large population rather than specific sets of individuals. In order to conduct studies at an individual level or specific cohorts, identifying posts mentioning intake of medicine by the user is necessary. Towards this objective we develop a classifier for identifying mentions of personal intake of medicine in tweets. We train a stacked ensemble of shallow convolutional neural network (CNN) models on an annotated dataset. We use random search for tuning the hyper-parameters of the CNN models and present an ensemble of best models for the prediction task. Our system produces state-of-the-art result, with a micro-averaged F-score of 0.693. We believe that the developed classifier has direct uses in the areas of psychology, health informatics, pharmacovigilance and affective computing for tracking moods, emotions and sentiments of patients expressing intake of medicine in social media. 	1808.02082	Debanjan Mahata, Jasper Friedrichs, Rajiv Ratn Shah, Jing Jiang	2018-08-02		4	1	Did you take the pill? - Detecting Personal Intake of Medicine from Twitter		cs.CL	https://arxiv.org/pdf/1808.02082.pdf
294974	 Word embeddings and convolutional neural networks (CNN) have attracted extensive attention in various classification tasks for Twitter, e.g. sentiment classification. However, the effect of the configuration used to train and generate the word embeddings on the classification performance has not been studied in the existing literature. In this paper, using a Twitter election classification task that aims to detect election-related tweets, we investigate the impact of the background dataset used to train the embedding models, the context window size and the dimensionality of word embeddings on the classification performance. By comparing the classification results of two word embedding models, which are trained using different background corpora (e.g. Wikipedia articles and Twitter microposts), we show that the background data type should align with the Twitter classification dataset to achieve a better performance. Moreover, by evaluating the results of word embeddings models trained using various context window sizes and dimensionalities, we found that large context window and dimension sizes are preferable to improve the performance. Our experimental results also show that using word embeddings and CNN leads to statistically significant improvements over various baselines such as random, SVM with TF-IDF and SVM with word embeddings. 	1606.07006	Xiao Yang, Craig Macdonald, Iadh Ounis	2016-06-22		3	2	Using Word Embeddings in Twitter Election Classification	2017-03-21	cs.IR, cs.CL	https://arxiv.org/pdf/1606.07006.pdf
328221	" Social media datasets, especially Twitter tweets, are popular in the field of text classification. Tweets are a valuable source of micro-text (sometimes referred to as ""micro-blogs""), and have been studied in domains such as sentiment analysis, recommendation systems, spam detection, clustering, among others. Tweets often include keywords referred to as ""Hashtags"" that can be used as labels for the tweet. Using tweets encompassing 50 labels, we studied the impact of word versus character-level feature selection and extraction on different learners to solve a multi-class classification task. We show that feature extraction of simple character-level groups performs better than simple word groups and pre-processing methods like normalizing using Porter's Stemming and Part-of-Speech (""POS"")-Lemmatization. "	1708.08123	Ankit Vadehra, Maura R. Grossman, Gordon V. Cormack	2017-08-27		3	2	Impact of Feature Selection on Micro-Text Classification		cs.IR, cs.CL	https://arxiv.org/pdf/1708.08123.pdf
328330	" The effect of amplifiers, downtoners, and negations has been studied in general and particularly in the context of sentiment analysis. However, there is only limited work which aims at transferring the results and methods to discrete classes of emotions, e. g., joy, anger, fear, sadness, surprise, and disgust. For instance, it is not straight-forward to interpret which emotion the phrase ""not happy"" expresses. With this paper, we aim at obtaining a better understanding of such modifiers in the context of emotion-bearing words and their impact on document-level emotion classification, namely, microposts on Twitter. We select an appropriate scope detection method for modifiers of emotion words, incorporate it in a document-level emotion classification model as additional bag of words and show that this approach improves the performance of emotion classification. In addition, we build a term weighting approach based on the different modifiers into a lexical model for the analysis of the semantics of modifiers and their impact on emotion meaning. We show that amplifiers separate emotions expressed with an emotion- bearing word more clearly from other secondary connotations. Downtoners have the opposite effect. In addition, we discuss the meaning of negations of emotion-bearing words. For instance we show empirically that ""not happy"" is closer to sadness than to anger and that fear-expressing words in the scope of downtoners often express surprise. "	1808.10653	Florian Strohm, Roman Klinger	2018-08-31		2	1	An Empirical Analysis of the Role of Amplifiers, Downtoners, and Negations in Emotion Classification in Microblogs		cs.CL	https://arxiv.org/pdf/1808.10653.pdf
338381	 Given the growing assortment of sentiment measuring instruments, it is imperative to understand which aspects of sentiment dictionaries contribute to both their classification accuracy and their ability to provide richer understanding of texts. Here, we perform detailed, quantitative tests and qualitative assessments of 6 dictionary-based methods applied, and briefly examine a further 20 methods. We show that while inappropriate for sentences, dictionary-based methods are generally robust in their classification accuracy for longer texts. Stories often following distinct emotional trajectories, forming patterns that are meaningful to us. By classifying the emotional arcs for a filtered subset of 4,803 stories from Project Gutenberg's fiction collection, we find a set of six core trajectories which form the building blocks of complex narratives. Of profound scientific interest will be the degree to which we can eventually understand the full landscape of human stories, and data driven approaches will play a crucial role. Finally, we utilize web-scale data from Twitter to study the limits of what social data can tell us about public health, mental illness, discourse around the protest movement of #BlackLivesMatter, discourse around climate change, and hidden networks. We conclude with a review of published works in complex systems that separately analyze charitable donations, the happiness of words in 10 languages, 100 years of daily temperature data across the United States, and Australian Rules Football games. 	1712.06163	Andrew J. Reagan	2017-12-17		1	1	Towards a science of human stories: using sentiment analysis and emotional arcs to understand the building blocks of complex social systems		cs.CL	https://arxiv.org/pdf/1712.06163.pdf
352680	 Sentiment Analysis in Arabic is a challenging task due to the rich morphology of the language. Moreover, the task is further complicated when applied to Twitter data that is known to be highly informal and noisy. In this paper, we develop a hybrid method for sentiment analysis for Arabic tweets for a specific Arabic dialect which is the Saudi Dialect. Several features were engineered and evaluated using a feature backward selection method. Then a hybrid method that combines a corpus-based and lexicon-based method was developed for several classification models (two-way, three-way, four-way). The best F1-score for each of these models was (69.9,61.63,55.07) respectively. 	1805.08533	Nora Al-Twairesh, Hend Al-Khalifa, AbdulMalik Alsalman, Yousef Al-Ohali	2018-05-22		4	1	Sentiment Analysis of Arabic Tweets: Feature Engineering and A Hybrid Approach		cs.CL	https://arxiv.org/pdf/1805.08533.pdf
366677	 The ability to accurately represent sentences is central to language understanding. We describe a convolutional architecture dubbed the Dynamic Convolutional Neural Network (DCNN) that we adopt for the semantic modelling of sentences. The network uses Dynamic k-Max Pooling, a global pooling operation over linear sequences. The network handles input sentences of varying length and induces a feature graph over the sentence that is capable of explicitly capturing short and long-range relations. The network does not rely on a parse tree and is easily applicable to any language. We test the DCNN in four experiments: small scale binary and multi-class sentiment prediction, six-way question classification and Twitter sentiment prediction by distant supervision. The network achieves excellent performance in the first three tasks and a greater than 25% error reduction in the last task with respect to the strongest baseline. 	1404.2188	Nal Kalchbrenner, Edward Grefenstette, Phil Blunsom	2014-04-08		3	1	A Convolutional Neural Network for Modelling Sentences		cs.CL	https://arxiv.org/pdf/1404.2188.pdf
428317	 This paper addresses the task of user gender classification in social media, with an application to Twitter. The approach automatically predicts gender by leveraging observable information such as the tweet behavior, linguistic content of the user's Twitter feed and the celebrities followed by the user. This paper first evaluates linguistic content based features using LIWC dictionary and popular neighborhood features using Wikipedia and Freebase. Then augments both features which yielded a significant increase in the accuracy for gender prediction. Results show that rich linguistic features combined with popular neighborhood prove valuables and promising for additional user classification needs. 	1405.6667	Puneet Singh Ludu	2014-05-26		1	2	Inferring gender of a Twitter user using celebrities it follows		cs.IR, cs.CL	https://arxiv.org/pdf/1405.6667.pdf
497980	 The utilization of social media material in journalistic workflows is increasing, demanding automated methods for the identification of mis- and disinformation. Since textual contradiction across social media posts can be a signal of rumorousness, we seek to model how claims in Twitter posts are being textually contradicted. We identify two different contexts in which contradiction emerges: its broader form can be observed across independently posted tweets and its more specific form in threaded conversations. We define how the two scenarios differ in terms of central elements of argumentation: claims and conversation structure. We design and evaluate models for the two scenarios uniformly as 3-way Recognizing Textual Entailment tasks in order to represent claims and conversation structure implicitly in a generic inference model, while previous studies used explicit or no representation of these properties. To address noisy text, our classifiers use simple similarity features derived from the string and part-of-speech level. Corpus statistics reveal distribution differences for these features in contradictory as opposed to non-contradictory tweet relations, and the classifiers yield state of the art performance. 	1611.02588	Piroska Lendvai, Uwe D. Reichel	2016-11-08		2	1	Contradiction Detection for Rumorous Claims	2016-11-11	cs.CL	https://arxiv.org/pdf/1611.02588.pdf
500073	 In the age of social news, it is important to understand the types of reactions that are evoked from news sources with various levels of credibility. In the present work we seek to better understand how users react to trusted and deceptive news sources across two popular, and very different, social media platforms. To that end, (1) we develop a model to classify user reactions into one of nine types, such as answer, elaboration, and question, etc, and (2) we measure the speed and the type of reaction for trusted and deceptive news sources for 10.8M Twitter posts and 6.2M Reddit comments. We show that there are significant differences in the speed and the type of reactions between trusted and deceptive news sources on Twitter, but far smaller differences on Reddit. 	1805.12032	Maria Glenski, Tim Weninger, Svitlana Volkova	2018-05-30		3	1	Identifying and Understanding User Reactions to Deceptive and Trusted Social News Sources		cs.CL	https://arxiv.org/pdf/1805.12032.pdf
548254	 We propose MVCNN, a convolution neural network (CNN) architecture for sentence classification. It (i) combines diverse versions of pretrained word embeddings and (ii) extracts features of multigranular phrases with variable-size convolution filters. We also show that pretraining MVCNN is critical for good performance. MVCNN achieves state-of-the-art performance on four tasks: on small-scale binary, small-scale multi-class and largescale Twitter sentiment prediction and on subjectivity classification. 	1603.04513	Wenpeng Yin, Hinrich Schütze	2016-03-14		2	1	Multichannel Variable-Size Convolution for Sentence Classification		cs.CL	https://arxiv.org/pdf/1603.04513.pdf
573370	 We present results of empirical studies on positive speech on Twitter. By positive speech we understand speech that works for the betterment of a given situation, in this case relations between different communities in a conflict-prone country. We worked with four Twitter data sets. Through semi-manual opinion mining, we found that positive speech accounted for < 1% of the data . In fully automated studies, we tested two approaches: unsupervised statistical analysis, and supervised text classification based on distributed word representation. We discuss benefits and challenges of those approaches and report empirical evidence obtained in the study. 	1702.08866	Marina Sokolova, Vera Sazonova, Kanyi Huang, Rudraneel Chakraboty, Stan Matwin	2017-02-24		5	1	Studying Positive Speech on Twitter		cs.CL	https://arxiv.org/pdf/1702.08866.pdf
591775	 Stance classification determines the attitude, or stance, in a (typically short) text. The task has powerful applications, such as the detection of fake news or the automatic extraction of attitudes toward entities or events in the media. This paper describes a surprisingly simple and efficient classification approach to open stance classification in Twitter, for rumour and veracity classification. The approach profits from a novel set of automatically identifiable problem-specific features, which significantly boost classifier accuracy and achieve above state-of-the-art results on recent benchmark datasets. This calls into question the value of using complex sophisticated models for stance classification without first doing informed feature extraction. 	1708.05286	Ahmet Aker, Leon Derczynski, Kalina Bontcheva	2017-08-17		3	1	Simple Open Stance Classification for Rumour Analysis	2017-09-14	cs.CL	https://arxiv.org/pdf/1708.05286.pdf
631436	 Sentiment analysis on large-scale social media data is important to bridge the gaps between social media contents and real world activities including political election prediction, individual and public emotional status monitoring and analysis, and so on. Although textual sentiment analysis has been well studied based on platforms such as Twitter and Instagram, analysis of the role of extensive emoji uses in sentiment analysis remains light. In this paper, we propose a novel scheme for Twitter sentiment analysis with extra attention on emojis. We first learn bi-sense emoji embeddings under positive and negative sentimental tweets individually, and then train a sentiment classifier by attending on these bi-sense emoji embeddings with an attention-based long short-term memory network (LSTM). Our experiments show that the bi-sense embedding is effective for extracting sentiment-aware embeddings of emojis and outperforms the state-of-the-art models. We also visualize the attentions to show that the bi-sense emoji embedding provides better guidance on the attention mechanism to obtain a more robust understanding of the semantics and sentiments. 	1807.07961	Yuxiao Chen, Jianbo Yuan, Quanzeng You, Jiebo Luo	2018-07-20	10.1145/3240508.3240533	4	2	Twitter Sentiment Analysis via Bi-sense Emoji Embedding and Attention-based LSTM	2018-08-06	cs.CL, cs.MM	https://arxiv.org/pdf/1807.07961.pdf
708185	 Named Entity Recognition (NER) is an important subtask of information extraction that seeks to locate and recognise named entities. Despite recent achievements, we still face limitations with correctly detecting and classifying entities, prominently in short and noisy text, such as Twitter. An important negative aspect in most of NER approaches is the high dependency on hand-crafted features and domain-specific knowledge, necessary to achieve state-of-the-art results. Thus, devising models to deal with such linguistically complex contexts is still challenging. In this paper, we propose a novel multi-level architecture that does not rely on any specific linguistic resource or encoded rule. Unlike traditional approaches, we use features extracted from images and text to classify named entities. Experimental tests against state-of-the-art NER for Twitter on the Ritter dataset present competitive results (0.59 F-measure), indicating that this approach may lead towards better NER models. 	1710.11027	Diego Esteves, Rafael Peres, Jens Lehmann, Giulio Napolitano	2017-10-30		4	2	Named Entity Recognition in Twitter using Images and Text		cs.IR, cs.CL	https://arxiv.org/pdf/1710.11027.pdf
713575	 Twitter messages often contain so-called hashtags to denote keywords related to them. Using a dataset of 29 million messages, I explore relations among these hashtags with respect to co-occurrences. Furthermore, I present an attempt to classify hashtags into five intuitive classes, using a machine-learning approach. The overall outcome is an interactive Web application to explore Twitter hashtags. 	1111.6553	Jan Pöschko	2011-11-28		1	1	Exploring Twitter Hashtags		cs.CL	https://arxiv.org/pdf/1111.6553.pdf
782234	 With the development of the Internet, natural language processing (NLP), in which sentiment analysis is an important task, became vital in information processing.Sentiment analysis includes aspect sentiment classification. Aspect sentiment can provide complete and in-depth results with increased attention on aspect-level. Different context words in a sentence influence the sentiment polarity of a sentence variably, and polarity varies based on the different aspects in a sentence. Take the sentence, 'I bought a new camera. The picture quality is amazing but the battery life is too short.'as an example. If the aspect is picture quality, then the expected sentiment polarity is 'positive', if the battery life aspect is considered, then the sentiment polarity should be 'negative'; therefore, aspect is important to consider when we explore aspect sentiment in the sentence. Recurrent neural network (RNN) is regarded as a good model to deal with natural language processing, and RNNs has get good performance on aspect sentiment classification including Target-Dependent LSTM (TD-LSTM) ,Target-Connection LSTM (TC-LSTM) (Tang, 2015a, b), AE-LSTM, AT-LSTM, AEAT-LSTM (Wang et al., 2016).There are also extensive literatures on sentiment classification utilizing convolutional neural network, but there is little literature on aspect sentiment classification using convolutional neural network. In our paper, we develop attention-based input layers in which aspect information is considered by input layer. We then incorporate attention-based input layers into convolutional neural network (CNN) to introduce context words information. In our experiment, incorporating aspect information into CNN improves the latter's aspect sentiment classification performance without using syntactic parser or external sentiment lexicons in a benchmark dataset from Twitter but get better performance compared with other models. 	1807.01704	Yongping Xing, Chuangbai Xiao, Yifei Wu, Ziming Ding	2018-07-04		4	1	A Convolutional Neural Network for Aspect Sentiment Classification		cs.CL	https://arxiv.org/pdf/1807.01704.pdf
805698	 Toxic online content has become a major issue in today's world due to an exponential increase in the use of internet by people of different cultures and educational background. Differentiating hate speech and offensive language is a key challenge in automatic detection of toxic text content. In this paper, we propose an approach to automatically classify tweets on Twitter into three classes: hateful, offensive and clean. Using Twitter dataset, we perform experiments considering n-grams as features and passing their term frequency-inverse document frequency (TFIDF) values to multiple machine learning models. We perform comparative analysis of the models considering several values of n in n-grams and TFIDF normalization methods. After tuning the model giving the best results, we achieve 95.6% accuracy upon evaluating it on test data. We also create a module which serves as an intermediate between user and Twitter. 	1809.08651	Aditya Gaydhani, Vikrant Doma, Shrikant Kendre, Laxmi Bhagwat	2018-09-23		4	1	Detecting Hate Speech and Offensive Language on Twitter using Machine Learning: An N-gram and TFIDF based Approach		cs.CL	https://arxiv.org/pdf/1809.08651.pdf
821677	 In this paper, we present TwiSent, a sentiment analysis system for Twitter. Based on the topic searched, TwiSent collects tweets pertaining to it and categorizes them into the different polarity classes positive, negative and objective. However, analyzing micro-blog posts have many inherent challenges compared to the other text genres. Through TwiSent, we address the problems of 1) Spams pertaining to sentiment analysis in Twitter, 2) Structural anomalies in the text in the form of incorrect spellings, nonstandard abbreviations, slangs etc., 3) Entity specificity in the context of the topic searched and 4) Pragmatics embedded in text. The system performance is evaluated on manually annotated gold standard data and on an automatically annotated tweet set based on hashtags. It is a common practise to show the efficacy of a supervised system on an automatically annotated dataset. However, we show that such a system achieves lesser classification accurcy when tested on generic twitter dataset. We also show that our system performs much better than an existing system. 	1209.2495	Subhabrata Mukherjee, Akshat Malu, A. R. Balamurali, Pushpak Bhattacharyya	2012-09-12		4	2	TwiSent: A Multistage System for Analyzing Sentiment in Twitter	2012-09-18	cs.IR, cs.CL	https://arxiv.org/pdf/1209.2495.pdf
853609	 Target-dependent sentiment classification remains a challenge: modeling the semantic relatedness of a target with its context words in a sentence. Different context words have different influences on determining the sentiment polarity of a sentence towards the target. Therefore, it is desirable to integrate the connections between target word and context words when building a learning system. In this paper, we develop two target dependent long short-term memory (LSTM) models, where target information is automatically taken into account. We evaluate our methods on a benchmark dataset from Twitter. Empirical results show that modeling sentence representation with standard LSTM does not perform well. Incorporating target information into LSTM can significantly boost the classification accuracy. The target-dependent LSTM models achieve state-of-the-art performances without using syntactic parser or external sentiment lexicons. 	1512.01100	Duyu Tang, Bing Qin, Xiaocheng Feng, Ting Liu	2015-12-03		4	1	Effective LSTMs for Target-Dependent Sentiment Classification	2016-09-29	cs.CL	https://arxiv.org/pdf/1512.01100.pdf
856027	 Hate speech detection on Twitter is critical for applications like controversial event extraction, building AI chatterbots, content recommendation, and sentiment analysis. We define this task as being able to classify a tweet as racist, sexist or neither. The complexity of the natural language constructs makes this task very challenging. We perform extensive experiments with multiple deep learning architectures to learn semantic word embeddings to handle this complexity. Our experiments on a benchmark dataset of 16K annotated tweets show that such deep learning methods outperform state-of-the-art char/word n-gram methods by ~18 F1 points. 	1706.00188	Pinkesh Badjatiya, Shashank Gupta, Manish Gupta, Vasudeva Varma	2017-06-01	10.1145/3041021.3054223	4	2	Deep Learning for Hate Speech Detection in Tweets		cs.CL, cs.IR	https://arxiv.org/pdf/1706.00188.pdf
892807	 This paper introduces a novel deep learning framework including a lexicon-based approach for sentence-level prediction of sentiment label distribution. We propose to first apply semantic rules and then use a Deep Convolutional Neural Network (DeepCNN) for character-level embeddings in order to increase information for word-level embedding. After that, a Bidirectional Long Short-Term Memory Network (Bi-LSTM) produces a sentence-wide feature representation from the word-level embedding. We evaluate our approach on three Twitter sentiment classification datasets. Experimental results show that our model can improve the classification accuracy of sentence-level sentiment analysis in Twitter social networking. 	1706.08032	Huy Nguyen, Minh-Le Nguyen	2017-06-25		2	1	A Deep Neural Architecture for Sentence-level Sentiment Classification in Twitter Social Networking		cs.CL	https://arxiv.org/pdf/1706.08032.pdf
910224	 Sentiment analysis (or opinion mining) on Twitter data has attracted much attention recently. One of the system's key features, is the immediacy in communication with other users in an easy, user-friendly and fast way. Consequently, people tend to express their feelings freely, which makes Twitter an ideal source for accumulating a vast amount of opinions towards a wide diversity of topics. This amount of information offers huge potential and can be harnessed to receive the sentiment tendency towards these topics. However, since none can invest an infinite amount of time to read through these tweets, an automated decision making approach is necessary. Nevertheless, most existing solutions are limited in centralized environments only. Thus, they can only process at most a few thousand tweets. Such a sample, is not representative to define the sentiment polarity towards a topic due to the massive number of tweets published daily. In this paper, we go one step further and develop a novel method for sentiment learning in the MapReduce framework. Our algorithm exploits the hashtags and emoticons inside a tweet, as sentiment labels, and proceeds to a classification procedure of diverse sentiment types in a parallel and distributed manner. Moreover, we utilize Bloom filters to compact the storage size of intermediate data and boost the performance of our algorithm. Through an extensive experimental evaluation, we prove that our solution is efficient, robust and scalable and confirm the quality of our sentiment identification. 	1602.01248	Nikolaos Nodarakis, Spyros Sioutas, Athanasios Tsakalidis, Giannis Tzimas	2016-02-03		4	3	Using Hadoop for Large Scale Analysis on Twitter: A Technical Report		cs.DB, cs.CL, cs.IR	https://arxiv.org/pdf/1602.01248.pdf
952467	 Mining social media messages for health and drug related information has received significant interest in pharmacovigilance research. Social media sites (e.g., Twitter), have been used for monitoring drug abuse, adverse reactions of drug usage and analyzing expression of sentiments related to drugs. Most of these studies are based on aggregated results from a large population rather than specific sets of individuals. In order to conduct studies at an individual level or specific cohorts, identifying posts mentioning intake of medicine by the user is necessary. Towards this objective, we train different deep neural network classification models on a publicly available annotated dataset and study their performances on identifying mentions of personal intake of medicine in tweets. We also design and train a new architecture of a stacked ensemble of shallow convolutional neural network (CNN) ensembles. We use random search for tuning the hyperparameters of the models and share the details of the values taken by the hyperparameters for the best learnt model in different deep neural network architectures. Our system produces state-of-the-art results, with a micro- averaged F-score of 0.693. 	1805.06375	Debanjan Mahata, Jasper Friedrichs, Hitkul, Rajiv Ratn Shah	2018-05-16		4	1	#phramacovigilance - Exploring Deep Learning Techniques for Identifying Mentions of Medication Intake from Twitter		cs.CL	https://arxiv.org/pdf/1805.06375.pdf
952720	" We show that information about social relationships can be used to improve user-level sentiment analysis. The main motivation behind our approach is that users that are somehow ""connected"" may be more likely to hold similar opinions; therefore, relationship information can complement what we can extract about a user's viewpoints from their utterances. Employing Twitter as a source for our experimental data, and working within a semi-supervised framework, we propose models that are induced either from the Twitter follower/followee network or from the network in Twitter formed by users referring to each other using ""@"" mentions. Our transductive learning results reveal that incorporating social-network information can indeed lead to statistically significant sentiment-classification improvements over the performance of an approach based on Support Vector Machines having access only to textual features. "	1109.6018	Chenhao Tan, Lillian Lee, Jie Tang, Long Jiang, Ming Zhou, Ping Li	2011-09-27		6	4	User-level sentiment analysis incorporating social networks		cs.CL, cs.IR, physics.data-an, physics.soc-ph	https://arxiv.org/pdf/1109.6018.pdf
958959	 Measuring public sentiment is a key task for researchers and policymakers alike. The explosion of available social media data allows for a more time-sensitive and geographically specific analysis than ever before. In this paper we analyze data from the micro-blogging site Twitter and generate a sentiment map of New York City. We develop a classifier specifically tuned for 140-character Twitter messages, or tweets, using key words, phrases and emoticons to determine the mood of each tweet. This method, combined with geotagging provided by users, enables us to gauge public sentiment on extremely fine-grained spatial and temporal scales. We find that public mood is generally highest in public parks and lowest at transportation hubs, and locate other areas of strong sentiment such as cemeteries, medical centers, a jail, and a sewage facility. Sentiment progressively improves with proximity to Times Square. Periodic patterns of sentiment fluctuate on both a daily and a weekly scale: more positive tweets are posted on weekends than on weekdays, with a daily peak in sentiment around midnight and a nadir between 9:00 a.m. and noon. 	1308.5010	Karla Z. Bertrand, Maya Bialik, Kawandeep Virdee, Andreas Gros, Yaneer Bar-Yam	2013-08-22		5	3	Sentiment in New York City: A High Resolution Spatial and Temporal View		physics.soc-ph, cs.CL, cs.CY	https://arxiv.org/pdf/1308.5010.pdf
977598	 In this paper , we tackle Sentiment Analysis conditioned on a Topic in Twitter data using Deep Learning . We propose a 2-tier approach : In the first phase we create our own Word Embeddings and see that they do perform better than state-of-the-art embeddings when used with standard classifiers. We then perform inference on these embeddings to learn more about a word with respect to all the topics being considered, and also the top n-influencing words for each topic. In the second phase we use these embeddings to predict the sentiment of the tweet with respect to a given topic, and all other topics under discussion. 	1710.10498	Sharath T. S., Shubhangi Tandon	2017-10-28		2	2	Topic Based Sentiment Analysis Using Deep Learning		cs.CL, cs.IR	https://arxiv.org/pdf/1710.10498.pdf
1097032	 Nowadays, geographic information related to Twitter is crucially important for fine-grained applications. However, the amount of geographic information avail- able on Twitter is low, which makes the pursuit of many applications challenging. Under such circumstances, estimating the location of a tweet is an important goal of the study. Unlike most previous studies that estimate the pre-defined district as the classification task, this study employs a probability distribution to represent richer information of the tweet, not only the location but also its ambiguity. To realize this modeling, we propose the convolutional mixture density network (CMDN), which uses text data to estimate the mixture model parameters. Experimentally obtained results reveal that CMDN achieved the highest prediction performance among the method for predicting the exact coordinates. It also provides a quantitative representation of the location ambiguity for each tweet that properly works for extracting the reliable location estimations. 	1705.02750	Hayate Iso, Shoko Wakamiya, Eiji Aramaki	2017-05-08		3	1	Density Estimation for Geolocation via Convolutional Mixture Density Network		cs.CL	https://arxiv.org/pdf/1705.02750.pdf
1111765	 Micro-blogging service Twitter is a lucrative source for data mining applications on global sentiment. But due to the omnifariousness of the subjects mentioned in each data item; it is inefficient to run a data mining algorithm on the raw data. This paper discusses an algorithm to accurately classify the entire stream in to a given number of mutually exclusive collectively exhaustive streams upon each of which the data mining algorithm can be run separately yielding more relevant results with a high efficiency. 	1705.09995	Nisansa de Silva, Danaja Maldeniya, Chamilka Wijeratne	2017-05-28		3	1	Subject Specific Stream Classification Preprocessing Algorithm for Twitter Data Stream		cs.CL	https://arxiv.org/pdf/1705.09995.pdf
1115746	 Though dialectal language is increasingly abundant on social media, few resources exist for developing NLP tools to handle such language. We conduct a case study of dialectal language in online conversational text by investigating African-American English (AAE) on Twitter. We propose a distantly supervised model to identify AAE-like language from demographics associated with geo-located messages, and we verify that this language follows well-known AAE linguistic phenomena. In addition, we analyze the quality of existing language identification and dependency parsing tools on AAE-like text, demonstrating that they perform poorly on such text compared to text associated with white speakers. We also provide an ensemble classifier for language identification which eliminates this disparity and release a new corpus of tweets containing AAE-like language. 	1608.08868	Su Lin Blodgett, Lisa Green, Brendan O'Connor	2016-08-31		3	1	Demographic Dialectal Variation in Social Media: A Case Study of African-American English		cs.CL	https://arxiv.org/pdf/1608.08868.pdf
1162303	 Deep neural networks have shown good data modelling capabilities when dealing with challenging and large datasets from a wide range of application areas. Convolutional Neural Networks (CNNs) offer advantages in selecting good features and Long Short-Term Memory (LSTM) networks have proven good abilities of learning sequential data. Both approaches have been reported to provide improved results in areas such image processing, voice recognition, language translation and other Natural Language Processing (NLP) tasks. Sentiment classification for short text messages from Twitter is a challenging task, and the complexity increases for Arabic language sentiment classification tasks because Arabic is a rich language in morphology. In addition, the availability of accurate pre-processing tools for Arabic is another current limitation, along with limited research available in this area. In this paper, we investigate the benefits of integrating CNNs and LSTMs and report obtained improved accuracy for Arabic sentiment analysis on different datasets. Additionally, we seek to consider the morphological diversity of particular Arabic words by using different sentiment classification levels. 	1807.02911	Abdulaziz M. Alayba, Vasile Palade, Matthew England, Rahat Iqbal	2018-07-08	10.1007/978-3-319-99740-7_12	4	1	A Combined CNN and LSTM Model for Arabic Sentiment Analysis	2018-07-21	cs.CL	https://arxiv.org/pdf/1807.02911.pdf
1212530	 Widespread use of social media has led to the generation of substantial amounts of information about individuals, including health-related information. Social media provides the opportunity to study health-related information about selected population groups who may be of interest for a particular study. In this paper, we explore the possibility of utilizing social media to perform targeted data collection and analysis from a particular population group -- pregnant women. We hypothesize that we can use social media to identify cohorts of pregnant women and follow them over time to analyze crucial health-related information. To identify potentially pregnant women, we employ simple rule-based searches that attempt to detect pregnancy announcements with moderate precision. To further filter out false positives and noise, we employ a supervised classifier using a small number of hand-annotated data. We then collect their posts over time to create longitudinal health timelines and attempt to divide the timelines into different pregnancy trimesters. Finally, we assess the usefulness of the timelines by performing a preliminary analysis to estimate drug intake patterns of our cohort at different trimesters. Our rule-based cohort identification technique collected 53,820 users over thirty months from Twitter. Our pregnancy announcement classification technique achieved an F-measure of 0.81 for the pregnancy class, resulting in 34,895 user timelines. Analysis of the timelines revealed that pertinent health-related information, such as drug-intake and adverse reactions can be mined from the data. Our approach to using user timelines in this fashion has produced very encouraging results and can be employed for other important tasks where cohorts, for which health-related information may not be available from other sources, are required to be followed over time to derive population-based estimates. 	1702.02261	Pramod Bharadwaj Chandrashekar, Arjun Magge, Abeed Sarker, Graciela Gonzalez	2017-02-07		4	1	Social media mining for identification and exploration of health-related information from pregnant women		cs.CL	https://arxiv.org/pdf/1702.02261.pdf
1250819	 Most of existing work learn sentiment-specific word representation for improving Twitter sentiment classification, which encoded both n-gram and distant supervised tweet sentiment information in learning process. They assume all words within a tweet have the same sentiment polarity as the whole tweet, which ignores the word its own sentiment polarity. To address this problem, we propose to learn sentiment-specific word embedding by exploiting both lexicon resource and distant supervised information. We develop a multi-level sentiment-enriched word embedding learning method, which uses parallel asymmetric neural network to model n-gram, word level sentiment and tweet level sentiment in learning process. Experiments on standard benchmarks show our approach outperforms state-of-the-art methods. 	1611.00126	Shufeng Xiong	2016-11-01	10.1016/j.neucom.2017.11.023	1	1	Improving Twitter Sentiment Classification via Multi-Level Sentiment-Enriched Word Embeddings		cs.CL	https://arxiv.org/pdf/1611.00126.pdf
1287723	 Suicide is among the leading causes of death in China. However, technical approaches toward preventing suicide are challenging and remaining under development. Recently, several actual suicidal cases were preceded by users who posted microblogs with suicidal ideation to Sina Weibo, a Chinese social media network akin to Twitter. It would therefore be desirable to detect suicidal ideations from microblogs in real-time, and immediately alert appropriate support groups, which may lead to successful prevention. In this paper, we propose a real-time suicidal ideation detection system deployed over Weibo, using machine learning and known psychological techniques. Currently, we have identified 53 known suicidal cases who posted suicide notes on Weibo prior to their deaths.We explore linguistic features of these known cases using a psychological lexicon dictionary, and train an effective suicidal Weibo post detection model. 6714 tagged posts and several classifiers are used to verify the model. By combining both machine learning and psychological knowledge, SVM classifier has the best performance of different classifiers, yielding an F-measure of 68:3%, a Precision of 78:9%, and a Recall of 60:3%. 	1411.0778	Xiaolei Huang, Lei Zhang, Tianli Liu, David Chiu, Tingshao Zhu, Xin Li	2014-11-03		6	1	Detecting Suicidal Ideation in Chinese Microblogs with Psychological Lexicons		cs.CL	https://arxiv.org/pdf/1411.0778.pdf
1288833	 A major challenge in paraphrase research is the lack of parallel corpora. In this paper, we present a new method to collect large-scale sentential paraphrases from Twitter by linking tweets through shared URLs. The main advantage of our method is its simplicity, as it gets rid of the classifier or human in the loop needed to select data before annotation and subsequent application of paraphrase identification algorithms in the previous work. We present the largest human-labeled paraphrase corpus to date of 51,524 sentence pairs and the first cross-domain benchmarking for automatic paraphrase identification. In addition, we show that more than 30,000 new sentential paraphrases can be easily and continuously captured every month at ~70% precision, and demonstrate their utility for downstream NLP tasks through phrasal paraphrase extraction. We make our code and data freely available. 	1708.00391	Wuwei Lan, Siyu Qiu, Hua He, Wei Xu	2017-08-01		4	1	A Continuously Growing Dataset of Sentential Paraphrases		cs.CL	https://arxiv.org/pdf/1708.00391.pdf
1415917	 Use of social media has grown dramatically during the last few years. Users follow informal languages in communicating through social media. The language of communication is often mixed in nature, where people transcribe their regional language with English and this technique is found to be extremely popular. Natural language processing (NLP) aims to infer the information from these text where Part-of-Speech (PoS) tagging plays an important role in getting the prosody of the written text. For the task of PoS tagging on Code-Mixed Indian Social Media Text, we develop a supervised system based on Conditional Random Field classifier. In order to tackle the problem effectively, we have focused on extracting rich linguistic features. We participate in three different language pairs, ie. English-Hindi, English-Bengali and English-Telugu on three different social media platforms, Twitter, Facebook & WhatsApp. The proposed system is able to successfully assign coarse as well as fine-grained PoS tag labels for a given a code-mixed sentence. Experiments show that our system is quite generic that shows encouraging performance levels on all the three language pairs in all the domains. 	1702.00167	Deepak Gupta, Shubham Tripathi, Asif Ekbal, Pushpak Bhattacharyya	2017-02-01		4	1	SMPOST: Parts of Speech Tagger for Code-Mixed Indic Social Media Text	2017-02-02	cs.CL	https://arxiv.org/pdf/1702.00167.pdf
400088	 In this paper, we attempt to classify tweets into root categories of the Amazon browse node hierarchy using a set of tweets with browse node ID labels, a much larger set of tweets without labels, and a set of Amazon reviews. Examining twitter data presents unique challenges in that the samples are short (under 140 characters) and often contain misspellings or abbreviations that are trivial for a human to decipher but difficult for a computer to parse. A variety of query and document expansion techniques are implemented in an effort to improve information retrieval to modest success. 	1511.08299	Matthew Long, Aditya Jami, Ashutosh Saxena	2015-11-26		3	4	Hierarchical classification of e-commerce related social media		cs.SI, cs.CL, cs.IR, cs.LG	https://arxiv.org/pdf/1511.08299.pdf
1400722	 Various modifications of decision trees have been extensively used during the past years due to their high efficiency and interpretability. Tree node splitting based on relevant feature selection is a key step of decision tree learning, at the same time being their major shortcoming: the recursive nodes partitioning leads to geometric reduction of data quantity in the leaf nodes, which causes an excessive model complexity and data overfitting. In this paper, we present a novel architecture - a Decision Stream, - aimed to overcome this problem. Instead of building a tree structure during the learning process, we propose merging nodes from different branches based on their similarity that is estimated with two-sample test statistics, which leads to generation of a deep directed acyclic graph of decision rules that can consist of hundreds of levels. To evaluate the proposed solution, we test it on several common machine learning problems - credit scoring, twitter sentiment analysis, aircraft flight control, MNIST and CIFAR image classification, synthetic data classification and regression. Our experimental results reveal that the proposed approach significantly outperforms the standard decision tree learning methods on both regression and classification tasks, yielding a prediction error decrease up to 35%. 	1704.07657	Dmitry Ignatov, Andrey Ignatov	2017-04-25		2	1	Decision Stream: Cultivating Deep Decision Trees	2017-09-03	cs.LG	https://arxiv.org/pdf/1704.07657.pdf
337860	 News articles are extremely time sensitive by nature. There is also intense competition among news items to propagate as widely as possible. Hence, the task of predicting the popularity of news items on the social web is both interesting and challenging. Prior research has dealt with predicting eventual online popularity based on early popularity. It is most desirable, however, to predict the popularity of items prior to their release, fostering the possibility of appropriate decision making to modify an article and the manner of its publication. In this paper, we construct a multi-dimensional feature space derived from properties of an article and evaluate the efficacy of these features to serve as predictors of online popularity. We examine both regression and classification algorithms and demonstrate that despite randomness in human behavior, it is possible to predict ranges of popularity on twitter with an overall 84% accuracy. Our study also serves to illustrate the differences between traditionally prominent sources and those immensely popular on the social web. 	1202.0332	Roja Bandari, Sitaram Asur, Bernardo A. Huberman	2012-02-01		3	4	The Pulse of News in Social Media: Forecasting Popularity		cs.CY, cs.NI, cs.SI, physics.soc-ph	https://arxiv.org/pdf/1202.0332.pdf
1204436	 With the acceptance of Western culture and science, Traditional Chinese Medicine (TCM) has become a controversial issue in China. So, it's important to study the public's sentiment and opinion on TCM. The rapid development of online social network, such as twitter, make it convenient and efficient to sample hundreds of millions of people for the aforementioned sentiment study. To the best of our knowledge, the present work is the first attempt that applies sentiment analysis to the domain of TCM on Sina Weibo (a twitter-like microblogging service in China). In our work, firstly we collect tweets topic about TCM from Sina Weibo, and label the tweets as supporting TCM and opposing TCM automatically based on user tag. Then, a support vector machine classifier has been built to predict the sentiment of TCM tweets without labels. Finally, we present a method to adjust the classifier result. The performance of F-measure attained with our method is 97%. 	1410.3460	Junhui Shen, Peiyan Zhu, Rui Fan, Wei Tan	2014-10-13		4	2	Sentiment Analysis based on User Tag for Traditional Chinese Medicine in Weibo		cs.CL, cs.SI	https://arxiv.org/pdf/1410.3460.pdf
612816	 In this work we address the issue of generic automated disease incidence monitoring on twitter. We employ an ontology of disease related concepts and use it to obtain a conceptual representation of tweets. Unlike previous key word based systems and topic modeling approaches, our ontological approach allows us to apply more stringent criteria for determining which messages are relevant such as spatial and temporal characteristics whilst giving a stronger guarantee that the resulting models will perform well on new data that may be lexically divergent. We achieve this by training learners on concepts rather than individual words. For training we use a dataset containing mentions of influenza and Listeria and use the learned models to classify datasets containing mentions of an arbitrary selection of other diseases. We show that our ontological approach achieves good performance on this task using a variety of Natural Language Processing Techniques. We also show that word vectors can be learned directly from our concepts to achieve even better results. 	1611.06671	Mark Abraham Magumba, Peter Nabende	2016-11-21		2	2	Ontology Driven Disease Incidence Detection on Twitter		cs.CL, cs.IR	https://arxiv.org/pdf/1611.06671.pdf
961424	 Social media platforms like twitter and facebook have be- come two of the largest mediums used by people to express their views to- wards different topics. Generation of such large user data has made NLP tasks like sentiment analysis and opinion mining much more important. Using sarcasm in texts on social media has become a popular trend lately. Using sarcasm reverses the meaning and polarity of what is implied by the text which poses challenge for many NLP tasks. The task of sarcasm detection in text is gaining more and more importance for both commer- cial and security services. We present the first English-Hindi code-mixed dataset of tweets marked for presence of sarcasm and irony where each token is also annotated with a language tag. We present a baseline su- pervised classification system developed using the same dataset which achieves an average F-score of 78.4 after using random forest classifier and performing 10-fold cross validation. 	1805.11869	Sahil Swami, Ankush Khandelwal, Vinay Singh, Syed Sarfaraz Akhtar, Manish Shrivastava	2018-05-30		5	1	A Corpus of English-Hindi Code-Mixed Tweets for Sarcasm Detection		cs.CL	https://arxiv.org/pdf/1805.11869.pdf
973363	 As humans, we can often detect from a persons utterances if he or she is in favor of or against a given target entity (topic, product, another person, etc). But from the perspective of a computer, we need means to automatically deduce the stance of the tweeter, given just the tweet text. In this paper, we present our results of performing stance detection on twitter data using a supervised approach. We begin by extracting bag-of-words to perform classification using TIMBL, then try and optimize the features to improve stance detection accuracy, followed by extending the dataset with two sets of lexicons - arguing, and MPQA subjectivity; next we explore the MALT parser and construct features using its dependency triples, finally we perform analysis using Scikit-learn Random Forest implementation. 	1703.02019	Gourav G. Shenoy, Erika H. Dsouza, Sandra Kübler	2017-03-06		3	1	Performing Stance Detection on Twitter Data using Computational Linguistics Techniques		cs.CL	https://arxiv.org/pdf/1703.02019.pdf
996826	 Stance classification aims to identify, for a particular issue under discussion, whether the speaker or author of a conversational turn has Pro (Favor) or Con (Against) stance on the issue. Detecting stance in tweets is a new task proposed for SemEval-2016 Task6, involving predicting stance for a dataset of tweets on the topics of abortion, atheism, climate change, feminism and Hillary Clinton. Given the small size of the dataset, our team created our own topic-specific training corpus by developing a set of high precision hashtags for each topic that were used to query the twitter API, with the aim of developing a large training corpus without additional human labeling of tweets for stance. The hashtags selected for each topic were predicted to be stance-bearing on their own. Experimental results demonstrate good performance for our features for opinion-target pairs based on generalizing dependency features using sentiment lexicons. 	1709.01895	Amita Misra, Brian Ecker, Theodore Handleman, Nicolas Hahn, Marilyn Walker	2017-09-03	10.18653/v1/s16-1068	5	1	A Semi-Supervised Approach to Detecting Stance in Tweets		cs.CL	https://arxiv.org/pdf/1709.01895.pdf
1289986	 Harmful speech has various forms and it has been plaguing the social media in different ways. If we need to crackdown different degrees of hate speech and abusive behavior amongst it, the classification needs to be based on complex ramifications which needs to be defined and hold accountable for, other than racist, sexist or against some particular group and community. This paper primarily describes how we created an ontological classification of harmful speech based on degree of hateful intent, and used it to annotate twitter data accordingly. The key contribution of this paper is the new dataset of tweets we created based on ontological classes and degrees of harmful speech found in the text. We also propose supervised classification system for recognizing these respective harmful speech classes in the texts hence. 	1806.04197	Sanjana Sharma, Saksham Agrawal, Manish Shrivastava	2018-06-11		3	1	Degree based Classification of Harmful Speech using Twitter Data		cs.CL	https://arxiv.org/pdf/1806.04197.pdf
292929	 Complex networks are often categorized according to the underlying phenomena that they represent such as molecular interactions, re-tweets, and brain activity. In this work, we investigate the problem of predicting the category (domain) of arbitrary networks. This includes complex networks from different domains as well as synthetically generated graphs from five different network models. A classification accuracy of $96.6\%$ is achieved using a random forest classifier with both real and synthetic networks. This work makes two important findings. First, our results indicate that complex networks from various domains have distinct structural properties that allow us to predict with high accuracy the category of a new previously unseen network. Second, synthetic graphs are trivial to classify as the classification model can predict with near-certainty the network model used to generate it. Overall, the results demonstrate that networks drawn from different domains (and network models) are trivial to distinguish using only a handful of simple structural properties. 	1805.02682	James P. Canning, Emma E. Ingram, Sammantha Nowak-Wolff, Adriana M. Ortiz, Nesreen K. Ahmed, Ryan A. Rossi, Karl R. B. Schmitt, Sucheta Soundarajan	2018-05-07		8	3	Predicting Graph Categories from Structural Properties		cs.SI, cs.AI, stat.ML	https://arxiv.org/pdf/1805.02682.pdf
671707	 Health related social media mining is a valuable apparatus for the early recognition of the diverse antagonistic medicinal conditions. Mostly, the existing methods are based on machine learning with knowledge-based learning. This working note presents the Recurrent neural network (RNN) and Long short-term memory (LSTM) based embedding for automatic health text classification in the social media mining. For each task, two systems are built and that classify the tweet at the tweet level. RNN and LSTM are used for extracting features and non-linear activation function at the last layer facilitates to distinguish the tweets of different categories. The experiments are conducted on 2nd Social Media Mining for Health Applications Shared Task at AMIA 2017. The experiment results are considerable; however the proposed method is appropriate for the health text classification. This is primarily due to the reason that, it doesn't rely on any feature engineering mechanisms. 	1710.08396	Vinayakumar R, Barathi Ganesh HB, Anand Kumar M, Soman KP	2017-10-23		4	2	Deep Health Care Text Classification		cs.CL, cs.AI	https://arxiv.org/pdf/1710.08396.pdf
736614	 In this era of digitization, knowing the user's sociolect aspects have become essential features to build the user specific recommendation systems. These sociolect aspects could be found by mining the user's language sharing in the form of text in social media and reviews. This paper describes about the experiment that was performed in PAN Author Profiling 2017 shared task. The objective of the task is to find the sociolect aspects of the users from their tweets. The sociolect aspects considered in this experiment are user's gender and native language information. Here user's tweets written in a different language from their native language are represented as Document - Term Matrix with document frequency as the constraint. Further classification is done using the Support Vector Machine by taking gender and native language as target classes. This experiment attains the average accuracy of 73.42% in gender prediction and 76.26% in the native language identification task. 	1708.06068	Barathi Ganesh HB, Anand Kumar M, Soman KP	2017-08-20		3	3	Vector Space Model as Cognitive Space for Text Classification		cs.CL, cs.AI, cs.SI	https://arxiv.org/pdf/1708.06068.pdf
907899	 A plethora of words are used to describe the spectrum of human emotions, but how many emotions are there really, and how do they interact? Over the past few decades, several theories of emotion have been proposed, each based around the existence of a set of 'basic emotions', and each supported by an extensive variety of research including studies in facial expression, ethology, neurology and physiology. Here we present research based on a theory that people transmit their understanding of emotions through the language they use surrounding emotion keywords. Using a labelled corpus of over 21,000 tweets, six of the basic emotion sets proposed in existing literature were analysed using Latent Semantic Clustering (LSC), evaluating the distinctiveness of the semantic meaning attached to the emotional label. We hypothesise that the more distinct the language is used to express a certain emotion, then the more distinct the perception (including proprioception) of that emotion is, and thus more 'basic'. This allows us to select the dimensions best representing the entire spectrum of emotion. We find that Ekman's set, arguably the most frequently used for classifying emotions, is in fact the most semantically distinct overall. Next, taking all analysed (that is, previously proposed) emotion terms into account, we determine the optimal semantically irreducible basic emotion set using an iterative LSC algorithm. Our newly-derived set (Accepting, Ashamed, Contempt, Interested, Joyful, Pleased, Sleepy, Stressed) generates a 6.1% increase in distinctiveness over Ekman's set (Angry, Disgusted, Joyful, Sad, Scared). We also demonstrate how using LSC data can help visualise emotions. We introduce the concept of an Emotion Profile and briefly analyse compound emotions both visually and mathematically. 	1212.6527	Eugene Yuta Bann	2012-12-28		1	2	Discovering Basic Emotion Sets via Semantic Clustering on a Twitter Corpus		cs.AI, cs.CL	https://arxiv.org/pdf/1212.6527.pdf
1004421	 While humor has been historically studied from a psychological, cognitive and linguistic standpoint, its study from a computational perspective is an area yet to be explored in Computational Linguistics. There exist some previous works, but a characterization of humor that allows its automatic recognition and generation is far from being specified. In this work we build a crowdsourced corpus of labeled tweets, annotated according to its humor value, letting the annotators subjectively decide which are humorous. A humor classifier for Spanish tweets is assembled based on supervised learning, reaching a precision of 84% and a recall of 69%. 	1703.09527	Santiago Castro, Matías Cubero, Diego Garat, Guillermo Moncecchi	2017-03-28	10.1007/978-3-319-47955-2_12	4	2	Is This a Joke? Detecting Humor in Spanish Tweets		cs.CL, cs.AI	https://arxiv.org/pdf/1703.09527.pdf
92573	 Since the events of the Arab Spring, there has been increased interest in using social media to anticipate social unrest. While efforts have been made toward automated unrest prediction, we focus on filtering the vast volume of tweets to identify tweets relevant to unrest, which can be provided to downstream users for further analysis. We train a supervised classifier that is able to label Arabic language tweets as relevant to unrest with high reliability. We examine the relationship between training data size and performance and investigate ways to optimize the model building process while minimizing cost. We also explore how confidence thresholds can be set to achieve desired levels of performance. 	1702.06216	Alan Mishler, Kevin Wonus, Wendy Chambers, Michael Bloodgood	2017-02-20	10.1109/ICSC.2017.75	4	4	Filtering Tweets for Social Unrest	2017-04-01	cs.CL, cs.IR, cs.LG, stat.ML	https://arxiv.org/pdf/1702.06216.pdf
416277	 Recently a lot of progress has been made in rumor modeling and rumor detection for micro-blogging streams. However, existing automated methods do not perform very well for early rumor detection, which is crucial in many settings, e.g., in crisis situations. One reason for this is that aggregated rumor features such as propagation features, which work well on the long run, are - due to their accumulating characteristic - not very helpful in the early phase of a rumor. In this work, we present an approach for early rumor detection, which leverages Convolutional Neural Networks for learning the hidden representations of individual rumor-related tweets to gain insights on the credibility of each tweets. We then aggregate the predictions from the very beginning of a rumor to obtain the overall event credits (so-called wisdom), and finally combine it with a time series based rumor classification model. Our extensive experiments show a clearly improved classification performance within the critical very first hours of a rumor. For a better understanding, we also conduct an extensive feature evaluation that emphasized on the early stage and shows that the low-level credibility has best predictability at all phases of the rumor lifetime. 	1709.04402	Tu Ngoc Nguyen, Cheng Li, Claudia Niederée	2017-09-13		3	2	On Early-stage Debunking Rumors on Twitter: Leveraging the Wisdom of Weak Learners		cs.SI, cs.LG	https://arxiv.org/pdf/1709.04402.pdf
581287	 Sentiment analysis or opinion mining has become an open research domain after proliferation of Internet and Web 2.0 social media. People express their attitudes and opinions on social media including blogs, discussion forums, tweets, etc. and, sentiment analysis concerns about detecting and extracting sentiment or opinion from online text. Sentiment based text classification is different from topical text classification since it involves discrimination based on expressed opinion on a topic. Feature selection is significant for sentiment analysis as the opinionated text may have high dimensions, which can adversely affect the performance of sentiment analysis classifier. This paper explores applicability of feature selection methods for sentiment analysis and investigates their performance for classification in term of recall, precision and accuracy. Five feature selection methods (Document Frequency, Information Gain, Gain Ratio, Chi Squared, and Relief-F) and three popular sentiment feature lexicons (HM, GI and Opinion Lexicon) are investigated on movie reviews corpus with a size of 2000 documents. The experimental results show that Information Gain gave consistent results and Gain Ratio performs overall best for sentimental feature selection while sentiment lexicons gave poor performance. Furthermore, we found that performance of the classifier depends on appropriate number of representative feature selected from text. 	1309.3949	Anuj sharma, Shubhamoy Dey	2013-09-16		2	3	Performance Investigation of Feature Selection Methods		cs.IR, cs.CL, cs.LG	https://arxiv.org/pdf/1309.3949.pdf
750428	" Applications that learn from opinionated documents, like tweets or product reviews, face two challenges. First, the opinionated documents constitute an evolving stream, where both the author's attitude and the vocabulary itself may change. Second, labels of documents are scarce and labels of words are unreliable, because the sentiment of a word depends on the (unknown) context in the author's mind. Most of the research on mining over opinionated streams focuses on the first aspect of the problem, whereas for the second a continuous supply of labels from the stream is assumed. Such an assumption though is utopian as the stream is infinite and the labeling cost is prohibitive. To this end, we investigate the potential of active stream learning algorithms that ask for labels on demand. Our proposed ACOSTREAM 1 approach works with limited labels: it uses an initial seed of labeled documents, occasionally requests additional labels for documents from the human expert and incrementally adapts to the underlying stream while exploiting the available labeled documents. In its core, ACOSTREAM consists of a MNB classifier coupled with ""sampling"" strategies for requesting class labels for new unlabeled documents. In the experiments, we evaluate the classifier performance over time by varying: (a) the class distribution of the opinionated stream, while assuming that the set of the words in the vocabulary is fixed but their polarities may change with the class distribution; and (b) the number of unknown words arriving at each moment, while the class polarity may also change. Our results show that active learning on a stream of opinionated documents, delivers good performance while requiring a small selection of labels "	1509.01288	Max Zimmermann, Eirini Ntoutsi, Myra Spiliopoulou	2015-09-03		3	3	Incremental Active Opinion Learning Over a Stream of Opinionated Documents		cs.IR, cs.CL, cs.LG	https://arxiv.org/pdf/1509.01288.pdf
1005506	 Social media is a rich source of rumours and corresponding community reactions. Rumours reflect different characteristics, some shared and some individual. We formulate the problem of classifying tweet level judgements of rumours as a supervised learning task. Both supervised and unsupervised domain adaptation are considered, in which tweets from a rumour are classified on the basis of other annotated rumours. We demonstrate how multi-task learning helps achieve good results on rumours from the 2011 England riots. 	1506.00468	Michal Lukasik, Trevor Cohn, Kalina Bontcheva	2015-06-01		3	3	Classifying Tweet Level Judgements of Rumours in Social Media	2015-09-10	cs.SI, cs.CL, cs.LG	https://arxiv.org/pdf/1506.00468.pdf
1117750	 Recent work have done a good job in modeling rumors and detecting them over microblog streams. However, the performance of their automatic approaches are not relatively high when looking early in the diffusion. A first intuition is that, at early stage, most of the aggregated rumor features (e.g., propagation features) are not mature and distinctive enough. The objective of rumor debunking in microblogs, however, are to detect these misinformation as early as possible. In this work, we leverage neural models in learning the hidden representations of individual rumor-related tweets at the very beginning of a rumor. Our extensive experiments show that the resulting signal improves our classification performance over time, significantly within the first 10 hours. To deepen the understanding of these low and high-level features in contributing to the model performance over time, we conduct an extensive study on a wide range of high impact rumor features for the 48 hours range. The end model that engages these features are shown to be competitive, reaches over 90% accuracy and out-performs strong baselines in our carefully cured dataset. 	1711.00726	Tu Ngoc Nguyen	2017-11-02		1	2	A Comprehensive Low and High-level Feature Analysis for Early Rumor Detection on Twitter	2018-09-06	cs.SI, cs.LG	https://arxiv.org/pdf/1711.00726.pdf
1225080	 Task-specific word identification aims to choose the task-related words that best describe a short text. Existing approaches require well-defined seed words or lexical dictionaries (e.g., WordNet), which are often unavailable for many applications such as social discrimination detection and fake review detection. However, we often have a set of labeled short texts where each short text has a task-related class label, e.g., discriminatory or non-discriminatory, specified by users or learned by classification algorithms. In this paper, we focus on identifying task-specific words and phrases from short texts by exploiting their class labels rather than using seed words or lexical dictionaries. We consider the task-specific word and phrase identification as feature learning. We train a convolutional neural network over a set of labeled texts and use score vectors to localize the task-specific words and phrases. Experimental results on sentiment word identification show that our approach significantly outperforms existing methods. We further conduct two case studies to show the effectiveness of our approach. One case study on a crawled tweets dataset demonstrates that our approach can successfully capture the discrimination-related words/phrases. The other case study on fake review detection shows that our approach can identify the fake-review words/phrases. 	1706.00884	Shuhan Yuan, Xintao Wu, Yang Xiang	2017-06-02		3	3	Task-specific Word Identification from Short Texts Using a Convolutional Neural Network		cs.CL, cs.IR, cs.LG	https://arxiv.org/pdf/1706.00884.pdf
43193	 The automatic content analysis of mass media in the social sciences has become necessary and possible with the raise of social media and computational power. One particularly promising avenue of research concerns the use of sentiment analysis in microblog streams. However, one of the main challenges consists in aggregating sentiment polarity in a timely fashion that can be fed to the prediction method. We investigated a large set of sentiment aggregate functions and performed a regression analysis using political opinion polls as gold standard. Our dataset contains nearly 233 000 tweets, classified according to their polarity (positive, negative or neutral), regarding the five main Portuguese political leaders during the Portuguese bailout (2011-2014). Results show that different sentiment aggregate functions exhibit different feature importance over time while the error keeps almost unchanged. 	1606.05242	Pedro Saleiro, Luís Gomes, Carlos Soares	2016-06-16		3	1	Sentiment Aggregate Functions for Political Opinion Polling using Microblog Streams		cs.SI	https://arxiv.org/pdf/1606.05242.pdf
57879	 In the last years researchers in the field of intelligent transportation systems have made several efforts to extract valuable information from social media streams. However, collecting domain-specific data from any social media is a challenging task demanding appropriate and robust classification methods. In this work we focus on exploring geo-located tweets in order to create a travel-related tweet classifier using a combination of bag-of-words and word embeddings. The resulting classification makes possible the identification of interesting spatio-temporal relations in S\~ao Paulo and Rio de Janeiro. 	1706.05090	João Pereira, Arian Pasquali, Pedro Saleiro, Rosaldo Rossetti	2017-06-15		4	2	Transportation in Social Media: an automatic classifier for travel-related tweets		cs.CY, cs.SI	https://arxiv.org/pdf/1706.05090.pdf
103284	 Rumour stance classification, defined as classifying the stance of specific social media posts into one of supporting, denying, querying or commenting on an earlier post, is becoming of increasing interest to researchers. While most previous work has focused on using individual tweets as classifier inputs, here we report on the performance of sequential classifiers that exploit the discourse features inherent in social media interactions or 'conversational threads'. Testing the effectiveness of four sequential classifiers -- Hawkes Processes, Linear-Chain Conditional Random Fields (Linear CRF), Tree-Structured Conditional Random Fields (Tree CRF) and Long Short Term Memory networks (LSTM) -- on eight datasets associated with breaking news stories, and looking at different types of local and contextual features, our work sheds new light on the development of accurate stance classifiers. We show that sequential classifiers that exploit the use of discourse properties in social media conversations while using only local features, outperform non-sequential classifiers. Furthermore, we show that LSTM using a reduced set of features can outperform the other sequential classifiers; this performance is consistent across datasets and across types of stances. To conclude, our work also analyses the different features under study, identifying those that best help characterise and distinguish between stances, such as supporting tweets being more likely to be accompanied by evidence than denying tweets. We also set forth a number of directions for future research. 	1712.02223	Arkaitz Zubiaga, Elena Kochkina, Maria Liakata, Rob Procter, Michal Lukasik, Kalina Bontcheva, Trevor Cohn, Isabelle Augenstein	2017-12-06	10.1016/j.ipm.2017.11.009	8	2	Discourse-Aware Rumour Stance Classification in Social Media Using Sequential Classifiers		cs.CL, cs.SI	https://arxiv.org/pdf/1712.02223.pdf
218256	 Social media has become an indispensable part of the everyday lives of millions of people around the world. It provides a platform for expressing opinions and beliefs, communicated to a massive audience. However, this ease with which people can express themselves has also allowed for the large scale spread of propaganda and hate speech. To prevent violating the abuse policies of social media platforms and also to avoid detection by automatic systems like Google's Conversation AI, racists have begun to use a code (a movement termed Operation Google). This involves substituting references to communities by benign words that seem out of context, in hate filled posts or Tweets. For example, users have used the words Googles and Bings to represent the African-American and Asian communities, respectively. By generating the list of users who post such content, we move a step forward from classifying tweets by allowing us to study the usage pattern of these concentrated set of users. 	1703.05443	Rijul Magu, Kshitij Joshi, Jiebo Luo	2017-03-15		3	1	Detecting the Hate Code on Social Media		cs.SI	https://arxiv.org/pdf/1703.05443.pdf
295021	 In contrast to much previous work that has focused on location classification of tweets restricted to a specific country, here we undertake the task in a broader context by classifying global tweets at the country level, which is so far unexplored in a real-time scenario. We analyse the extent to which a tweet's country of origin can be determined by making use of eight tweet-inherent features for classification. Furthermore, we use two datasets, collected a year apart from each other, to analyse the extent to which a model trained from historical tweets can still be leveraged for classification of new tweets. With classification experiments on all 217 countries in our datasets, as well as on the top 25 countries, we offer some insights into the best use of tweet-inherent features for an accurate country-level classification of tweets. We find that the use of a single feature, such as the use of tweet content alone -- the most widely used feature in previous work -- leaves much to be desired. Choosing an appropriate combination of both tweet content and metadata can actually lead to substantial improvements of between 20\% and 50\%. We observe that tweet content, the user's self-reported location and the user's real name, all of which are inherent in a tweet and available in a real-time scenario, are particularly useful to determine the country of origin. We also experiment on the applicability of a model trained on historical tweets to classify new tweets, finding that the choice of a particular combination of features whose utility does not fade over time can actually lead to comparable performance, avoiding the need to retrain. However, the difficulty of achieving accurate classification increases slightly for countries with multiple commonalities, especially for English and Spanish speaking countries. 	1604.07236	Arkaitz Zubiaga, Alex Voss, Rob Procter, Maria Liakata, Bo Wang, Adam Tsakalidis	2016-04-25		6	3	Towards Real-Time, Country-Level Location Classification of Worldwide Tweets	2017-04-25	cs.IR, cs.CL, cs.SI	https://arxiv.org/pdf/1604.07236.pdf
331004	 We develop a novel visual model which can recognize protesters, describe their activities by visual attributes and estimate the level of perceived violence in an image. Studies of social media and protests use natural language processing to track how individuals use hashtags and links, often with a focus on those items' diffusion. These approaches, however, may not be effective in fully characterizing actual real-world protests (e.g., violent or peaceful) or estimating the demographics of participants (e.g., age, gender, and race) and their emotions. Our system characterizes protests along these dimensions. We have collected geotagged tweets and their images from 2013-2017 and analyzed multiple major protest events in that period. A multi-task convolutional neural network is employed in order to automatically classify the presence of protesters in an image and predict its visual attributes, perceived violence and exhibited emotions. We also release the UCLA Protest Image Dataset, our novel dataset of 40,764 images (11,659 protest images and hard negatives) with various annotations of visual attributes and sentiments. Using this dataset, we train our model and demonstrate its effectiveness. We also present experimental results from various analysis on geotagged image data in several prevalent protest events. Our dataset will be made accessible at https://www.sscnet.ucla.edu/comm/jjoo/mm-protest/. 	1709.06204	Donghyeon Won, Zachary C. Steinert-Threlkeld, Jungseock Joo	2017-09-18		3	3	Protest Activity Detection and Perceived Violence Estimation from Social Media Images		cs.MM, cs.CV, cs.SI	https://arxiv.org/pdf/1709.06204.pdf
502594	 Research in social media analysis is experiencing a recent surge with a large number of works applying representation learning models to solve high-level syntactico-semantic tasks such as sentiment analysis, semantic textual similarity computation, hashtag prediction and so on. Although the performance of the representation learning models are better than the traditional baselines for the tasks, little is known about the core properties of a tweet encoded within the representations. Understanding these core properties would empower us in making generalizable conclusions about the quality of representations. Our work presented here constitutes the first step in opening the black-box of vector embedding for social media posts, with emphasis on tweets in particular. In order to understand the core properties encoded in a tweet representation, we evaluate the representations to estimate the extent to which it can model each of those properties such as tweet length, presence of words, hashtags, mentions, capitalization, and so on. This is done with the help of multiple classifiers which take the representation as input. Essentially, each classifier evaluates one of the syntactic or social properties which are arguably salient for a tweet. This is also the first holistic study on extensively analysing the ability to encode these properties for a wide variety of tweet representation models including the traditional unsupervised methods (BOW, LDA), unsupervised representation learning methods (Siamese CBOW, Tweet2Vec) as well as supervised methods (CNN, BLSTM). 	1611.04887	J Ganesh, Manish Gupta, Vasudeva Varma	2016-11-15		3	2	Interpreting the Syntactic and Social Elements of the Tweet Representations via Elementary Property Prediction Tasks		cs.CL, cs.SI	https://arxiv.org/pdf/1611.04887.pdf
530366	 Tourism is becoming a significant contributor to medium and long range travels in an increasingly globalized world. Leisure traveling has an important impact on the local and global economy as well as on the environment. The study of touristic trips is thus raising a considerable interest. In this work, we apply a method to assess the attractiveness of 20 of the most popular touristic sites worldwide using geolocated tweets as a proxy for human mobility. We first rank the touristic sites based on the spatial distribution of the visitors' place of residence. The Taj Mahal, the Pisa Tower and the Eiffel Tower appear consistently in the top 5 in these rankings. We then pass to a coarser scale and classify the travelers by country of residence. Touristic site's visiting figures are then studied by country of residence showing that the Eiffel Tower, Times Square and the London Tower welcome the majority of the visitors of each country. Finally, we build a network linking sites whenever a user has been detected in more than one site. This allow us to unveil relations between touristic sites and find which ones are more tightly interconnected. 	1601.07741	Aleix Bassolas, Maxime Lenormand, Antònia Tugores, Bruno Gonçalves, José J. Ramasco	2016-01-28	10.1140/epjds/s13688-016-0073-5	5	2	Touristic site attractiveness seen through Twitter	2016-03-25	physics.soc-ph, cs.SI	https://arxiv.org/pdf/1601.07741.pdf
851958	 Sentiment analysis on social media data such as tweets and weibo has become a very important and challenging task. Due to the intrinsic properties of such data, tweets are short, noisy, and of divergent topics, and sentiment classification on these data requires to modeling various contexts such as the retweet/reply history of a tweet, and the social context about authors and relationships. While few prior study has approached the issue of modeling contexts in tweet, this paper proposes to use a hierarchical LSTM to model rich contexts in tweet, particularly long-range context. Experimental results show that contexts can help us to perform sentiment classification remarkably better. 	1605.01478	Minlie Huang, Yujie Cao, Chao Dong	2016-05-04		3	3	Modeling Rich Contexts for Sentiment Classification with LSTM		cs.CL, cs.IR, cs.SI	https://arxiv.org/pdf/1605.01478.pdf
922501	 The purpose of this study was to do a dataset distribution analysis, a classification performance analysis, and a topical analysis concerning what people are tweeting about four disease characteristics: symptoms, transmission, prevention, and treatment. A combination of natural language processing and machine learning techniques were used to determine what people are tweeting about Zika. Specifically, a two-stage classifier system was built to find relevant tweets on Zika, and then categorize these into the four disease categories. Tweets in each disease category were then examined using latent dirichlet allocation (LDA) to determine the five main tweet topics for each disease characteristic. Results 1,234,605 tweets were collected. Tweets by males and females were similar (28% and 23% respectively). The classifier performed well on the training and test data for relevancy (F=0.87 and 0.99 respectively) and disease characteristics (F=0.79 and 0.90 respectively). Five topics for each category were found and discussed with a focus on the symptoms category. Through this process, we demonstrate how misinformation can be discovered so that public health officials can respond to the tweets with misinformation. 	1701.07490	Michele Miller, Dr. Tanvi Banerjee, RoopTeja Muppalla, Dr. William Romine, Dr. Amit Sheth	2017-01-17		5	2	What Are People Tweeting about Zika? An Exploratory Study Concerning Symptoms, Treatment, Transmission, and Prevention		cs.SI, q-bio.OT	https://arxiv.org/pdf/1701.07490.pdf
964683	 In this paper we present a method to identify tweets that a user may find interesting enough to retweet. The method is based on a global, but personalized classifier, which is trained on data from several users, represented in terms of user-specific features. Thus, the method is trained on a sufficient volume of data, while also being able to make personalized decisions, i.e., the same post received by two different users may lead to different classification decisions. Experimenting with a collection of approx.\ 130K tweets received by 122 journalists, we train a logistic regression classifier, using a wide variety of features: the content of each tweet, its novelty, its text similarity to tweets previously posted or retweeted by the recipient or sender of the tweet, the network influence of the author and sender, and their past interactions. Our system obtains F1 approx. 0.9 using only 10 features and 5K training instances. 	1709.06518	Michail Vougioukas, Ion Androutsopoulos, Georgios Paliouras	2017-08-21		3	1	Identifying Retweetable Tweets with a Personalized Global Classifier		cs.SI	https://arxiv.org/pdf/1709.06518.pdf
975285	 We demonstrate the power of data mining techniques for the analysis of collective social dynamics within British Tweets during the Olympic Games 2012. The classification accuracy of online activities related to the successes of British athletes significantly improved when emotional components of tweets were taken into account, but employing emotional variables for activity prediction decreased the classifiers' quality. The approach could be easily adopted for any prediction or classification study with a set of problem-specific variables. 	1412.7184	Jan Chołoniewski, Julian Sienkiewicz, Janusz A. Hołyst, Mike Thelwall	2014-12-11		4	2	The role of emotional variables in the classification and prediction of collective social dynamics		cs.SI, physics.soc-ph	https://arxiv.org/pdf/1412.7184.pdf
1024394	" We introduce the notion of ""seminar users"", who are social media users engaged in propaganda in support of a political entity. We develop a framework that can identify such users with 84.4% precision and 76.1% recall. While our dataset is from the Arab region, omitting language-specific features has only a minor impact on classification performance, and thus, our approach could work for detecting seminar users in other parts of the world and in other languages. We further explored a controversial political topic to observe the prevalence and potential potency of such users. In our case study, we found that 25% of the users engaged in the topic are in fact seminar users and their tweets make nearly a third of the on-topic tweets. Moreover, they are often successful in affecting mainstream discourse with coordinated hashtag campaigns. "	1707.07276	Kareem Darwish, Dimitar Alexandrov, Preslav Nakov, Yelena Mejova	2017-07-23		4	1	Seminar Users in the Arabic Twitter Sphere		cs.SI	https://arxiv.org/pdf/1707.07276.pdf
1143524	 With the rise of Social Media, people obtain and share information almost instantly on a 24/7 basis. Many research areas have tried to gain valuable insights from these large volumes of freely available user generated content. With the goal of extracting knowledge from social media streams that might be useful in the context of intelligent transportation systems and smart cities, we designed and developed a framework that provides functionalities for parallel collection of geo-located tweets from multiple pre-defined bounding boxes (cities or regions), including filtering of non-complying tweets, text pre-processing for Portuguese and English language, topic modeling, and transportation-specific text classifiers, as well as, aggregation and data visualization. We performed an exploratory data analysis of geo-located tweets in 5 different cities: Rio de Janeiro, S\~ao Paulo, New York City, London and Melbourne, comprising a total of more than 43 million tweets in a period of 3 months. Furthermore, we performed a large scale topic modelling comparison between Rio de Janeiro and S\~ao Paulo. Interestingly, most of the topics are shared between both cities which despite being in the same country are considered very different regarding population, economy and lifestyle. We take advantage of recent developments in word embeddings and train such representations from the collections of geo-located tweets. We then use a combination of bag-of-embeddings and traditional bag-of-words to train travel-related classifiers in both Portuguese and English to filter travel-related content from non-related. We created specific gold-standard data to perform empirical evaluation of the resulting classifiers. Results are in line with research work in other application areas by showing the robustness of using word embeddings to learn word similarities that bag-of-words is not able to capture. 	1709.03406	João Filipe Figueiredo Pereira	2017-09-11		1	3	Social Media Text Processing and Semantic Analysis for Smart Cities		cs.SI, cs.CL, cs.CY	https://arxiv.org/pdf/1709.03406.pdf
1177867	 As people increasingly use social media as a source for news consumption, its unmoderated nature enables the diffusion of hoaxes, which in turn jeopardises the credibility of information gathered from social media platforms. To mitigate this problem, we study the development of a hoax detection system that can distinguish true and false reports early on. We introduce a semi-automated approach that leverages the Wikidata knowledge base to build large-scale datasets for veracity classification, which enables us to create a dataset with 4,007 reports including over 13 million tweets, 15% of which are fake. We describe a method for learning class-specific word representations using word embeddings, which we call multiw2v. Our approach achieves competitive results with F1 scores over 72% within 10 minutes of the first tweet being posted, outperforming other baselines. Our dataset represents a realistic scenario with a real distribution of true and false stories, which we release for further use as a benchmark in future research. 	1801.07311	Arkaitz Zubiaga	2018-01-22		1	2	Learning Class-specific Word Representations for Early Detection of Hoaxes in Social Media		cs.CL, cs.SI	https://arxiv.org/pdf/1801.07311.pdf
1185159	 This paper addresses the important problem of discerning hateful content in social media. We propose a detection scheme that is an ensemble of Recurrent Neural Network (RNN) classifiers, and it incorporates various features associated with user-related information, such as the users' tendency towards racism or sexism. These data are fed as input to the above classifiers along with the word frequency vectors derived from the textual content. Our approach has been evaluated on a publicly available corpus of 16k tweets, and the results demonstrate its effectiveness in comparison to existing state of the art solutions. More specifically, our scheme can successfully distinguish racism and sexism messages from normal text, and achieve higher classification quality than current state-of-the-art algorithms. 	1801.04433	Georgios K. Pitsilis, Heri Ramampiaro, Helge Langseth	2018-01-13		3	3	Detecting Offensive Language in Tweets Using Deep Learning		cs.CL, cs.CY, cs.SI	https://arxiv.org/pdf/1801.04433.pdf
1225029	 Predicting popularity, or the total volume of information outbreaks, is an important subproblem for understanding collective behavior in networks. Each of the two main types of recent approaches to the problem, feature-driven and generative models, have desired qualities and clear limitations. This paper bridges the gap between these solutions with a new hybrid approach and a new performance benchmark. We model each social cascade with a marked Hawkes self-exciting point process, and estimate the content virality, memory decay, and user influence. We then learn a predictive layer for popularity prediction using a collection of cascade history. To our surprise, Hawkes process with a predictive overlay outperform recent feature-driven and generative approaches on existing tweet data [43] and a new public benchmark on news tweets. We also found that a basic set of user features and event time summary statistics performs competitively in both classification and regression tasks, and that adding point process information to the feature set further improves predictions. From these observations, we argue that future work on popularity prediction should compare across feature-driven and generative modeling approaches in both classification and regression tasks. 	1608.04862	Swapnil Mishra, Marian-Andrei Rizoiu, Lexing Xie	2016-08-17	10.1145/2983323.2983812	3	2	Feature Driven and Point Process Approaches for Popularity Prediction	2016-08-29	cs.SI, physics.soc-ph	https://arxiv.org/pdf/1608.04862.pdf
1385075	 A word embedding is a low-dimensional, dense and real- valued vector representation of a word. Word embeddings have been used in many NLP tasks. They are usually gener- ated from a large text corpus. The embedding of a word cap- tures both its syntactic and semantic aspects. Tweets are short, noisy and have unique lexical and semantic features that are different from other types of text. Therefore, it is necessary to have word embeddings learned specifically from tweets. In this paper, we present ten word embedding data sets. In addition to the data sets learned from just tweet data, we also built embedding sets from the general data and the combination of tweets with the general data. The general data consist of news articles, Wikipedia data and other web data. These ten embedding models were learned from about 400 million tweets and 7 billion words from the general text. In this paper, we also present two experiments demonstrating how to use the data sets in some NLP tasks, such as tweet sentiment analysis and tweet topic classification tasks. 	1708.03994	Quanzhi Li, Sameena Shah, Xiaomo Liu, Armineh Nourbakhsh	2017-08-13		4	2	Data Sets: Word Embeddings Learned from Tweets and General Data		cs.CL, cs.SI	https://arxiv.org/pdf/1708.03994.pdf
1389420	 Given the incessant growth of documents describing the opinions of different people circulating on the web, including Web 2.0 has made it possible to give an opinion on any product in the net. In this paper, we examine the various opinions expressed in the tweets and classify them positive, negative or neutral by using the emoticons for the Bayesian method and adjectives and adverbs for the Turney's method 	1402.5123	Abdelmalek Amine, Reda Mohamed Hamou, Michel Simonet	2014-02-20	10.5958/j.2249-3220.3.1.004	3	2	Detecting Opinions in Tweets		cs.CL, cs.SI	https://arxiv.org/pdf/1402.5123.pdf
103430	 Creating sentiment polarity lexicons is labor intensive. Automatically translating them from resourceful languages requires in-domain machine translation systems, which rely on large quantities of bi-texts. In this paper, we propose to replace machine translation by transferring words from the lexicon through word embeddings aligned across languages with a simple linear transform. The approach leads to no degradation, compared to machine translation, when tested on sentiment polarity classification on tweets from four languages. 	1612.05202	Mickael Rouvier, Benoit Favre	2016-12-15		2	1	Building a robust sentiment lexicon with (almost) no resource		cs.CL	https://arxiv.org/pdf/1612.05202.pdf
145357	 Rapid crisis response requires real-time analysis of messages. After a disaster happens, volunteers attempt to classify tweets to determine needs, e.g., supplies, infrastructure damage, etc. Given labeled data, supervised machine learning can help classify these messages. Scarcity of labeled data causes poor performance in machine training. Can we reuse old tweets to train classifiers? How can we choose labeled tweets for training? Specifically, we study the usefulness of labeled data of past events. Do labeled tweets in different language help? We observe the performance of our classifiers trained using different combinations of training sets obtained from past disasters. We perform extensive experimentation on real crisis datasets and show that the past labels are useful when both source and target events are of the same type (e.g. both earthquakes). For similar languages (e.g., Italian and Spanish), cross-language domain adaptation was useful, however, when for different languages (e.g., Italian and English), the performance decreased. 	1602.05388	Muhammad Imran, Prasenjit Mitra, Jaideep Srivastava	2016-02-17		3	1	Cross-Language Domain Adaptation for Classifying Crisis-Related Short Messages	2016-03-29	cs.CL	https://arxiv.org/pdf/1602.05388.pdf
148259	 Tweets pertaining to a single event, such as a national election, can number in the hundreds of millions. Automatically analyzing them is beneficial in many downstream natural language applications such as question answering and summarization. In this paper, we propose a new task: identifying the purpose behind electoral tweets--why do people post election-oriented tweets? We show that identifying purpose is correlated with the related phenomenon of sentiment and emotion detection, but yet significantly different. Detecting purpose has a number of applications including detecting the mood of the electorate, estimating the popularity of policies, identifying key issues of contention, and predicting the course of events. We create a large dataset of electoral tweets and annotate a few thousand tweets for purpose. We develop a system that automatically classifies electoral tweets as per their purpose, obtaining an accuracy of 43.56% on an 11-class task and an accuracy of 73.91% on a 3-class task (both accuracies well above the most-frequent-class baseline). Finally, we show that resources developed for emotion detection are also helpful for detecting purpose. 	1311.1194	Saif M. Mohammad, Svetlana Kiritchenko, Joel Martin	2013-11-05		3	1	Identifying Purpose Behind Electoral Tweets		cs.CL	https://arxiv.org/pdf/1311.1194.pdf
150279	 In this paper, we describe the system submitted for the SemEval 2018 Task 3 (Irony detection in English tweets) Subtask A by the team Binarizer. Irony detection is a key task for many natural language processing works. Our method treats ironical tweets to consist of smaller parts containing different emotions. We break down tweets into separate phrases using a dependency parser. We then embed those phrases using an LSTM-based neural network model which is pre-trained to predict emoticons for tweets. Finally, we train a fully-connected network to achieve classification. 	1805.01112	Nishant Nikhil, Muktabh Mayank Srivastava	2018-05-03		2	1	Binarizer at SemEval-2018 Task 3: Parsing dependency and deep learning for irony detection		cs.CL	https://arxiv.org/pdf/1805.01112.pdf
193206	 Stance detection is a classification problem in natural language processing where for a text and target pair, a class result from the set {Favor, Against, Neither} is expected. It is similar to the sentiment analysis problem but instead of the sentiment of the text author, the stance expressed for a particular target is investigated in stance detection. In this paper, we present a stance detection tweet data set for Turkish comprising stance annotations of these tweets for two popular sports clubs as targets. Additionally, we provide the evaluation results of SVM classifiers for each target on this data set, where the classifiers use unigram, bigram, and hashtag features. This study is significant as it presents one of the initial stance detection data sets proposed so far and the first one for Turkish language, to the best of our knowledge. The data set and the evaluation results of the corresponding SVM-based approaches will form plausible baselines for the comparison of future studies on stance detection. 	1706.06894	Dilek Küçük	2017-06-21		1	1	Stance Detection in Turkish Tweets		cs.CL	https://arxiv.org/pdf/1706.06894.pdf
211853	 This paper describes our system created to detect stance in online discussions. The goal is to identify whether the author of a comment is in favor of the given target or against. Our approach is based on a maximum entropy classifier, which uses surface-level, sentiment and domain-specific features. The system was originally developed to detect stance in English tweets. We adapted it to process Czech news commentaries. 	1701.00504	Peter Krejzl, Barbora Hourová, Josef Steinberger	2017-01-02		3	1	Stance detection in online discussions		cs.CL	https://arxiv.org/pdf/1701.00504.pdf
222816	 Stance detection is a subproblem of sentiment analysis where the stance of the author of a piece of natural language text for a particular target (either explicitly stated in the text or not) is explored. The stance output is usually given as Favor, Against, or Neither. In this paper, we target at stance detection on sports-related tweets and present the performance results of our SVM-based stance classifiers on such tweets. First, we describe three versions of our proprietary tweet data set annotated with stance information, all of which are made publicly available for research purposes. Next, we evaluate SVM classifiers using different feature sets for stance detection on this data set. The employed features are based on unigrams, bigrams, hashtags, external links, emoticons, and lastly, named entities. The results indicate that joint use of the features based on unigrams, hashtags, and named entities by SVM classifiers is a plausible approach for stance detection problem on sports-related tweets. 	1803.08910	Dilek Küçük, Fazli Can	2018-03-23		2	1	Stance Detection on Tweets: An SVM-based Approach		cs.CL	https://arxiv.org/pdf/1803.08910.pdf
223085	 The tremendous amount of user generated data through social networking sites led to the gaining popularity of automatic text classification in the field of computational linguistics over the past decade. Within this domain, one problem that has drawn the attention of many researchers is automatic humor detection in texts. In depth semantic understanding of the text is required to detect humor which makes the problem difficult to automate. With increase in the number of social media users, many multilingual speakers often interchange between languages while posting on social media which is called code-mixing. It introduces some challenges in the field of linguistic analysis of social media content (Barman et al., 2014), like spelling variations and non-grammatical structures in a sentence. Past researches include detecting puns in texts (Kao et al., 2016) and humor in one-lines (Mihalcea et al., 2010) in a single language, but with the tremendous amount of code-mixed data available online, there is a need to develop techniques which detects humor in code-mixed tweets. In this paper, we analyze the task of humor detection in texts and describe a freely available corpus containing English-Hindi code-mixed tweets annotated with humorous(H) or non-humorous(N) tags. We also tagged the words in the tweets with Language tags (English/Hindi/Others). Moreover, we describe the experiments carried out on the corpus and provide a baseline classification system which distinguishes between humorous and non-humorous texts. 	1806.05513	Ankush Khandelwal, Sahil Swami, Syed S. Akhtar, Manish Shrivastava	2018-06-14		4	1	Humor Detection in English-Hindi Code-Mixed Social Media Content : Corpus and Baseline System		cs.CL	https://arxiv.org/pdf/1806.05513.pdf
277757	 The rise of social media such as blogs and social networks has fueled interest in sentiment analysis. With the proliferation of reviews, ratings, recommendations and other forms of online expression, online opinion has turned into a kind of virtual currency for businesses looking to market their products, identify new opportunities and manage their reputations, therefore many are now looking to the field of sentiment analysis. In this paper, we present a feature-based sentence level approach for Arabic sentiment analysis. Our approach is using Arabic idioms/saying phrases lexicon as a key importance for improving the detection of the sentiment polarity in Arabic sentences as well as a number of novels and rich set of linguistically motivated features contextual Intensifiers, contextual Shifter and negation handling), syntactic features for conflicting phrases which enhance the sentiment classification accuracy. Furthermore, we introduce an automatic expandable wide coverage polarity lexicon of Arabic sentiment words. The lexicon is built with gold-standard sentiment words as a seed which is manually collected and annotated and it expands and detects the sentiment orientation automatically of new sentiment words using synset aggregation technique and free online Arabic lexicons and thesauruses. Our data focus on modern standard Arabic (MSA) and Egyptian dialectal Arabic tweets and microblogs (hotel reservation, product reviews, etc.). The experimental results using our resources and techniques with SVM classifier indicate high performance levels, with accuracies of over 95%. 	1505.03105	Hossam S. Ibrahim, Sherif M. Abdou, Mervat Gheith	2015-05-12	10.5121/ijnlc.2015.4207	3	1	Sentiment Analysis For Modern Standard Arabic And Colloquial		cs.CL	https://arxiv.org/pdf/1505.03105.pdf
318403	 We propose a Long Short-Term Memory (LSTM) with attention mechanism to classify psychological stress from self-conducted interview transcriptions. We apply distant supervision by automatically labeling tweets based on their hashtag content, which complements and expands the size of our corpus. This additional data is used to initialize the model parameters, and which it is fine-tuned using the interview data. This improves the model's robustness, especially by expanding the vocabulary size. The bidirectional LSTM model with attention is found to be the best model in terms of accuracy (74.1%) and f-score (74.3%). Furthermore, we show that distant supervision fine-tuning enhances the model's performance by 1.6% accuracy and 2.1% f-score. The attention mechanism helps the model to select informative words. 	1805.12307	Genta Indra Winata, Onno Pepijn Kampman, Pascale Fung	2018-05-30		3	1	Attention-Based LSTM for Psychological Stress Detection from Spoken Language Using Distant Supervision		cs.CL	https://arxiv.org/pdf/1805.12307.pdf
337174	 A key challenge for automatic hate-speech detection on social media is the separation of hate speech from other instances of offensive language. Lexical detection methods tend to have low precision because they classify all messages containing particular terms as hate speech and previous work using supervised learning has failed to distinguish between the two categories. We used a crowd-sourced hate speech lexicon to collect tweets containing hate speech keywords. We use crowd-sourcing to label a sample of these tweets into three categories: those containing hate speech, only offensive language, and those with neither. We train a multi-class classifier to distinguish between these different categories. Close analysis of the predictions and the errors shows when we can reliably separate hate speech from other offensive language and when this differentiation is more difficult. We find that racist and homophobic tweets are more likely to be classified as hate speech but that sexist tweets are generally classified as offensive. Tweets without explicit hate keywords are also more difficult to classify. 	1703.04009	Thomas Davidson, Dana Warmsley, Michael Macy, Ingmar Weber	2017-03-11		4	1	Automated Hate Speech Detection and the Problem of Offensive Language		cs.CL	https://arxiv.org/pdf/1703.04009.pdf
419253	 Clickbait detection in tweets remains an elusive challenge. In this paper, we describe the solution for the Zingel Clickbait Detector at the Clickbait Challenge 2017, which is capable of evaluating each tweet's level of click baiting. We first reformat the regression problem as a multi-classification problem, based on the annotation scheme. To perform multi-classification, we apply a token-level, self-attentive mechanism on the hidden states of bi-directional Gated Recurrent Units (biGRU), which enables the model to generate tweets' task-specific vector representations by attending to important tokens. The self-attentive neural network can be trained end-to-end, without involving any manual feature engineering. Our detector ranked first in the final evaluation of Clickbait Challenge 2017. 	1710.05364	Yiwei Zhou	2017-10-15		1	1	Clickbait Detection in Tweets Using Self-attentive Network		cs.CL	https://arxiv.org/pdf/1710.05364.pdf
425861	" Hashtags are semantico-syntactic constructs used across various social networking and microblogging platforms to enable users to start a topic specific discussion or classify a post into a desired category. Segmenting and linking the entities present within the hashtags could therefore help in better understanding and extraction of information shared across the social media. However, due to lack of space delimiters in the hashtags (e.g #nsavssnowden), the segmentation of hashtags into constituent entities (""NSA"" and ""Edward Snowden"" in this case) is not a trivial task. Most of the current state-of-the-art social media analytics systems like Sentiment Analysis and Entity Linking tend to either ignore hashtags, or treat them as a single word. In this paper, we present a context aware approach to segment and link entities in the hashtags to a knowledge base (KB) entry, based on the context within the tweet. Our approach segments and links the entities in hashtags such that the coherence between hashtag semantics and the tweet is maximized. To the best of our knowledge, no existing study addresses the issue of linking entities in hashtags for extracting semantic information. We evaluate our method on two different datasets, and demonstrate the effectiveness of our technique in improving the overall entity linking in tweets via additional semantic information provided by segmenting and linking entities in a hashtag. "	1501.03210	Piyush Bansal, Romil Bansal, Vasudeva Varma	2015-01-13		3	2	Towards Deep Semantic Analysis Of Hashtags		cs.IR, cs.CL	https://arxiv.org/pdf/1501.03210.pdf
505420	 Our team, NRC-Canada, participated in two shared tasks at the AMIA-2017 Workshop on Social Media Mining for Health Applications (SMM4H): Task 1 - classification of tweets mentioning adverse drug reactions, and Task 2 - classification of tweets describing personal medication intake. For both tasks, we trained Support Vector Machine classifiers using a variety of surface-form, sentiment, and domain-specific features. With nine teams participating in each task, our submissions ranked first on Task 1 and third on Task 2. Handling considerable class imbalance proved crucial for Task 1. We applied an under-sampling technique to reduce class imbalance (from about 1:10 to 1:2). Standard n-gram features, n-grams generalized over domain terms, as well as general-domain and domain-specific word embeddings had a substantial impact on the overall performance in both tasks. On the other hand, including sentiment lexicon features did not result in any improvement. 	1805.04558	Svetlana Kiritchenko, Saif M. Mohammad, Jason Morin, Berry de Bruijn	2018-05-11		4	1	NRC-Canada at SMM4H Shared Task: Classifying Tweets Mentioning Adverse Drug Reactions and Medication Intake		cs.CL	https://arxiv.org/pdf/1805.04558.pdf
525742	 Stance detection is a critical component of rumour and fake news identification. It involves the extraction of the stance a particular author takes related to a given claim, both expressed in text. This paper investigates stance classification for Russian. It introduces a new dataset, RuStance, of Russian tweets and news comments from multiple sources, covering multiple stories, as well as text classification approaches to stance detection as benchmarks over this data in this language. As well as presenting this openly-available dataset, the first of its kind for Russian, the paper presents a baseline for stance prediction in the language. 	1809.01574	Nikita Lozhnikov, Leon Derczynski, Manuel Mazzara	2018-09-05	10.13140/RG.2.2.15252.76161/1	3	1	Stance Prediction for Russian: Data and Analysis		cs.CL	https://arxiv.org/pdf/1809.01574.pdf
584893	 It is important for machines to interpret human emotions properly for better human-machine communications, as emotion is an essential part of human-to-human communications. One aspect of emotion is reflected in the language we use. How to represent emotions in texts is a challenge in natural language processing (NLP). Although continuous vector representations like word2vec have become the new norm for NLP problems, their limitations are that they do not take emotions into consideration and can unintentionally contain bias toward certain identities like different genders. This thesis focuses on improving existing representations in both word and sentence levels by explicitly taking emotions inside text and model bias into account in their training process. Our improved representations can help to build more robust machine learning models for affect-related text classification like sentiment/emotion analysis and abusive language detection. We first propose representations called emotional word vectors (EVEC), which is learned from a convolutional neural network model with an emotion-labeled corpus, which is constructed using hashtags. Secondly, we extend to learning sentence-level representations with a huge corpus of texts with the pseudo task of recognizing emojis. Our results show that, with the representations trained from millions of tweets with weakly supervised labels such as hashtags and emojis, we can solve sentiment/emotion analysis tasks more effectively. Lastly, as examples of model bias in representations of existing approaches, we explore a specific problem of automatic detection of abusive language. We address the issue of gender bias in various neural network models by conducting experiments to measure and reduce those biases in the representations in order to build more robust classification models. 	1808.07235	Ji Ho Park	2018-08-22		1	1	Finding Good Representations of Emotions for Text Classification		cs.CL	https://arxiv.org/pdf/1808.07235.pdf
612019	 We introduce openXBOW, an open-source toolkit for the generation of bag-of-words (BoW) representations from multimodal input. In the BoW principle, word histograms were first used as features in document classification, but the idea was and can easily be adapted to, e.g., acoustic or visual low-level descriptors, introducing a prior step of vector quantisation. The openXBOW toolkit supports arbitrary numeric input features and text input and concatenates computed subbags to a final bag. It provides a variety of extensions and options. To our knowledge, openXBOW is the first publicly available toolkit for the generation of crossmodal bags-of-words. The capabilities of the tool are exemplified in two sample scenarios: time-continuous speech-based emotion recognition and sentiment analysis in tweets where improved results over other feature representation forms were observed. 	1605.06778	Maximilian Schmitt, Björn W. Schuller	2016-05-22		2	3	openXBOW - Introducing the Passau Open-Source Crossmodal Bag-of-Words Toolkit		cs.CV, cs.CL, cs.IR	https://arxiv.org/pdf/1605.06778.pdf
615297	 Social media has become one of the main channels for peo- ple to communicate and share their views with the society. We can often detect from these views whether the person is in favor, against or neu- tral towards a given topic. These opinions from social media are very useful for various companies. We present a new dataset that consists of 3545 English-Hindi code-mixed tweets with opinion towards Demoneti- sation that was implemented in India in 2016 which was followed by a large countrywide debate. We present a baseline supervised classification system for stance detection developed using the same dataset that uses various machine learning techniques to achieve an accuracy of 58.7% on 10-fold cross validation. 	1805.11868	Sahil Swami, Ankush Khandelwal, Vinay Singh, Syed Sarfaraz Akhtar, Manish Shrivastava	2018-05-30		5	1	An English-Hindi Code-Mixed Corpus: Stance Annotation and Baseline System		cs.CL	https://arxiv.org/pdf/1805.11868.pdf
691302	 Public disclosure of important security information, such as knowledge of vulnerabilities or exploits, often occurs in blogs, tweets, mailing lists, and other online sources months before proper classification into structured databases. In order to facilitate timely discovery of such knowledge, we propose a novel semi-supervised learning algorithm, PACE, for identifying and classifying relevant entities in text sources. The main contribution of this paper is an enhancement of the traditional bootstrapping method for entity extraction by employing a time-memory trade-off that simultaneously circumvents a costly corpus search while strengthening pattern nomination, which should increase accuracy. An implementation in the cyber-security domain is discussed as well as challenges to Natural Language Processing imposed by the security domain. 	1308.4648	Nikki McNeil, Robert A. Bridges, Michael D. Iannacone, Bogdan Czejdo, Nicolas Perez, John R. Goodall	2013-08-21		6	2	PACE: Pattern Accurate Computationally Efficient Bootstrapping for Timely Discovery of Cyber-Security Concepts	2013-10-11	cs.IR, cs.CL	https://arxiv.org/pdf/1308.4648.pdf
722365	 Non-linear models recently receive a lot of attention as people are starting to discover the power of statistical and embedding features. However, tree-based models are seldom studied in the context of structured learning despite their recent success on various classification and ranking tasks. In this paper, we propose S-MART, a tree-based structured learning framework based on multiple additive regression trees. S-MART is especially suitable for handling tasks with dense features, and can be used to learn many different structures under various loss functions. We apply S-MART to the task of tweet entity linking --- a core component of tweet information extraction, which aims to identify and link name mentions to entities in a knowledge base. A novel inference algorithm is proposed to handle the special structure of the task. The experimental results show that S-MART significantly outperforms state-of-the-art tweet entity linking systems. 	1609.08075	Yi Yang, Ming-Wei Chang	2016-09-26		2	1	S-MART: Novel Tree-based Structured Learning Algorithms Applied to Tweet Entity Linking		cs.CL	https://arxiv.org/pdf/1609.08075.pdf
790620	 The complexities of Arabic language in morphology, orthography and dialects makes sentiment analysis for Arabic more challenging. Also, text feature extraction from short messages like tweets, in order to gauge the sentiment, makes this task even more difficult. In recent years, deep neural networks were often employed and showed very good results in sentiment classification and natural language processing applications. Word embedding, or word distributing approach, is a current and powerful tool to capture together the closest words from a contextual text. In this paper, we describe how we construct Word2Vec models from a large Arabic corpus obtained from ten newspapers in different Arab countries. By applying different machine learning algorithms and convolutional neural networks with different text feature selections, we report improved accuracy of sentiment classification (91%-95%) on our publicly available Arabic language health sentiment dataset [1] 	1803.00124	Abdulaziz M. Alayba, Vasile Palade, Matthew England, Rahat Iqbal	2018-02-28		4	1	Improving Sentiment Analysis in Arabic Using Word Representation	2018-03-30	cs.CL	https://arxiv.org/pdf/1803.00124.pdf
813755	 We can often detect from a person's utterances whether he/she is in favor of or against a given target entity -- their stance towards the target. However, a person may express the same stance towards a target by using negative or positive language. Here for the first time we present a dataset of tweet--target pairs annotated for both stance and sentiment. The targets may or may not be referred to in the tweets, and they may or may not be the target of opinion in the tweets. Partitions of this dataset were used as training and test sets in a SemEval-2016 shared task competition. We propose a simple stance detection system that outperforms submissions from all 19 teams that participated in the shared task. Additionally, access to both stance and sentiment annotations allows us to explore several research questions. We show that while knowing the sentiment expressed by a tweet is beneficial for stance classification, it alone is not sufficient. Finally, we use additional unlabeled data through distant supervision techniques and word embeddings to further improve stance classification. 	1605.01655	Saif M. Mohammad, Parinaz Sobhani, Svetlana Kiritchenko	2016-05-05		3	1	Stance and Sentiment in Tweets		cs.CL	https://arxiv.org/pdf/1605.01655.pdf
913251	" In this paper, through multi-task ensemble framework we address three problems of emotion and sentiment analysis i.e. ""emotion classification & intensity"", ""valence, arousal & dominance for emotion"" and ""valence & arousal} for sentiment"". The underlying problems cover two granularities (i.e. coarse-grained and fine-grained) and a diverse range of domains (i.e. tweets, Facebook posts, news headlines, blogs, letters etc.). The ensemble model aims to leverage the learned representations of three deep learning models (i.e. CNN, LSTM and GRU) and a hand-crafted feature representation for the predictions. Experimental results on the benchmark datasets show the efficacy of our proposed multi-task ensemble frameworks. We obtain the performance improvement of 2-3 points on an average over single-task systems for most of the problems and domains. "	1808.01216	Md Shad Akhtar, Deepanway Ghosal, Asif Ekbal, Pushpak Bhattacharyya	2018-08-03		4	1	A Multi-task Ensemble Framework for Emotion, Sentiment and Intensity Prediction		cs.CL	https://arxiv.org/pdf/1808.01216.pdf
1011277	" Past work in computational sarcasm deals primarily with sarcasm detection. In this paper, we introduce a novel, related problem: sarcasm target identification i.e., extracting the target of ridicule in a sarcastic sentence). We present an introductory approach for sarcasm target identification. Our approach employs two types of extractors: one based on rules, and another consisting of a statistical classifier. To compare our approach, we use two baselines: a na\""ive baseline and another baseline based on work in sentiment target identification. We perform our experiments on book snippets and tweets, and show that our hybrid approach performs better than the two baselines and also, in comparison with using the two extractors individually. Our introductory approach establishes the viability of sarcasm target identification, and will serve as a baseline for future work. "	1610.07091	Aditya Joshi, Pranav Goel, Pushpak Bhattacharyya, Mark Carman	2016-10-22		4	1	Automatic Identification of Sarcasm Target: An Introductory Approach	2017-08-25	cs.CL	https://arxiv.org/pdf/1610.07091.pdf
1070441	 Numerals that contain much information in financial documents are crucial for financial decision making. They play different roles in financial analysis processes. This paper is aimed at understanding the meanings of numerals in financial tweets for fine-grained crowd-based forecasting. We propose a taxonomy that classifies the numerals in financial tweets into 7 categories, and further extend some of these categories into several subcategories. Neural network-based models with word and character-level encoders are proposed for 7-way classification and 17-way classification. We perform backtest to confirm the effectiveness of the numeric opinions made by the crowd. This work is the first attempt to understand numerals in financial social media data, and we provide the first comparison of fine-grained opinion of individual investors and analysts based on their forecast price. The numeral corpus used in our experiments, called FinNum 1.0 , is available for research purposes. 	1809.05356	Chung-Chi Chen, Hen-Hsen Huang, Yow-Ting Shiue, Hsin-Hsi Chen	2018-09-14		4	1	Numeral Understanding in Financial Tweets for Fine-grained Crowd-based Forecasting		cs.CL	https://arxiv.org/pdf/1809.05356.pdf
1099058	 Alcohol abuse may lead to unsociable behavior such as crime, drunk driving, or privacy leaks. We introduce automatic drunk-texting prediction as the task of identifying whether a text was written when under the influence of alcohol. We experiment with tweets labeled using hashtags as distant supervision. Our classifiers use a set of N-gram and stylistic features to detect drunk tweets. Our observations present the first quantitative evidence that text contains signals that can be exploited to detect drunk-texting. 	1610.00879	Aditya Joshi, Abhijit Mishra, Balamurali AR, Pushpak Bhattacharyya, Mark Carman	2016-10-04		5	1	A Computational Approach to Automatic Prediction of Drunk Texting		cs.CL	https://arxiv.org/pdf/1610.00879.pdf
1100245	 In the wake of a polarizing election, social media is laden with hateful content. To address various limitations of supervised hate speech classification methods including corpus bias and huge cost of annotation, we propose a weakly supervised two-path bootstrapping approach for an online hate speech detection model leveraging large-scale unlabeled data. This system significantly outperforms hate speech detection systems that are trained in a supervised manner using manually annotated data. Applying this model on a large quantity of tweets collected before, after, and on election day reveals motivations and patterns of inflammatory language. 	1710.07394	Lei Gao, Alexis Kuppersmith, Ruihong Huang	2017-10-19		3	1	Recognizing Explicit and Implicit Hate Speech Using a Weakly Supervised Two-path Bootstrapping Approach	2018-05-21	cs.CL	https://arxiv.org/pdf/1710.07394.pdf
1216918	 Computer systems need to be able to react to stress in order to perform optimally on some tasks. This article describes TensiStrength, a system to detect the strength of stress and relaxation expressed in social media text messages. TensiStrength uses a lexical approach and a set of rules to detect direct and indirect expressions of stress or relaxation, particularly in the context of transportation. It is slightly more effective than a comparable sentiment analysis program, although their similar performances occur despite differences on almost half of the tweets gathered. The effectiveness of TensiStrength depends on the nature of the tweets classified, with tweets that are rich in stress-related terms being particularly problematic. Although generic machine learning methods can give better performance than TensiStrength overall, they exploit topic-related terms in a way that may be undesirable in practical applications and that may not work as well in more focused contexts. In conclusion, TensiStrength and generic machine learning approaches work well enough to be practical choices for intelligent applications that need to take advantage of stress information, and the decision about which to use depends on the nature of the texts analysed and the purpose of the task. 	1607.00139	Mike Thelwall	2016-07-01	10.1016/j.ipm.2016.06.009	1	1	TensiStrength: Stress and relaxation magnitude detection for social media texts	2016-07-04	cs.CL	https://arxiv.org/pdf/1607.00139.pdf
1222117	 This article presents classifiers based on SVM and Convolutional Neural Networks (CNN) for the TASS 2017 challenge on tweets sentiment analysis. The classifier with the best performance in general uses a combination of SVM and CNN. The use of word embeddings was particularly useful for improving the classifiers performance. 	1710.06393	Aiala Rosá, Luis Chiruzzo, Mathias Etcheverry, Santiago Castro	2017-10-17		4	1	RETUYT in TASS 2017: Sentiment Analysis for Spanish Tweets using SVM and CNN		cs.CL	https://arxiv.org/pdf/1710.06393.pdf
1224811	" This paper describes Infosys's participation in the ""2nd Social Media Mining for Health Applications Shared Task at AMIA, 2017, Task 2"". Mining social media messages for health and drug related information has received significant interest in pharmacovigilance research. This task targets at developing automated classification models for identifying tweets containing descriptions of personal intake of medicines. Towards this objective we train a stacked ensemble of shallow convolutional neural network (CNN) models on an annotated dataset provided by the organizers. We use random search for tuning the hyper-parameters of the CNN and submit an ensemble of best models for the prediction task. Our system secured first place among 9 teams, with a micro-averaged F-score of 0.693. "	1803.07718	Jasper Friedrichs, Debanjan Mahata, Shubham Gupta	2018-03-20		3	1	InfyNLP at SMM4H Task 2: Stacked Ensemble of Shallow Convolutional Neural Networks for Identifying Personal Medication Intake from Twitter		cs.CL	https://arxiv.org/pdf/1803.07718.pdf
1241624	 We propose a new method to detect when users express the intent to leave a service, also known as churn. While previous work focuses solely on social media, we show that this intent can be detected in chatbot conversations. As companies increasingly rely on chatbots they need an overview of potentially churny users. To this end, we crowdsource and publish a dataset of churn intent expressions in chatbot interactions in German and English. We show that classifiers trained on social media data can detect the same intent in the context of chatbots. We introduce a classification architecture that outperforms existing work on churn intent detection in social media. Moreover, we show that, using bilingual word embeddings, a system trained on combined English and German data outperforms monolingual approaches. As the only existing dataset is in English, we crowdsource and publish a novel dataset of German tweets. We thus underline the universal aspect of the problem, as examples of churn intent in English help us identify churn in German tweets and chatbot conversations. 	1808.08432	Christian Abbet, Meryem M'hamdi, Athanasios Giannakopoulos, Robert West, Andreea Hossmann, Michael Baeriswyl, Claudiu Musat	2018-08-25		7	1	Churn Intent Detection in Multilingual Chatbot Conversations and Social Media		cs.CL	https://arxiv.org/pdf/1808.08432.pdf
1271345	 Topic Models have been reported to be beneficial for aspect-based sentiment analysis. This paper reports a simple topic model for sarcasm detection, a first, to the best of our knowledge. Designed on the basis of the intuition that sarcastic tweets are likely to have a mixture of words of both sentiments as against tweets with literal sentiment (either positive or negative), our hierarchical topic model discovers sarcasm-prevalent topics and topic-level sentiment. Using a dataset of tweets labeled using hashtags, the model estimates topic-level, and sentiment-level distributions. Our evaluation shows that topics such as `work', `gun laws', `weather' are sarcasm-prevalent topics. Our model is also able to discover the mixture of sentiment-bearing words that exist in a text of a given sentiment-related label. Finally, we apply our model to predict sarcasm in tweets. We outperform two prior work based on statistical classifiers with specific features, by around 25\%. 	1611.04326	Aditya Joshi, Prayas Jain, Pushpak Bhattacharyya, Mark Carman	2016-11-14		4	1	`Who would have thought of that!': A Hierarchical Topic Model for Extraction of Sarcasm-prevalent Topics and Sarcasm Detection	2016-11-22	cs.CL	https://arxiv.org/pdf/1611.04326.pdf
1313994	 In this paper, we describe how we created two state-of-the-art SVM classifiers, one to detect the sentiment of messages such as tweets and SMS (message-level task) and one to detect the sentiment of a term within a submissions stood first in both tasks on tweets, obtaining an F-score of 69.02 in the message-level task and 88.93 in the term-level task. We implemented a variety of surface-form, semantic, and sentiment features. with sentiment-word hashtags, and one from tweets with emoticons. In the message-level task, the lexicon-based features provided a gain of 5 F-score points over all others. Both of our systems can be replicated us available resources. 	1308.6242	Saif M. Mohammad, Svetlana Kiritchenko, Xiaodan Zhu	2013-08-28		3	1	NRC-Canada: Building the State-of-the-Art in Sentiment Analysis of Tweets		cs.CL	https://arxiv.org/pdf/1308.6242.pdf
1399554	" Social media users often make explicit predictions about upcoming events. Such statements vary in the degree of certainty the author expresses toward the outcome:""Leonardo DiCaprio will win Best Actor"" vs. ""Leonardo DiCaprio may win"" or ""No way Leonardo wins!"". Can popular beliefs on social media predict who will win? To answer this question, we build a corpus of tweets annotated for veridicality on which we train a log-linear classifier that detects positive veridicality with high precision. We then forecast uncertain outcomes using the wisdom of crowds, by aggregating users' explicit predictions. Our method for forecasting winners is fully automated, relying only on a set of contenders as input. It requires no training data of past outcomes and outperforms sentiment and tweet volume baselines on a broad range of contest prediction tasks. We further demonstrate how our approach can be used to measure the reliability of individual accounts' predictions and retrospectively identify surprise outcomes. "	1707.07212	Sandesh Swamy, Alan Ritter, Marie-Catherine de Marneffe	2017-07-22		3	1	"""i have a feeling trump will win.................."": Forecasting Winners and Losers from User Predictions on Twitter"	2017-08-31	cs.CL	https://arxiv.org/pdf/1707.07212.pdf
1416705	 Although, the fair amount of works in sentiment analysis (SA) and opinion mining (OM) systems in the last decade and with respect to the performance of these systems, but it still not desired performance, especially for morphologically-Rich Language (MRL) such as Arabic, due to the complexities and challenges exist in the nature of the languages itself. One of these challenges is the detection of idioms or proverbs phrases within the writer text or comment. An idiom or proverb is a form of speech or an expression that is peculiar to itself. Grammatically, it cannot be understood from the individual meanings of its elements and can yield different sentiment when treats as separate words. Consequently, In order to facilitate the task of detection and classification of lexical phrases for automated SA systems, this paper presents AIPSeLEX a novel idioms/ proverbs sentiment lexicon for modern standard Arabic (MSA) and colloquial. AIPSeLEX is manually collected and annotated at sentence level with semantic orientation (positive or negative). The efforts of manually building and annotating the lexicon are reported. Moreover, we build a classifier that extracts idioms and proverbs, phrases from text using n-gram and similarity measure methods. Finally, several experiments were carried out on various data, including Arabic tweets and Arabic microblogs (hotel reservation, product reviews, and TV program comments) from publicly available Arabic online reviews websites (social media, blogs, forums, e-commerce web sites) to evaluate the coverage and accuracy of AIPSeLEX. 	1506.01906	Hossam S. Ibrahim, Sherif M. Abdou, Mervat Gheith	2015-06-05	10.5120/20790-3435	3	1	Idioms-Proverbs Lexicon for Modern Standard Arabic and Colloquial Sentiment Analysis		cs.CL	https://arxiv.org/pdf/1506.01906.pdf
1429620	 This paper presents a computational approach to author profiling taking gender and language variety into account. We apply an ensemble system with the output of multiple linear SVM classifiers trained on character and word $n$-grams. We evaluate the system using the dataset provided by the organizers of the 2017 PAN lab on author profiling. Our approach achieved 75% average accuracy on gender identification on tweets written in four languages and 97% accuracy on language variety identification for Portuguese. 	1707.00621	Alina Maria Ciobanu, Marcos Zampieri, Shervin Malmasi, Liviu P. Dinu	2017-07-03		4	1	Including Dialects and Language Varieties in Author Profiling		cs.CL	https://arxiv.org/pdf/1707.00621.pdf
452111	 Using geometric data analysis, our objective is the analysis of narrative, with narrative of emotion being the focus in this work. The following two principles for analysis of emotion inform our work. Firstly, emotion is revealed not as a quality in its own right but rather through interaction. We study the 2-way relationship of Ilsa and Rick in the movie Casablanca, and the 3-way relationship of Emma, Charles and Rodolphe in the novel {\em Madame Bovary}. Secondly, emotion, that is expression of states of mind of subjects, is formed and evolves within the narrative that expresses external events and (personal, social, physical) context. In addition to the analysis methodology with key aspects that are innovative, the input data used is crucial. We use, firstly, dialogue, and secondly, broad and general description that incorporates dialogue. In a follow-on study, we apply our unsupervised narrative mapping to data streams with very low emotional expression. We map the narrative of Twitter streams. Thus we demonstrate map analysis of general narratives. 	1405.3539	Fionn Murtagh, Adam Ganz	2014-05-14		2	2	Pattern Recognition in Narrative: Tracking Emotional Expression in Context	2015-05-04	cs.AI, cs.CL	https://arxiv.org/pdf/1405.3539.pdf
567324	 We investigate evaluation metrics for dialogue response generation systems where supervised labels, such as task completion, are not available. Recent works in response generation have adopted metrics from machine translation to compare a model's generated response to a single target response. We show that these metrics correlate very weakly with human judgements in the non-technical Twitter domain, and not at all in the technical Ubuntu domain. We provide quantitative and qualitative results highlighting specific weaknesses in existing metrics, and provide recommendations for future development of better automatic evaluation metrics for dialogue systems. 	1603.08023	Chia-Wei Liu, Ryan Lowe, Iulian V. Serban, Michael Noseworthy, Laurent Charlin, Joelle Pineau	2016-03-25		6	4	How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation	2017-01-03	cs.CL, cs.AI, cs.LG, cs.NE	https://arxiv.org/pdf/1603.08023.pdf
581552	 We propose a framework for inferring the latent attitudes or preferences of users by performing probabilistic first-order logical reasoning over the social network graph. Our method answers questions about Twitter users like {\em Does this user like sushi?} or {\em Is this user a New York Knicks fan?} by building a probabilistic model that reasons over user attributes (the user's location or gender) and the social network (the user's friends and spouse), via inferences like homophily (I am more likely to like sushi if spouse or friends like sushi, I am more likely to like the Knicks if I live in New York). The algorithm uses distant supervision, semi-supervised data harvesting and vector space models to extract user attributes (e.g. spouse, education, location) and preferences (likes and dislikes) from text. The extracted propositions are then fed into a probabilistic reasoner (we investigate both Markov Logic and Probabilistic Soft Logic). Our experiments show that probabilistic logical reasoning significantly improves the performance on attribute and relation extraction, and also achieves an F-score of 0.791 at predicting a users likes or dislikes, significantly better than two strong baselines. 	1411.2679	Jiwei Li, Alan Ritter, Dan Jurafsky	2014-11-10		3	4	Inferring User Preferences by Probabilistic Logical Reasoning over Social Networks		cs.SI, cs.AI, cs.CL, cs.LG	https://arxiv.org/pdf/1411.2679.pdf
1083480	 Dating and romantic relationships not only play a huge role in our personal lives but also collectively influence and shape society. Today, many romantic partnerships originate from the Internet, signifying the importance of technology and the web in modern dating. In this paper, we present a text-based computational approach for estimating the relationship compatibility of two users on social media. Unlike many previous works that propose reciprocal recommender systems for online dating websites, we devise a distant supervision heuristic to obtain real world couples from social platforms such as Twitter. Our approach, the CoupleNet is an end-to-end deep learning based estimator that analyzes the social profiles of two users and subsequently performs a similarity match between the users. Intuitively, our approach performs both user profiling and match-making within a unified end-to-end framework. CoupleNet utilizes hierarchical recurrent neural models for learning representations of user profiles and subsequently coupled attention mechanisms to fuse information aggregated from two users. To the best of our knowledge, our approach is the first data-driven deep learning approach for our novel relationship recommendation problem. We benchmark our CoupleNet against several machine learning and deep learning baselines. Experimental results show that our approach outperforms all approaches significantly in terms of precision. Qualitative analysis shows that our model is capable of also producing explainable results to users. 	1805.11535	Yi Tay, Anh Tuan Luu, Siu Cheung Hui	2018-05-29		3	4	CoupleNet: Paying Attention to Couples with Coupled Attention for Relationship Recommendation		cs.CL, cs.AI, cs.IR, cs.NE	https://arxiv.org/pdf/1805.11535.pdf
1139891	 We study the extent to which we can infer users' geographical locations from social media. Location inference from social media can benefit many applications, such as disaster management, targeted advertising, and news content tailoring. In recent years, a number of algorithms have been proposed for identifying user locations on social media platforms such as Twitter and Facebook from message contents, friend networks, and interactions between users. In this paper, we propose a novel probabilistic model based on factor graphs for location inference that offers several unique advantages for this task. First, the model generalizes previous methods by incorporating content, network, and deep features learned from social context. The model is also flexible enough to support both supervised learning and semi-supervised learning. Second, we explore several learning algorithms for the proposed model, and present a Two-chain Metropolis-Hastings (MH+) algorithm, which improves the inference accuracy. Third, we validate the proposed model on three different genres of data - Twitter, Weibo, and Facebook - and demonstrate that the proposed model can substantially improve the inference accuracy (+3.3-18.5% by F1-score) over that of several state-of-the-art methods. 	1702.07281	Yujie Qian, Jie Tang, Zhilin Yang, Binxuan Huang, Wei Wei, Kathleen M. Carley	2017-02-23		6	2	A Probabilistic Framework for Location Inference from Social Media	2017-03-01	cs.AI, cs.SI	https://arxiv.org/pdf/1702.07281.pdf
983938	 Tencent Weibo, as one of the most popular micro-blogging services in China, has attracted millions of users, producing 30-60 millions of weibo (similar as tweet in Twitter) daily. With the overload problem of user generate content, Tencent users find it is more and more hard to browse and find valuable information at the first time. In this paper, we propose a Factor Graph based weibo recommendation algorithm TSI-WR (Topic-Level Social Influence based Weibo Recommendation), which could help Tencent users to find most suitable information. The main innovation is that we consider both direct and indirect social influence from topic level based on social balance theory. The main advantages of adopting this strategy are that it could first build a more accurate description of latent relationship between two users with weak connections, which could help to solve the data sparsity problem; second provide a more accurate recommendation for a certain user from a wider range. Other meaningful contextual information is also combined into our model, which include: Users profile, Users influence, Content of weibos, Topic information of weibos and etc. We also design a semi-supervised algorithm to further reduce the influence of data sparisty. The experiments show that all the selected variables are important and the proposed model outperforms several baseline methods. 	1210.7047	Daifeng Li, Zhipeng Luo, Golden Guo-zheng Sun, Jie Tang, Jingwei Zhang	2012-10-25		5	3	User-level Weibo Recommendation incorporating Social Influence based on Semi-Supervised Algorithm		cs.SI, cs.CY, cs.LG	https://arxiv.org/pdf/1210.7047.pdf
18556	 Due to the proliferation of online social networks (OSNs), users find themselves participating in multiple OSNs. These users leave their activity traces as they maintain friendships and interact with other users in these OSNs. In this work, we analyze how users maintain friendship in multiple OSNs by studying users who have accounts in both Twitter and Instagram. Specifically, we study the similarity of a user's friendship and the evenness of friendship distribution in multiple OSNs. Our study shows that most users in Twitter and Instagram prefer to maintain different friendships in the two OSNs, keeping only a small clique of common friends in across the OSNs. Based upon our empirical study, we conduct link prediction experiments to predict missing friendship links in multiple OSNs using the neighborhood features, neighborhood friendship maintenance features and cross-link features. Our link prediction experiments shows that un- supervised methods can yield good accuracy in predicting links in one OSN using another OSN data and the link prediction accuracy can be further improved using supervised method with friendship maintenance and others measures as features. 	1703.00857	Roy Ka-Wei Lee, Ee-Peng Lim	2017-03-02		2	2	Friendship Maintenance and Prediction in Multiple Social Networks		cs.SI, physics.soc-ph	https://arxiv.org/pdf/1703.00857.pdf
157755	 Online Social Networks (OSN) are increasingly being used as platform for an effective communication, to engage with other users, and to create a social worth via number of likes, followers and shares. Such metrics and crowd-sourced ratings give the OSN user a sense of social reputation which she tries to maintain and boost to be more influential. Users artificially bolster their social reputation via black-market web services. In this work, we identify users which manipulate their projected follower count using an unsupervised local neighborhood detection method. We identify a neighborhood of the user based on a robust set of features which reflect user similarity in terms of the expected follower count. We show that follower count estimation using our method has 84.2% accuracy with a low error rate. In addition, we estimate the follower count of the user under suspicion by finding its neighborhood drawn from a large random sample of Twitter. We show that our method is highly tolerant to synthetic manipulation of followers. Using the deviation of predicted follower count from the displayed count, we are also able to detect customers with a high precision of 98.62% 	1802.03625	Anupama Aggarwal, Saravana Kumar, Kushagra Bhargava, Ponnurangam Kumaraguru	2018-02-10		4	1	The Follower Count Fallacy: Detecting Twitter Users with Manipulated Follower Count		cs.SI	https://arxiv.org/pdf/1802.03625.pdf
201752	 Religiosity is a powerful force shaping human societies, affecting domains as diverse as economic growth or the ability to cope with illness. As more religious leaders and organizations as well as believers start using social networking sites (e.g., Twitter, Facebook), online activities become important extensions to traditional religious rituals and practices. However, there has been lack of research on religiosity in online social networks. This paper takes a step toward the understanding of several important aspects of religiosity on Twitter, based on the analysis of more than 250k U.S. users who self-declared their religions/belief, including Atheism, Buddhism, Christianity, Hinduism, Islam, and Judaism. Specifically, (i) we examine the correlation of geographic distribution of religious people between Twitter and offline surveys. (ii) We analyze users' tweets and networks to identify discriminative features of each religious group, and explore supervised methods to identify believers of different religions. (iii) We study the linkage preference of different religious groups, and observe a strong preference of Twitter users connecting to others sharing the same religion. 	1409.8578	Lu Chen, Ingmar Weber, Adam Okulicz-Kozaryn	2014-09-30		3	2	U.S. Religious Landscape on Twitter		cs.SI, physics.soc-ph	https://arxiv.org/pdf/1409.8578.pdf
204864	 In many social networks, several different link relations will exist between the same set of users. Additionally, attribute or textual information will be associated with those users, such as demographic details or user-generated content. For many data analysis tasks, such as community finding and data visualisation, the provision of multiple heterogeneous types of user data makes the analysis process more complex. We propose an unsupervised method for integrating multiple data views to produce a single unified graph representation, based on the combination of the k-nearest neighbour sets for users derived from each view. These views can be either relation-based or feature-based. The proposed method is evaluated on a number of annotated multi-view Twitter datasets, where it is shown to support the discovery of the underlying community structure in the data. 	1301.5809	Derek Greene, Pádraig Cunningham	2013-01-24		2	2	Producing a Unified Graph Representation from Multiple Social Network Views	2013-02-18	cs.SI, physics.soc-ph	https://arxiv.org/pdf/1301.5809.pdf
208782	" Background: Social media has the capacity to afford the healthcare industry with valuable feedback from patients who reveal and express their medical decision-making process, as well as self-reported quality of life indicators both during and post treatment. In prior work, [Crannell et. al.], we have studied an active cancer patient population on Twitter and compiled a set of tweets describing their experience with this disease. We refer to these online public testimonies as ""Invisible Patient Reported Outcomes"" (iPROs), because they carry relevant indicators, yet are difficult to capture by conventional means of self-report. Methods: Our present study aims to identify tweets related to the patient experience as an additional informative tool for monitoring public health. Using Twitter's public streaming API, we compiled over 5.3 million ""breast cancer"" related tweets spanning September 2016 until mid December 2017. We combined supervised machine learning methods with natural language processing to sift tweets relevant to breast cancer patient experiences. We analyzed a sample of 845 breast cancer patient and survivor accounts, responsible for over 48,000 posts. We investigated tweet content with a hedonometric sentiment analysis to quantitatively extract emotionally charged topics. Results: We found that positive experiences were shared regarding patient treatment, raising support, and spreading awareness. Further discussions related to healthcare were prevalent and largely negative focusing on fear of political legislation that could result in loss of coverage. Conclusions: Social media can provide a positive outlet for patients to discuss their needs and concerns regarding their healthcare coverage and treatment needs. Capturing iPROs from online communication can help inform healthcare professionals and lead to more connected and personalized treatment regimens. "	1805.09959	Eric M. Clark, Ted James, Chris A. Jones, Amulya Alapati, Promise Ukandu, Christopher M. Danforth, Peter Sheridan Dodds	2018-05-24		7	2	A Sentiment Analysis of Breast Cancer Treatment Experiences and Healthcare Perceptions Across Twitter		cs.CL, cs.SI	https://arxiv.org/pdf/1805.09959.pdf
218577	 In the context of Twitter, social capitalists are specific users trying to increase their number of followers and interactions by any means. These users are not healthy for the Twitter network since they flaw notions of influence and visibility. Indeed, it has recently been observed that they are real and active users that can help malicious users such as spammers gaining influence. Studying their behavior and understanding their position in Twitter is thus of important interest. A recent work provided an efficient way to detect social capitalists using two simple topological measures. Based on this detection method, we study how social capitalists are distributed over Twitter's friend-to-follower network. We are especially interested in analyzing how they are organized, and how their links spread across the network. Answering these questions allows to know whether the social capitalism methods increase the actual visibility on the service. To that aim, we study the position of social capitalists on Twitter w.r.t. the community structure of the network. We base our work on the concept of community role of a node, which describes its position in a network depending on its connectivity at the community level. The topological measures originally defined to characterize these roles consider only some aspects of community-related connectivity and rely on a set of empirically fixed thresholds. We first show the limitations of such measures and then extend and generalize them by considering new aspects of the community-related connectivity. Moreover, we use an unsupervised approach to distinguish the roles, in order to provide more flexibility relatively to the studied system. We then apply our method to the case of social capitalists and show that they are highly visible on Twitter, due to the specific roles they occupy. 	1406.6611	Vincent Labatut, Nicolas Dugué, Anthony Perez	2014-06-25	10.1109/ASONAM.2014.6921612	3	2	Identifying the Community Roles of Social Capitalists in the Twitter Network		cs.SI, physics.soc-ph	https://arxiv.org/pdf/1406.6611.pdf
231034	 With social media penetration deepening among both citizens and political figures, there is a pressing need to understand whether and how political use of major platforms is electorally influential. Particularly, the literature focused on campaign usage is thin and often describe the engagement strategies of politicians or attempt to quantify the impact of social media engagement on political learning, participation, or voting. Few have considered implications for campaign fundraising despite its recognized importance in American politics. This paper is the first to quantify a financial payoff for social media campaigning. Drawing on candidate-level data from Facebook and Twitter, Google Trends, Wikipedia page views, and Federal Election Commission (FEC) donation records, we analyze the relationship between the topic and volume of social media content and campaign funds received by all 108 candidates in the 2016 US Senate general elections. By applying an unsupervised learning approach to identify themes in candidate content across the platforms, we find that more frequent posting overall and of issue-related content are associated with higher donation income when controlling for incumbency, state population, and information-seeking about a candidate, though campaigning-related content has a stronger effect than the latter when the number rather than value of donations is considered. 	1711.10380	Lily McElwee, Taha Yasseri	2017-11-28		2	3	Social Media, Money, and Politics: Campaign Finance in the 2016 US Congressional Cycle		physics.soc-ph, cs.CY, cs.SI	https://arxiv.org/pdf/1711.10380.pdf
234130	 Twitter has increasingly become a popular platform to share news and user opinion. A tweet is considered to be important if it receives high number of affirmative reactions from other Twitter users via Retweets. Retweet count is thus considered as a surrogate measure for positive crowd-sourced reactions - high number of retweets of a tweet aid in making its topic trending. This in turn bolsters the social reputation of the author of the tweet. Since social reputation/impact of users/tweets influences many decisions (such as promoting brands, advertisement etc.), several blackmarket syndicates have actively been engaged in producing fake retweets in a collusive manner. Users who want to boost the impact of their tweets approach the blackmarket services, and gain retweets for their own tweets by either paying money (Premium Services) or by retweeting other customers' tweets. Thus they become customers of blackmarket syndicates and engage in fake activities. Interestingly, these customers are neither bots, nor even fake users - they are usually normal human beings; they express a mix of organic and inorganic retweeting activities, and there is no synchronicity across their behaviors. In this paper, we make a first attempt to investigate such blackmarket customers engaged in producing fake retweets. We collected and annotated a novel dataset comprising of customers of many blackmarket services and show how their social behavior differs from genuine users. We then use state-of-the-art supervised models to detect three types of customers (bots, promotional, normal) and genuine users. We achieve a Macro F1-score of 0.87 with SVM, outperforming four other baselines significantly. We further design a browser extension, SCoRe which, given the link of a tweet, spots its fake retweeters in real-time. We also collected users' feedback on the performance of SCoRe and obtained 85% accuracy. 	1806.08979	Hridoy Sankar Dutta, Aditya Chetan, Brihi Joshi, Tanmoy Chakraborty	2018-06-23		4	1	Retweet Us, We Will Retweet You: Spotting Collusive Retweeters Involved in Blackmarket Services		cs.SI	https://arxiv.org/pdf/1806.08979.pdf
394444	 In recent years, Twitter has seen a proliferation of automated accounts or bots that send spam, offer clickbait, compromise security using malware, and attempt to skew public opinion. Previous research estimates that around 9% to 17% of Twitter accounts are bots contributing to between 16% to 56% of tweets on the medium. This paper introduces an unsupervised approach to detect Twitter spam campaigns in real-time. The bot groups we detect tweet duplicate content with shortened embedded URLs over extended periods of time. Our experiments with the detection protocol reveal that bots consistently account for 10% to 50% of tweets generated from 7 popular URL shortening services on Twitter. More importantly, we discover that bots using shortened URLs are connected to large scale spam campaigns that control thousands of domains. There appear to be two distinct mechanisms used to control bot groups and we investigate both in this paper. Our detection system runs 24/7 and actively collects bots involved in spam campaigns and adds them to an evolving database of malicious bots. We make our database of detected bots available for query through a REST API so others can filter tweets from malicious bots to get high quality Twitter datasets for analysis. 	1804.05232	Zhouhan Chen, Devika Subramanian	2018-04-14		2	1	An Unsupervised Approach to Detect Spam Campaigns that Use Botnets on Twitter		cs.SI	https://arxiv.org/pdf/1804.05232.pdf
404956	" Over the past few years, online bullying and aggression have become increasingly prominent, and manifested in many different forms on social media. However, there is little work analyzing the characteristics of abusive users and what distinguishes them from typical social media users. In this paper, we start addressing this gap by analyzing tweets containing a great large amount of abusiveness. We focus on a Twitter dataset revolving around the Gamergate controversy, which led to many incidents of cyberbullying and cyberaggression on various gaming and social media platforms. We study the properties of the users tweeting about Gamergate, the content they post, and the differences in their behavior compared to typical Twitter users. We find that while their tweets are often seemingly about aggressive and hateful subjects, ""Gamergaters"" do not exhibit common expressions of online anger, and in fact primarily differ from typical users in that their tweets are less joyful. They are also more engaged than typical Twitter users, which is an indication as to how and why this controversy is still ongoing. Surprisingly, we find that Gamergaters are less likely to be suspended by Twitter, thus we analyze their properties to identify differences from typical users and what may have led to their suspension. We perform an unsupervised machine learning analysis to detect clusters of users who, though currently active, could be considered for suspension since they exhibit similar behaviors with suspended users. Finally, we confirm the usefulness of our analyzed features by emulating the Twitter suspension mechanism with a supervised learning method, achieving very good precision and recall. "	1705.03345	Despoina Chatzakou, Nicolas Kourtellis, Jeremy Blackburn, Emiliano De Cristofaro, Gianluca Stringhini, Athena Vakali	2017-05-09		6	2	Hate is not Binary: Studying Abusive Behavior of #GamerGate on Twitter		cs.SI, cs.CR	https://arxiv.org/pdf/1705.03345.pdf
413466	 Cybercriminals have leveraged the popularity of a large user base available on Online Social Networks to spread spam campaigns by propagating phishing URLs, attaching malicious contents, etc. However, another kind of spam attacks using phone numbers has recently become prevalent on OSNs, where spammers advertise phone numbers to attract users' attention and convince them to make a call to these phone numbers. The dynamics of phone number based spam is different from URL-based spam due to an inherent trust associated with a phone number. While previous work has proposed strategies to mitigate URL-based spam attacks, phone number based spam attacks have received less attention. In this paper, we aim to detect spammers that use phone numbers to promote campaigns on Twitter. To this end, we collected information about 3,370 campaigns spread by 670,251 users. We model the Twitter dataset as a heterogeneous network by leveraging various interconnections between different types of nodes present in the dataset. In particular, we make the following contributions: (i) We propose a simple yet effective metric, called Hierarchical Meta-Path Score (HMPS) to measure the proximity of an unknown user to the other known pool of spammers. (ii) We design a feedback-based active learning strategy and show that it significantly outperforms three state-of-the-art baselines for the task of spam detection. Our method achieves 6.9% and 67.3% higher F1-score and AUC, respectively compared to the best baseline method. (iii) To overcome the problem of less training instances for supervised learning, we show that our proposed feedback strategy achieves 25.6% and 46% higher F1-score and AUC respectively than other oversampling strategies. Finally, we perform a case study to show how our method is capable of detecting those users as spammers who have not been suspended by Twitter (and other baselines) yet. 	1802.04168	Srishti Gupta, Abhinav Khattar, Arpit Gogia, Ponnurangam Kumaraguru, Tanmoy Chakraborty	2018-02-12		5	1	Collective Classification of Spam Campaigners on Twitter: A Hierarchical Meta-Path Based Approach		cs.SI	https://arxiv.org/pdf/1802.04168.pdf
447526	 In this work, we tackle the problem of predicting entity popularity on Twitter based on the news cycle. We apply a supervised learn- ing approach and extract four types of features: (i) signal, (ii) textual, (iii) sentiment and (iv) semantic, which we use to predict whether the popularity of a given entity will be high or low in the following hours. We run several experiments on six different entities in a dataset of over 150M tweets and 5M news and obtained F1 scores over 0.70. Error analysis indicates that news perform better on predicting entity popularity on Twitter when they are the primary information source of the event, in opposition to events such as live TV broadcasts, political debates or football matches. 	1607.03057	Pedro Saleiro, Carlos Soares	2016-07-11		2	1	Learning from the News: Predicting Entity Popularity on Twitter		cs.SI	https://arxiv.org/pdf/1607.03057.pdf
452572	 Censorship in social media has been well studied and provides insight into how governments stifle freedom of expression online. Comparatively less (or no) attention has been paid to detecting (self) censorship in traditional media (e.g., news) using social media as a bellweather. We present a novel unsupervised approach that views social media as a sensor to detect censorship in news media wherein statistically significant differences between information published in the news media and the correlated information published in social media are automatically identified as candidate censored events. We develop a hypothesis testing framework to identify and evaluate censored clusters of keywords, and a new near-linear-time algorithm (called GraphDPD) to identify the highest scoring clusters as indicators of censorship. We outline extensive experiments on semi-synthetic data as well as real datasets (with Twitter and local news media) from Mexico and Venezuela, highlighting the capability to accurately detect real-world self censorship events. 	1611.06947	Rongrong Tao, Baojian Zhou, Feng Chen, Naifeng Liu, David Mares, Patrick Butler, Naren Ramakrishnan	2016-11-21		7	1	Can Self-Censorship in News Media be Detected Algorithmically? A Case Study in Latin America	2017-03-17	cs.SI	https://arxiv.org/pdf/1611.06947.pdf
458057	" Much work in Social Network Analysis has focused on the identification of the most important actors in a social network. This has resulted in several measures of influence and authority. While most of such sociometrics (e.g., PageRank) are driven by intuitions based on an actors location in a network, asking for the ""most influential"" actors in itself is an ill-posed question, unless it is put in context with a specific measurable task. Constructing a predictive task of interest in a given domain provides a mechanism to quantitatively compare different measures of influence. Furthermore, when we know what type of actionable insight to gather, we need not rely on a single network centrality measure. A combination of measures is more likely to capture various aspects of the social network that are predictive and beneficial for the task. Towards this end, we propose an approach to supervised rank aggregation, driven by techniques from Social Choice Theory. We illustrate the effectiveness of this method through experiments on Twitter and citation networks. "	1108.4801	Karthik Subbian, Prem Melville	2011-08-24		2	4	Supervised Rank Aggregation for Predicting Influence in Networks		cs.SI, cs.GT, cs.IR, physics.soc-ph	https://arxiv.org/pdf/1108.4801.pdf
482122	 Social media is often viewed as a sensor into various societal events such as disease outbreaks, protests, and elections. We describe the use of social media as a crowdsourced sensor to gain insight into ongoing cyber-attacks. Our approach detects a broad range of cyber-attacks (e.g., distributed denial of service (DDOS) attacks, data breaches, and account hijacking) in an unsupervised manner using just a limited fixed set of seed event triggers. A new query expansion strategy based on convolutional kernels and dependency parses helps model reporting structure and aids in identifying key event characteristics. Through a large-scale analysis over Twitter, we demonstrate that our approach consistently identifies and encodes events, outperforming existing methods. 	1702.07745	Rupinder Paul Khandpur, Taoran Ji, Steve Jan, Gang Wang, Chang-Tien Lu, Naren Ramakrishnan	2017-02-24	10.1145/3132847.3132866	6	4	Crowdsourcing Cybersecurity: Cyber Attack Detection using Social Media		cs.CR, cs.HC, cs.IR, cs.SI	https://arxiv.org/pdf/1702.07745.pdf
484848	 Modelling user voting intention in social media is an important research area, with applications in analysing electorate behaviour, online political campaigning and advertising. Previous approaches mainly focus on predicting national general elections, which are regularly scheduled and where data of past results and opinion polls are available. However, there is no evidence of how such models would perform during a sudden vote under time-constrained circumstances. That poses a more challenging task compared to traditional elections, due to its spontaneous nature. In this paper, we focus on the 2015 Greek bailout referendum, aiming to nowcast on a daily basis the voting intention of 2,197 Twitter users. We propose a semi-supervised multiple convolution kernel learning approach, leveraging temporally sensitive text and network information. Our evaluation under a real-time simulation framework demonstrates the effectiveness and robustness of our approach against competitive baselines, achieving a significant 20% increase in F-score compared to solely text-based models. 	1808.08538	Adam Tsakalidis, Nikolaos Aletras, Alexandra I. Cristea, Maria Liakata	2018-08-26	10.1145/3269206.3271783	4	3	Nowcasting the Stance of Social Media Users in a Sudden Vote: The Case of the Greek Referendum		cs.CY, cs.CL, cs.SI	https://arxiv.org/pdf/1808.08538.pdf
502755	 Most current approaches to characterize and detect hate speech focus on \textit{content} posted in Online Social Networks. They face shortcomings to collect and annotate hateful speech due to the incompleteness and noisiness of OSN text and the subjectivity of hate speech. These limitations are often aided with constraints that oversimplify the problem, such as considering only tweets containing hate-related words. In this work we partially address these issues by shifting the focus towards \textit{users}. We develop and employ a robust methodology to collect and annotate hateful users which does not depend directly on lexicon and where the users are annotated given their entire profile. This results in a sample of Twitter's retweet graph containing $100,386$ users, out of which $4,972$ were annotated. We also collect the users who were banned in the three months that followed the data collection. We show that hateful users differ from normal ones in terms of their activity patterns, word usage and as well as network structure. We obtain similar results comparing the neighbors of hateful vs. neighbors of normal users and also suspended users vs. active users, increasing the robustness of our analysis. We observe that hateful users are densely connected, and thus formulate the hate speech detection problem as a task of semi-supervised learning over a graph, exploiting the network of connections on Twitter. We find that a node embedding algorithm, which exploits the graph structure, outperforms content-based approaches for the detection of both hateful ($95\%$ AUC vs $88\%$ AUC) and suspended users ($93\%$ AUC vs $88\%$ AUC). Altogether, we present a user-centric view of hate speech, paving the way for better detection and understanding of this relevant and challenging issue. 	1803.08977	Manoel Horta Ribeiro, Pedro H. Calais, Yuri A. Santos, Virgílio A. F. Almeida, Wagner Meira	2018-03-23		5	2	Characterizing and Detecting Hateful Users on Twitter		cs.CY, cs.SI	https://arxiv.org/pdf/1803.08977.pdf
561217	 In this paper, we propose a unsupervised framework to reconstruct a person's life history by creating a chronological list for {\it personal important events} (PIE) of individuals based on the tweets they published. By analyzing individual tweet collections, we find that what are suitable for inclusion in the personal timeline should be tweets talking about personal (as opposed to public) and time-specific (as opposed to time-general) topics. To further extract these types of topics, we introduce a non-parametric multi-level Dirichlet Process model to recognize four types of tweets: personal time-specific (PersonTS), personal time-general (PersonTG), public time-specific (PublicTS) and public time-general (PublicTG) topics, which, in turn, are used for further personal event extraction and timeline generation. To the best of our knowledge, this is the first work focused on the generation of timeline for individuals from twitter data. For evaluation, we have built a new golden standard Timelines based on Twitter and Wikipedia that contain PIE related events from 20 {\it ordinary twitter users} and 20 {\it celebrities}. Experiments on real Twitter data quantitatively demonstrate the effectiveness of our approach. 	1309.7313	Jiwei Li, Claire Cardie	2013-09-27		2	2	Timeline Generation: Tracking individuals on Twitter	2014-02-07	cs.SI, cs.IR	https://arxiv.org/pdf/1309.7313.pdf
564515	 Internet users and businesses are increasingly using online social networks (OSN) to drive audience traffic and increase their popularity. In order to boost social presence, OSN users need to increase the visibility and reach of their online profile, like - Facebook likes, Twitter followers, Instagram comments and Yelp reviews. For example, an increase in Twitter followers not only improves the audience reach of the user but also boosts the perceived social reputation and popularity. This has led to a scope for an underground market that provides followers, likes, comments, etc. via a network of fraudulent and compromised accounts and various collusion techniques. In this paper, we landscape the underground markets that provide Twitter followers by studying their basic building blocks - merchants, customers and phony followers. We charecterize the services provided by merchants to understand their operational structure and market hierarchy. Twitter underground markets can operationalize using a premium monetary scheme or other incentivized freemium schemes. We find out that freemium market has an oligopoly structure with few merchants being the market leaders. We also show that merchant popularity does not have any correlation with the quality of service provided by the merchant to its customers. Our findings also shed light on the characteristics and quality of market customers and the phony followers provided. We draw comparison between legitimate users and phony followers, and find out key identifiers to separate such users. With the help of these differentiating features, we build a supervised learning model to predict suspicious following behaviour with an accuracy of 89.2%. 	1411.4299	Anupama Aggarwal, Ponnurangam Kumaraguru	2014-11-16		2	1	What They Do in Shadows: Twitter Underground Follower Market		cs.SI	https://arxiv.org/pdf/1411.4299.pdf
568377	 The growing popularity of social media (e.g, Twitter) allows users to easily share information with each other and influence others by expressing their own sentiments on various subjects. In this work, we propose an unsupervised \emph{tri-clustering} framework, which analyzes both user-level and tweet-level sentiments through co-clustering of a tripartite graph. A compelling feature of the proposed framework is that the quality of sentiment clustering of tweets, users, and features can be mutually improved by joint clustering. We further investigate the evolution of user-level sentiments and latent feature vectors in an online framework and devise an efficient online algorithm to sequentially update the clustering of tweets, users and features with newly arrived data. The online framework not only provides better quality of both dynamic user-level and tweet-level sentiment analysis, but also improves the computational and storage efficiency. We verified the effectiveness and efficiency of the proposed approaches on the November 2012 California ballot Twitter data. 	1402.6010	Linhong Zhu, Aram Galstyan, James Cheng, Kristina Lerman	2014-02-24	10.1145/2588555.2593682	4	3	Tripartite Graph Clustering for Dynamic Sentiment Analysis on Social Media	2014-06-12	cs.SI, cs.CL, cs.IR	https://arxiv.org/pdf/1402.6010.pdf
595806	 Organizational relationships are usually very complex in real life. It is difficult or impossible to directly measure such correlations among different organizations, because important information is usually not publicly available (e.g., the correlations of terrorist organizations). Nowadays, an increasing amount of organizational information can be posted online by individuals and spread instantly through Twitter. Such information can be crucial for detecting organizational correlations. In this paper, we study the problem of discovering correlations among organizations from Twitter. Mining organizational correlations is a very challenging task due to the following reasons: a) Data in Twitter occurs as large volumes of mixed information. The most relevant information about organizations is often buried. Thus, the organizational correlations can be scattered in multiple places, represented by different forms; b) Making use of information from Twitter collectively and judiciously is difficult because of the multiple representations of organizational correlations that are extracted. In order to address these issues, we propose multi-CG (multiple Correlation Graphs based model), an unsupervised framework that can learn a consensus of correlations among organizations based on multiple representations extracted from Twitter, which is more accurate and robust than correlations based on a single representation. Empirical study shows that the consensus graph extracted from Twitter can capture the organizational correlations effectively. 	1410.6001	Jingyuan Zhang, Xiaoxiao Shi, Xiangnan Kong, Hong-Han Shuai, Philip S. Yu	2014-10-22		5	2	Discovering Organizational Correlations from Twitter		cs.SI, physics.soc-ph	https://arxiv.org/pdf/1410.6001.pdf
605991	 Automated social agents, or bots, are increasingly becoming a problem on social media platforms. There is a growing body of literature and multiple tools to aid in the detection of such agents on online social networking platforms. We propose that the social network topology of a user would be sufficient to determine whether the user is a automated agent or a human. To test this, we use a publicly available dataset containing users on Twitter labelled as either automated social agent or human. Using an unsupervised machine learning approach, we obtain a detection accuracy rate of 70%. 	1809.06190	Laurenz A Cornelissen, Richard J Barnett, Petrus Schoonwinkel, Brent D. Eichstadt, Hluma B. Magodla	2018-09-17	10.1145/3278681.3278692	5	2	A Network Topology Approach to Bot Classification		cs.SI, physics.soc-ph	https://arxiv.org/pdf/1809.06190.pdf
793156	 We developed computational models to predict the emergence of depression and Post-Traumatic Stress Disorder in Twitter users. Twitter data and details of depression history were collected from 204 individuals (105 depressed, 99 healthy). We extracted predictive features measuring affect, linguistic style, and context from participant tweets (N=279,951) and built models using these features with supervised learning algorithms. Resulting models successfully discriminated between depressed and healthy content, and compared favorably to general practitioners' average success rates in diagnosing depression. Results held even when the analysis was restricted to content posted before first depression diagnosis. State-space temporal analysis suggests that onset of depression may be detectable from Twitter data several months prior to diagnosis. Predictive results were replicated with a separate sample of individuals diagnosed with PTSD (174 users, 243,775 tweets). A state-space time series model revealed indicators of PTSD almost immediately post-trauma, often many months prior to clinical diagnosis. These methods suggest a data-driven, predictive approach for early screening and detection of mental illness. 	1608.07740	Andrew G. Reece, Andrew J. Reagan, Katharina L. M. Lix, Peter Sheridan Dodds, Christopher M. Danforth, Ellen J. Langer	2016-08-27		6	2	Forecasting the onset and course of mental illness with Twitter data		physics.soc-ph, cs.SI	https://arxiv.org/pdf/1608.07740.pdf
1062701	 Compromised social media accounts are legitimate user accounts that have been hijacked by a third (malicious) party and can cause various kinds of damage. Early detection of such compromised accounts is very important in order to control the damage. In this work we propose a novel general framework for discovering compromised accounts by utilizing statistical text analysis. The framework is built on the observation that users will use language that is measurably different from the language that a hacker (or spammer) would use, when the account is compromised. We use the framework to develop specific algorithms based on language modeling and use the similarity of language models of users and spammers as features in a supervised learning setup to identify compromised accounts. Evaluation results on a large Twitter corpus of over 129 million tweets show promising results of the proposed approach. 	1804.07247	Dominic Seyler, Lunan Li, ChengXiang Zhai	2018-04-19		3	3	Identifying Compromised Accounts on Social Media Using Statistical Text Analysis		cs.SI, cs.CL, cs.CR	https://arxiv.org/pdf/1804.07247.pdf
1161871	 This is a report, where preliminary work regarding the topic of voting intention inference from Social Media - such as Twitter - is presented. Our case study is the UK 2010 General Election and we are focusing on predicting the percentages of voting intention polls (conducted by YouGov) for the three major political parties - Conservatives, Labours and Liberal Democrats - during a 5-month period before the election date (May 6, 2010). We form three methodologies for extracting positive or negative sentiment from tweets, which build on each other, and then propose two supervised models for turning sentiment into voting intention percentages. Interestingly, when the content of tweets is enriched by attaching synonymous words, a significant improvement on inference performance is achieved reaching a mean absolute error of 4.34% +/- 2.13%; in that case, the predictions are also shown to be statistically significant. The presented methods should be considered as work-in-progress; limitations and suggestions for future work appear in the final section of this script. 	1204.0423	Vasileios Lampos	2012-04-02		1	2	On voting intentions inference from Twitter content: a case study on UK 2010 General Election	2012-05-21	cs.SI, physics.soc-ph	https://arxiv.org/pdf/1204.0423.pdf
1218516	 Online Social Media (OSM) is extensively used by contemporary Internet users to communicate, socialize and disseminate information. This has led to the creation of a distinct online social identity which in turn has created the need of online social reputation management techniques. A significant percentage of OSM users utilize various methods to drive and manage their reputation on OSM. This has given rise to underground markets which buy/sell fraudulent accounts, `likes', `comments' (Facebook, Instagram) and `followers' (Twitter) to artificially boost their social reputation. In this study, we present an anatomy of purchased followers on Twitter and their behaviour. We illustrate in detail the profile characteristics, content sharing and behavioural patterns of purchased follower accounts. Previous studies have analyzed the purchased follower markets and customers. Ours is the first study which analyzes the anatomy of purchased followers accounts. Some of the key insights of our study show that purchased followers have a very high unfollow entropy rate and low social engagement with their friends. In addition, we noticed that purchased follower accounts have significant difference in their interaction and content sharing patterns in comparison to random Twitter users. We also found that underground markets do not follow their service policies and guarantees they provide to customer. Our study highlights the key identifiers for suspicious follow behaviour. We then built a supervised learning mechanism to predict suspicious follower behaviour with 88.2% accuracy. We believe that understanding the anatomy and characteristics of purchased followers can help detect suspicious follower behaviour and fraudulent accounts to a larger extent. 	1408.1534	Anupama Aggarwal, Ponnurangam Kumaraguru	2014-08-07		2	1	Followers or Phantoms? An Anatomy of Purchased Twitter Followers		cs.SI	https://arxiv.org/pdf/1408.1534.pdf
1298722	 In the context of Twitter, social capitalists are specific users trying to increase their number of followers and interactions by any means. These users are not healthy for the service, because they are either spammers or real users flawing the notions of influence and visibility. Studying their behavior and understanding their position in Twit-ter is thus of important interest. It is also necessary to analyze how these methods effectively affect user visibility. Based on a recently proposed method allowing to identify social capitalists, we tackle both points by studying how they are organized, and how their links spread across the Twitter follower-followee network. To that aim, we consider their position in the network w.r.t. its community structure. We use the concept of community role of a node, which describes its position in a network depending on its connectiv-ity at the community level. However, the topological measures originally defined to characterize these roles consider only certain aspects of the community-related connectivity, and rely on a set of empirically fixed thresholds. We first show the limitations of these measures, before extending and generalizing them. Moreover, we use an unsupervised approach to identify the roles, in order to provide more flexibility relatively to the studied system. We then apply our method to the case of social capitalists and show they are highly visible on Twitter, due to the specific roles they hold. 	1506.04571	Nicolas Dugué, Vincent Labatut, Anthony Perez	2015-06-15	10.1007/s13278-015-0266-0	3	2	A community role approach to assess social capitalists visibility in the Twitter network		cs.SI, physics.soc-ph	https://arxiv.org/pdf/1506.04571.pdf
1308122	 Recently, numerous approaches have emerged in the social sciences to exploit the opportunities made possible by the vast amounts of data generated by online social networks (OSNs). Having access to information about users on such a scale opens up a range of possibilities, all without the limitations associated with often slow and expensive paper-based polls. A question that remains to be satisfactorily addressed, however, is how demography is represented in the OSN content? Here, we study language use in the US using a corpus of text compiled from over half a billion geo-tagged messages from the online microblogging platform Twitter. Our intention is to reveal the most important spatial patterns in language use in an unsupervised manner and relate them to demographics. Our approach is based on Latent Semantic Analysis (LSA) augmented with the Robust Principal Component Analysis (RPCA) methodology. We find spatially correlated patterns that can be interpreted based on the words associated with them. The main language features can be related to slang use, urbanization, travel, religion and ethnicity, the patterns of which are shown to correlate plausibly with traditional census data. Our findings thus validate the concept of demography being represented in OSN language use and show that the traits observed are inherently present in the word frequencies without any previous assumptions about the dataset. Thus, they could form the basis of further research focusing on the evaluation of demographic data estimation from other big data sources, or on the dynamical processes that result in the patterns found here. 	1605.02951	Eszter Bokányi, Dániel Kondor, László Dobos, Tamás Sebők, József Stéger, István Csabai, Gábor Vattay	2016-05-10	10.1057/palcomms.2016.10.	7	2	Race, Religion and the City: Twitter Word Frequency Patterns Reveal Dominant Demographic Dimensions in the United States	2016-05-11	physics.soc-ph, cs.SI	https://arxiv.org/pdf/1605.02951.pdf
1308226	 The notion of community structure is particularly useful when analyzing complex networks, because it provides an intermediate level, compared to the more classic global (whole network) and local (node neighborhood) approaches. The concept of community role of a node was derived from this base, in order to describe the position of a node in a network depending on its connectivity at the community level. However, the existing approaches are restricted to undirected networks, use topological measures which do not consider all aspects of community-related connectivity, and their role identification methods are not generalizable to all networks. We tackle these limitations by generalizing and extending the measures, and using an unsupervised approach to determine the roles. We then illustrate the applicability of our method by analyzing a Twitter network.We show how our modifications allow discovering the fact some particular users called social capitalists occupy very specific roles in this system. --- La notion de structure de communaut\'es est particuli\`erement utile pour \'etudier les r\'eseaux complexes, car elle am\`ene un niveau d'analyse interm\'ediaire, par opposition aux plus classiques niveaux local (voisinage des noeuds) et global (r\'eseau entier). Le concept de r\^ole communautaire permet de d\'ecrire le positionnement d'un noeud en fonction de sa connectivit\'e communautaire. Cependant, les approches existantes sont restreintes aux r\'eseaux non-orient\'es, utilisent des mesures topologiques ne consid\'erant pas tous les aspects de la connectivit\'e communautaire, et des m\'ethodes d'identification des r\^oles non-g\'en\'eralisables \`a tous les r\'eseaux. Nous proposons de r\'esoudre ces probl\`emes en g\'en\'eralisant les mesures existantes, et en utilisant une m\'ethode non-supervis\'ee pour d\'eterminer les r\^oles. Nous illustrons l'int\'er\^et de notre m\'ethode en l'appliquant au r\'eseau de Twitter. Nous montrons que nos modifications mettent en \'evidence les r\^oles sp\'ecifiques d'utilisateurs particuliers du r\'eseau, nomm\'es capitalistes sociaux. 	1312.3794	Nicolas Dugué, Vincent Labatut, Anthony Perez	2013-12-13		3	1	Identification de r\^oles communautaires dans des r\'eseaux orient\'es appliqu\'ee \`a Twitter		cs.SI	https://arxiv.org/pdf/1312.3794.pdf
1320046	 Influenza is an acute respiratory illness that occurs virtually every year and results in substantial disease, death and expense. Detection of Influenza in its earliest stage would facilitate timely action that could reduce the spread of the illness. Existing systems such as CDC and EISS which try to collect diagnosis data, are almost entirely manual, resulting in about two-week delays for clinical data acquisition. Twitter, a popular microblogging service, provides us with a perfect source for early-stage flu detection due to its real- time nature. For example, when a flu breaks out, people that get the flu may post related tweets which enables the detection of the flu breakout promptly. In this paper, we investigate the real-time flu detection problem on Twitter data by proposing Flu Markov Network (Flu-MN): a spatio-temporal unsupervised Bayesian algorithm based on a 4 phase Markov Network, trying to identify the flu breakout at the earliest stage. We test our model on real Twitter datasets from the United States along with baselines in multiple applications, such as real-time flu breakout detection, future epidemic phase prediction, or Influenza-like illness (ILI) physician visits. Experimental results show the robustness and effectiveness of our approach. We build up a real time flu reporting system based on the proposed approach, and we are hopeful that it would help government or health organizations in identifying flu outbreaks and facilitating timely actions to decrease unnecessary mortality. 	1309.7340	Jiwei Li, Claire Cardie	2013-09-27		2	2	Early Stage Influenza Detection from Twitter	2013-11-18	cs.SI, cs.CL	https://arxiv.org/pdf/1309.7340.pdf
1437233	 We present SAVITR, a system that leverages the information posted on the Twitter microblogging site to monitor and analyse emergency situations. Given that only a very small percentage of microblogs are geo-tagged, it is essential for such a system to extract locations from the text of the microblogs. We employ natural language processing techniques to infer the locations mentioned in the microblog text, in an unsupervised fashion and display it on a map-based interface. The system is designed for efficient performance, achieving an F-score of 0.79, and is approximately two orders of magnitude faster than other available tools for location extraction. 	1801.07757	Ritam Dutt, Kaustubh Hiware, Avijit Ghosh, Rameshwar Bhaskaran	2018-01-23		4	1	SAVITR: A System for Real-time Location Extraction from Microblogs during Emergencies		cs.SI	https://arxiv.org/pdf/1801.07757.pdf
42234	 In this paper, we present a customizable datacentric system that automatically generates common misspellings for complex health-related terms. The spelling variant generator relies on a dense vector model learned from large unlabeled text, which is used to find semantically close terms to the original/seed keyword, followed by the filtering of terms that are lexically dissimilar beyond a given threshold. The process is executed recursively, converging when no new terms similar (lexically and semantically) to the seed keyword are found. Weighting of intra-word character sequence similarities allows further problem-specific customization of the system. On a dataset prepared for this study, our system outperforms the current state-of-the-art for medication name variant generation with best F1-score of 0.69 and F1/4-score of 0.78. Extrinsic evaluation of the system on a set of cancer-related terms showed an increase of over 67% in retrieval rate from Twitter posts when the generated variants are included. Our proposed spelling variant generator has several advantages over the current state-of-the-art and other types of variant generators-(i) it is capable of filtering out lexically similar but semantically dissimilar terms, (ii) the number of variants generated is low as many low-frequency and ambiguous misspellings are filtered out, and (iii) the system is fully automatic, customizable and easily executable. While the base system is fully unsupervised, we show how supervision maybe employed to adjust weights for task-specific customization. The performance and significant relative simplicity of our proposed approach makes it a much needed misspelling generation resource for health-related text mining from noisy sources. The source code for the system has been made publicly available for research purposes. 	1806.00910	Abeed Sarker, Graciela Gonzalez-Hernandez	2018-06-03		2	1	An unsupervised and customizable misspelling generator for mining noisy health-related text sources		cs.CL	https://arxiv.org/pdf/1806.00910.pdf
81860	 We study the problem of evaluating automatic speech recognition (ASR) systems that target dialectal speech input. A major challenge in this case is that the orthography of dialects is typically not standardized. From an ASR evaluation perspective, this means that there is no clear gold standard for the expected output, and several possible outputs could be considered correct according to different human annotators, which makes standard word error rate (WER) inadequate as an evaluation metric. Such a situation is typical for machine translation (MT), and thus we borrow ideas from an MT evaluation metric, namely TERp, an extension of translation error rate which is closely-related to WER. In particular, in the process of comparing a hypothesis to a reference, we make use of spelling variants for words and phrases, which we mine from Twitter in an unsupervised fashion. Our experiments with evaluating ASR output for Egyptian Arabic, and further manual analysis, show that the resulting WERd (i.e., WER for dialects) metric, a variant of TERp, is more adequate than WER for evaluating dialectal ASR. 	1709.07484	Ahmed Ali, Preslav Nakov, Peter Bell, Steve Renals	2017-09-21		4	1	WERd: Using Social Text Spelling Variants for Evaluating Dialectal Speech Recognition		cs.CL	https://arxiv.org/pdf/1709.07484.pdf
196381	 Social media platforms such as Twitter and Facebook are becoming popular in multilingual societies. This trend induces portmanteau of South Asian languages with English. The blend of multiple languages as code-mixed data has recently become popular in research communities for various NLP tasks. Code-mixed data consist of anomalies such as grammatical errors and spelling variations. In this paper, we leverage the contextual property of words where the different spelling variation of words share similar context in a large noisy social media text. We capture different variations of words belonging to same context in an unsupervised manner using distributed representations of words. Our experiments reveal that preprocessing of the code-mixed dataset based on our approach improves the performance in state-of-the-art part-of-speech tagging (POS-tagging) and sentiment analysis tasks. 	1804.00804	Rajat Singh, Nurendra Choudhary, Manish Shrivastava	2018-04-02		3	1	Automatic Normalization of Word Variations in Code-Mixed Social Media Text		cs.CL	https://arxiv.org/pdf/1804.00804.pdf
212951	 We describe TweeTIME, a temporal tagger for recognizing and normalizing time expressions in Twitter. Most previous work in social media analysis has to rely on temporal resolvers that are designed for well-edited text, and therefore suffer from the reduced performance due to domain mismatch. We present a minimally supervised method that learns from large quantities of unlabeled data and requires no hand-engineered rules or hand-annotated training corpora. TweeTIME achieves 0.68 F1 score on the end-to-end task of resolving date expressions, outperforming a broad range of state-of-the-art systems. 	1608.02904	Jeniya Tabassum, Alan Ritter, Wei Xu	2016-08-09		3	2	TweeTime: A Minimally Supervised Method for Recognizing and Normalizing Time Expressions in Twitter	2016-10-01	cs.IR, cs.CL	https://arxiv.org/pdf/1608.02904.pdf
222537	 Adverse drug reactions (ADRs) are one of the leading causes of mortality in health care. Current ADR surveillance systems are often associated with a substantial time lag before such events are officially published. On the other hand, online social media such as Twitter contain information about ADR events in real-time, much before any official reporting. Current state-of-the-art methods in ADR mention extraction use Recurrent Neural Networks (RNN), which typically need large labeled corpora. Towards this end, we propose a semi-supervised method based on co-training which can exploit a large pool of unlabeled tweets to augment the limited supervised training data, and as a result enhance the performance. Experiments with 0.1M tweets show that the proposed approach outperforms the state-of-the-art methods for the ADR mention extraction task by 5% in terms of F1 score. 	1802.05121	Shashank Gupta, Manish Gupta, Vasudeva Varma, Sachin Pawar, Nitin Ramrakhiyani, Girish K. Palshikar	2018-02-14		6	2	Co-training for Extraction of Adverse Drug Reaction Mentions from Tweets		cs.IR, cs.CL	https://arxiv.org/pdf/1802.05121.pdf
714466	 This paper describes a pilot NER system for Twitter, comprising the USFD system entry to the W-NUT 2015 NER shared task. The goal is to correctly label entities in a tweet dataset, using an inventory of ten types. We employ structured learning, drawing on gazetteers taken from Linked Data, and on unsupervised clustering features, and attempting to compensate for stylistic and topic drift - a key challenge in social media text. Our result is competitive; we provide an analysis of the components of our methodology, and an examination of the target dataset in the context of this task. 	1511.03088	Leon Derczynski, Isabelle Augenstein, Kalina Bontcheva	2015-11-10		3	1	USFD: Twitter NER with Drift Compensation and Linked Data		cs.CL	https://arxiv.org/pdf/1511.03088.pdf
887713	 With the rise of social media, millions of people are routinely expressing their moods, feelings, and daily struggles with mental health issues on social media platforms like Twitter. Unlike traditional observational cohort studies conducted through questionnaires and self-reported surveys, we explore the reliable detection of clinical depression from tweets obtained unobtrusively. Based on the analysis of tweets crawled from users with self-reported depressive symptoms in their Twitter profiles, we demonstrate the potential for detecting clinical depression symptoms which emulate the PHQ-9 questionnaire clinicians use today. Our study uses a semi-supervised statistical model to evaluate how the duration of these symptoms and their expression on Twitter (in terms of word usage patterns and topical preferences) align with the medical findings reported via the PHQ-9. Our proactive and automatic screening tool is able to identify clinical depressive symptoms with an accuracy of 68% and precision of 72%. 	1710.05429	Amir Hossein Yazdavar, Hussein S. Al-Olimat, Monireh Ebrahimi, Goonmeet Bajaj, Tanvi Banerjee, Krishnaprasad Thirunarayan, Jyotishman Pathak, Amit Sheth	2017-10-15		8	1	Semi-Supervised Approach to Monitoring Clinical Depressive Symptoms in Social Media		cs.CL	https://arxiv.org/pdf/1710.05429.pdf
927928	 Adverse drug reactions (ADRs) are one of the leading causes of mortality in health care. Current ADR surveillance systems are often associated with a substantial time lag before such events are officially published. On the other hand, online social media such as Twitter contain information about ADR events in real-time, much before any official reporting. Current state-of-the-art in ADR mention extraction uses Recurrent Neural Networks (RNN), which typically need large labeled corpora. Towards this end, we propose a multi-task learning based method which can utilize a similar auxiliary task (adverse drug event detection) to enhance the performance of the main task, i.e., ADR extraction. Furthermore, in the absence of auxiliary task dataset, we propose a novel joint multi-task learning method to automatically generate weak supervision dataset for the auxiliary task when a large pool of unlabeled tweets is available. Experiments with 0.48M tweets show that the proposed approach outperforms the state-of-the-art methods for the ADR mention extraction task by 7.2% in terms of F1 score. 	1802.05130	Shashank Gupta, Manish Gupta, Vasudeva Varma, Sachin Pawar, Nitin Ramrakhiyani, Girish K. Palshikar	2018-02-14		6	2	Multi-Task Learning for Extraction of Adverse Drug Reaction Mentions from Tweets		cs.IR, cs.CL	https://arxiv.org/pdf/1802.05130.pdf
995947	 Analysis of informative contents and sentiments of social users has been attempted quite intensively in the recent past. Most of the systems are usable only for monolingual data and fails or gives poor results when used on data with code-mixing property. To gather attention and encourage researchers to work on this crisis, we prepared gold standard Bengali-English code-mixed data with language and polarity tag for sentiment analysis purposes. In this paper, we discuss the systems we prepared to collect and filter raw Twitter data. In order to reduce manual work while annotation, hybrid systems combining rule based and supervised models were developed for both language and sentiment tagging. The final corpus was annotated by a group of annotators following a few guidelines. The gold standard corpus thus obtained has impressive inter-annotator agreement obtained in terms of Kappa values. Various metrics like Code-Mixed Index (CMI), Code-Mixed Factor (CF) along with various aspects (language and emotion) also qualitatively polled the code-mixed and sentiment properties of the corpus. 	1803.04000	Soumil Mandal, Sainik Kumar Mahata, Dipankar Das	2018-03-11		3	1	Preparing Bengali-English Code-Mixed Corpus for Sentiment Analysis of Indian Languages		cs.CL	https://arxiv.org/pdf/1803.04000.pdf
1175216	 Predicting stock market movements is a well-known problem of interest. Now-a-days social media is perfectly representing the public sentiment and opinion about current events. Especially, twitter has attracted a lot of attention from researchers for studying the public sentiments. Stock market prediction on the basis of public sentiments expressed on twitter has been an intriguing field of research. Previous studies have concluded that the aggregate public mood collected from twitter may well be correlated with Dow Jones Industrial Average Index (DJIA). The thesis of this work is to observe how well the changes in stock prices of a company, the rises and falls, are correlated with the public opinions being expressed in tweets about that company. Understanding author's opinion from a piece of text is the objective of sentiment analysis. The present paper have employed two different textual representations, Word2vec and N-gram, for analyzing the public sentiments in tweets. In this paper, we have applied sentiment analysis and supervised machine learning principles to the tweets extracted from twitter and analyze the correlation between stock market movements of a company and sentiments in tweets. In an elaborate way, positive news and tweets in social media about a company would definitely encourage people to invest in the stocks of that company and as a result the stock price of that company would increase. At the end of the paper, it is shown that a strong correlation exists between the rise and falls in stock prices with the public sentiments in tweets. 	1610.09225	Venkata Sasank Pagolu, Kamal Nayan Reddy Challa, Ganapati Panda, Babita Majhi	2016-10-28		4	3	Sentiment Analysis of Twitter Data for Predicting Stock Market Movements		cs.IR, cs.CL, cs.SI	https://arxiv.org/pdf/1610.09225.pdf
1292167	 Social media is an useful platform to share health-related information due to its vast reach. This makes it a good candidate for public-health monitoring tasks, specifically for pharmacovigilance. We study the problem of extraction of Adverse-Drug-Reaction (ADR) mentions from social media, particularly from twitter. Medical information extraction from social media is challenging, mainly due to short and highly information nature of text, as compared to more technical and formal medical reports. Current methods in ADR mention extraction relies on supervised learning methods, which suffers from labeled data scarcity problem. The State-of-the-art method uses deep neural networks, specifically a class of Recurrent Neural Network (RNN) which are Long-Short-Term-Memory networks (LSTMs) \cite{hochreiter1997long}. Deep neural networks, due to their large number of free parameters relies heavily on large annotated corpora for learning the end task. But in real-world, it is hard to get large labeled data, mainly due to heavy cost associated with manual annotation. Towards this end, we propose a novel semi-supervised learning based RNN model, which can leverage unlabeled data also present in abundance on social media. Through experiments we demonstrate the effectiveness of our method, achieving state-of-the-art performance in ADR mention extraction. 	1709.01687	Shashank Gupta, Sachin Pawar, Nitin Ramrakhiyani, Girish Palshikar, Vasudeva Varma	2017-09-06		5	2	Semi-Supervised Recurrent Neural Network for Adverse Drug Reaction Mention Extraction		cs.IR, cs.CL	https://arxiv.org/pdf/1709.01687.pdf
33773	 We describe MITRE's submission to the SemEval-2016 Task 6, Detecting Stance in Tweets. This effort achieved the top score in Task A on supervised stance detection, producing an average F1 score of 67.8 when assessing whether a tweet author was in favor or against a topic. We employed a recurrent neural network initialized with features learned via distant supervision on two large unlabeled datasets. We trained embeddings of words and phrases with the word2vec skip-gram method, then used those features to learn sentence representations via a hashtag prediction auxiliary task. These sentence vectors were then fine-tuned for stance detection on several hundred labeled examples. The result was a high performing system that used transfer learning to maximize the value of the available training data. 	1606.03784	Guido Zarrella, Amy Marsh	2016-06-12		2	2	MITRE at SemEval-2016 Task 6: Transfer Learning for Stance Detection		cs.AI, cs.CL	https://arxiv.org/pdf/1606.03784.pdf
364271	 In this paper, we propose a Concept-level Emotion Cause Model (CECM), instead of the mere word-level models, to discover causes of microblogging users' diversified emotions on specific hot event. A modified topic-supervised biterm topic model is utilized in CECM to detect emotion topics' in event-related tweets, and then context-sensitive topical PageRank is utilized to detect meaningful multiword expressions as emotion causes. Experimental results on a dataset from Sina Weibo, one of the largest microblogging websites in China, show CECM can better detect emotion causes than baseline methods. 	1504.08050	Shuangyong Song, Yao Meng	2015-04-29		2	2	Detecting Concept-level Emotion Cause in Microblogging		cs.CL, cs.AI	https://arxiv.org/pdf/1504.08050.pdf
1054195	 Following a particular news story online is an important but difficult task, as the relevant information is often scattered across different domains/sources (e.g., news articles, blogs, comments, tweets), presented in various formats and language styles, and may overlap with thousands of other stories. In this work we join the areas of topic tracking and entity disambiguation, and propose a framework named Story Disambiguation - a cross-domain story tracking approach that builds on real-time entity disambiguation and a learning-to-rank framework to represent and update the rich semantic structure of news stories. Given a target news story, specified by a seed set of documents, the goal is to effectively select new story-relevant documents from an incoming document stream. We represent stories as entity graphs and we model the story tracking problem as a learning-to-rank task. This enables us to track content with high accuracy, from multiple domains, in real-time. We study a range of text, entity and graph based features to understand which type of features are most effective for representing stories. We further propose new semi-supervised learning techniques to automatically update the story representation over time. Our empirical study shows that we outperform the accuracy of state-of-the-art methods for tracking mixed-domain document streams, while requiring fewer labeled data to seed the tracked stories. This is particularly the case for local news stories that are easily over shadowed by other trending stories, and for complex news stories with ambiguous content in noisy stream environments. 	1808.05906	Bichen Shi, Thanh-Binh Le, Neil Hurley, Georgiana Ifrim	2018-08-16		4	4	Story Disambiguation: Tracking Evolving News Stories across News and Social Streams		cs.CL, cs.IR, cs.LG, stat.ML	https://arxiv.org/pdf/1808.05906.pdf
1095938	 NLP tasks are often limited by scarcity of manually annotated data. In social media sentiment analysis and related tasks, researchers have therefore used binarized emoticons and specific hashtags as forms of distant supervision. Our paper shows that by extending the distant supervision to a more diverse set of noisy labels, the models can learn richer representations. Through emoji prediction on a dataset of 1246 million tweets containing one of 64 common emojis we obtain state-of-the-art performance on 8 benchmark datasets within sentiment, emotion and sarcasm detection using a single pretrained model. Our analyses confirm that the diversity of our emotional labels yield a performance improvement over previous distant supervision approaches. 	1708.00524	Bjarke Felbo, Alan Mislove, Anders Søgaard, Iyad Rahwan, Sune Lehmann	2017-08-01		5	2	Using millions of emoji occurrences to learn any-domain representations for detecting sentiment, emotion and sarcasm	2017-10-07	stat.ML, cs.LG	https://arxiv.org/pdf/1708.00524.pdf
358603	 Human trafficking is a serious social problem, and it is challenging mainly because of its difficulty in collecting and organizing related information. With the increasing popularity of social media platforms, it provides us a novel channel to tackle the problem of human trafficking through detecting and analyzing a large amount of human trafficking related information. Existing supervised learning methods cannot be directly applied to this problem due to the distinct characteristics of the social media data. First, the short, noisy, and unstructured textual information makes traditional learning algorithms less effective in detecting human trafficking related tweets. Second, complex social interactions lead to a high-dimensional feature space and thus present great computational challenges. In the meanwhile, social sciences theories such as homophily have been well established and achieved success in various social media mining applications. Motivated by the sociological findings, in this paper, we propose to investigate whether the Network Structure Information (NSI) could be potentially helpful for the human trafficking problem. In particular, a novel mathematical optimization framework is proposed to integrate the network structure into content modeling. Experimental results on a real-world dataset demonstrate the effectiveness of our proposed framework in detecting human trafficking related information. 	1805.10617	Yang Yang, Xia Hu, Haoyan Liu, Jiawei Zhang, Zhoujun Li, Philip S. Yu	2018-05-27		6	2	Understanding and Monitoring Human Trafficking via Social Sensors: A Sociological Approach		cs.SI, cs.CY	https://arxiv.org/pdf/1805.10617.pdf
256457	 Research in analysis of microblogging platforms is experiencing a renewed surge with a large number of works applying representation learning models for applications like sentiment analysis, semantic textual similarity computation, hashtag prediction, etc. Although the performance of the representation learning models has been better than the traditional baselines for such tasks, little is known about the elementary properties of a tweet encoded within these representations, or why particular representations work better for certain tasks. Our work presented here constitutes the first step in opening the black-box of vector embeddings for tweets. Traditional feature engineering methods for high-level applications have exploited various elementary properties of tweets. We believe that a tweet representation is effective for an application because it meticulously encodes the application-specific elementary properties of tweets. To understand the elementary properties encoded in a tweet representation, we evaluate the representations on the accuracy to which they can model each of those properties such as tweet length, presence of particular words, hashtags, mentions, capitalization, etc. Our systematic extensive study of nine supervised and four unsupervised tweet representations against most popular eight textual and five social elementary properties reveal that Bi-directional LSTMs (BLSTMs) and Skip-Thought Vectors (STV) best encode the textual and social properties of tweets respectively. FastText is the best model for low resource settings, providing very little degradation with reduction in embedding size. Finally, we draw interesting insights by correlating the model performance obtained for elementary property prediction tasks with the highlevel downstream applications. 	1704.00898	J Ganesh, Manish Gupta, Vasudeva Varma	2017-04-04		3	1	Interpretation of Semantic Tweet Representations	2017-06-21	cs.CL	https://arxiv.org/pdf/1704.00898.pdf
402669	 We present our system for the CAp 2017 NER challenge which is about named entity recognition on French tweets. Our system leverages unsupervised learning on a larger dataset of French tweets to learn features feeding a CRF model. It was ranked first without using any gazetteer or structured external data, with an F-measure of 58.89\%. To the best of our knowledge, it is the first system to use fasttext embeddings (which include subword representations) and an embedding-based sentence representation for NER. 	1709.04820	Damien Sileo, Camille Pradel, Philippe Muller, Tim Van de Cruys	2017-09-14		4	1	Synapse at CAp 2017 NER challenge: Fasttext CRF		cs.CL	https://arxiv.org/pdf/1709.04820.pdf
1093055	 Unsupervised representation learning for tweets is an important research field which helps in solving several business applications such as sentiment analysis, hashtag prediction, paraphrase detection and microblog ranking. A good tweet representation learning model must handle the idiosyncratic nature of tweets which poses several challenges such as short length, informal words, unusual grammar and misspellings. However, there is a lack of prior work which surveys the representation learning models with a focus on tweets. In this work, we organize the models based on its objective function which aids the understanding of the literature. We also provide interesting future directions, which we believe are fruitful in advancing this field by building high-quality tweet representation learning models. 	1706.09673	Ganesh J	2017-06-29		1	1	Improving Distributed Representations of Tweets - Present and Future		cs.CL	https://arxiv.org/pdf/1706.09673.pdf
1238554	 Recently emerged intelligent assistants on smartphones and home electronics (e.g., Siri and Alexa) can be seen as novel hybrids of domain-specific task-oriented spoken dialogue systems and open-domain non-task-oriented ones. To realize such hybrid dialogue systems, this paper investigates determining whether or not a user is going to have a chat with the system. To address the lack of benchmark datasets for this task, we construct a new dataset consisting of 15; 160 utterances collected from the real log data of a commercial intelligent assistant (and will release the dataset to facilitate future research activity). In addition, we investigate using tweets and Web search queries for handling open-domain user utterances, which characterize the task of chat detection. Experiments demonstrated that, while simple supervised methods are effective, the use of the tweets and search queries further improves the F1-score from 86.21 to 87.53. 	1705.00746	Satoshi Akasaki, Nobuhiro Kaji	2017-05-01		2	1	Chat Detection in an Intelligent Assistant: Combining Task-oriented and Non-task-oriented Spoken Dialogue Systems	2018-07-23	cs.CL	https://arxiv.org/pdf/1705.00746.pdf
1374188	 In this work, we present a new dataset for computational humor, specifically comparative humor ranking, which attempts to eschew the ubiquitous binary approach to humor detection. The dataset consists of tweets that are humorous responses to a given hashtag. We describe the motivation for this new dataset, as well as the collection process, which includes a description of our semi-automated system for data collection. We also present initial experiments for this dataset using both unsupervised and supervised approaches. Our best supervised system achieved 63.7% accuracy, suggesting that this task is much more difficult than comparable humor detection tasks. Initial experiments indicate that a character-level model is more suitable for this task than a token-level model, likely due to a large amount of puns that can be captured by a character-level model. 	1612.03216	Peter Potash, Alexey Romanov, Anna Rumshisky	2016-12-09		3	1	#HashtagWars: Learning a Sense of Humor	2017-04-15	cs.CL	https://arxiv.org/pdf/1612.03216.pdf
1121681	 Nearly all previous work on geo-locating latent states and activities from social media confounds general discussions about activities, self-reports of users participating in those activities at times in the past or future, and self-reports made at the immediate time and place the activity occurs. Activities, such as alcohol consumption, may occur at different places and types of places, and it is important not only to detect the local regions where these activities occur, but also to analyze the degree of participation in them by local residents. In this paper, we develop new machine learning based methods for fine-grained localization of activities and home locations from Twitter data. We apply these methods to discover and compare alcohol consumption patterns in a large urban area, New York City, and a more suburban and rural area, Monroe County. We find positive correlations between the rate of alcohol consumption reported among a community's Twitter users and the density of alcohol outlets, demonstrating that the degree of correlation varies significantly between urban and suburban areas. While our experiments are focused on alcohol use, our methods for locating homes and distinguishing temporally-specific self-reports are applicable to a broad range of behaviors and latent states. 	1603.03181	Nabil Hossain, Tianran Hu, Roghayeh Feizi, Ann Marie White, Jiebo Luo, Henry Kautz	2016-03-10		6	2	Inferring Fine-grained Details on User Activities and Home Location from Social Media: Detecting Drinking-While-Tweeting Patterns in Communities		cs.AI, cs.SI	https://arxiv.org/pdf/1603.03181.pdf
1146426	 This paper is to explore the possibility to use alternative data and artificial intelligence techniques to trade stocks. The efficacy of the daily Twitter sentiment on predicting the stock return is examined using machine learning methods. Reinforcement learning(Q-learning) is applied to generate the optimal trading policy based on the sentiment signal. The predicting power of the sentiment signal is more significant if the stock price is driven by the expectation of the company growth and when the company has a major event that draws the public attention. The optimal trading strategy based on reinforcement learning outperforms the trading strategy based on the machine learning prediction. 	1801.02243	Catherine Xiao, Wanfeng Chen	2018-01-07		2	3	Trading the Twitter Sentiment with Reinforcement Learning		cs.AI, cs.CL, cs.SI	https://arxiv.org/pdf/1801.02243.pdf
1350009	 In the last several years, Twitter is being adopted by the companies as an alternative platform to interact with the customers to address their concerns. With the abundance of such unconventional conversation resources, push for developing effective virtual agents is more than ever. To address this challenge, a better understanding of such customer service conversations is required. Lately, there have been several works proposing a novel taxonomy for fine-grained dialogue acts as well as develop algorithms for automatic detection of these acts. The outcomes of these works are providing stepping stones for the ultimate goal of building efficient and effective virtual agents. But none of these works consider handling the notion of negation into the proposed algorithms. In this work, we developed an SVM-based dialogue acts prediction algorithm for Twitter customer service conversations where negation handling is an integral part of the end-to-end solution. For negation handling, we propose several efficient heuristics as well as adopt recent state-of- art third party machine learning based solutions. Empirically we show model's performance gain while handling negation compared to when we don't. Our experiments show that for the informal text such as tweets, the heuristic-based approach is more effective. 	1807.06107	Mansurul Bhuiyan, Amita Misra, Saurabh Tripathy, Jalal Mahmud, Rama Akkiraju	2018-07-16		5	2	Don't get Lost in Negation: An Effective Negation Handled Dialogue Acts Prediction Algorithm for Twitter Customer Service Conversations		cs.CL, cs.AI	https://arxiv.org/pdf/1807.06107.pdf
150636	 Twitter, a popular social network, presents great opportunities for on-line machine learning research. However, previous research has focused almost entirely on learning from passively collected data. We study the problem of learning to acquire followers through normative user behavior, as opposed to the mass following policies applied by many bots. We formalize the problem as a contextual bandit problem, in which we consider retweeting content to be the action chosen and each tweet (content) is accompanied by context. We design reward signals based on the change in followers. The result of our month long experiment with 60 agents suggests that (1) aggregating experience across agents can adversely impact prediction accuracy and (2) the Twitter community's response to different actions is non-stationary. Our findings suggest that actively learning on-line can provide deeper insights about how to attract followers than machine learning over passively collected data alone. 	1504.04114	Nir Levine, Timothy A. Mann, Shie Mannor	2015-04-16		3	3	Actively Learning to Attract Followers on Twitter		stat.ML, cs.LG, cs.SI	https://arxiv.org/pdf/1504.04114.pdf
170855	" In our work we analyse the political disaffection or ""the subjective feeling of powerlessness, cynicism, and lack of confidence in the political process, politicians, and democratic institutions, but with no questioning of the political regime"" by exploiting Twitter data through machine learning techniques. In order to validate the quality of the time-series generated by the Twitter data, we highlight the relations of these data with political disaffection as measured by means of public opinion surveys. Moreover, we show that important political news of Italian newspapers are often correlated with the highest peaks of the produced time-series. "	1301.6630	Corrado Monti, Alessandro Rozza, Giovanni Zappella, Matteo Zignani, Adam Arvidsson, Monica Poletti	2013-01-28		6	3	Political Disaffection: a case study on the Italian Twitter community	2013-02-08	cs.SI, cs.LG, physics.soc-ph	https://arxiv.org/pdf/1301.6630.pdf
642078	 Information extracted from social media streams has been leveraged to forecast the outcome of a large number of real-world events, from political elections to stock market fluctuations. An increasing amount of studies demonstrates how the analysis of social media conversations provides cheap access to the wisdom of the crowd. However, extents and contexts in which such forecasting power can be effectively leveraged are still unverified at least in a systematic way. It is also unclear how social-media-based predictions compare to those based on alternative information sources. To address these issues, here we develop a machine learning framework that leverages social media streams to automatically identify and predict the outcomes of soccer matches. We focus in particular on matches in which at least one of the possible outcomes is deemed as highly unlikely by professional bookmakers. We argue that sport events offer a systematic approach for testing the predictive power of social media, and allow to compare such power against the rigorous baselines set by external sources. Despite such strict baselines, our framework yields above 8% marginal profit when used to inform simple betting strategies. The system is based on real-time sentiment analysis and exploits data collected immediately before the games, allowing for informed bets. We discuss the rationale behind our approach, describe the learning framework, its prediction performance and the return it provides as compared to a set of betting strategies. To test our framework we use both historical Twitter data from the 2014 FIFA World Cup games, and real-time Twitter data collected by monitoring the conversations about all soccer matches of four major European tournaments (FA Premier League, Serie A, La Liga, and Bundesliga), and the 2014 UEFA Champions League, during the period between Oct. 25th 2014 and Nov. 26th 2014. 	1502.05886	Lei Le, Emilio Ferrara, Alessandro Flammini	2015-02-20	10.1145/2817946.2817949	3	4	On predictability of rare events leveraging social media: a machine learning perspective		cs.SI, cs.LG, physics.data-an, physics.soc-ph	https://arxiv.org/pdf/1502.05886.pdf
814522	 We present a machine learning framework that leverages a mixture of metadata, network, and temporal features to detect extremist users, and predict content adopters and interaction reciprocity in social media. We exploit a unique dataset containing millions of tweets generated by more than 25 thousand users who have been manually identified, reported, and suspended by Twitter due to their involvement with extremist campaigns. We also leverage millions of tweets generated by a random sample of 25 thousand regular users who were exposed to, or consumed, extremist content. We carry out three forecasting tasks, (i) to detect extremist users, (ii) to estimate whether regular users will adopt extremist content, and finally (iii) to predict whether users will reciprocate contacts initiated by extremists. All forecasting tasks are set up in two scenarios: a post hoc (time independent) prediction task on aggregated data, and a simulated real-time prediction task. The performance of our framework is extremely promising, yielding in the different forecasting scenarios up to 93% AUC for extremist user detection, up to 80% AUC for content adoption prediction, and finally up to 72% AUC for interaction reciprocity forecasting. We conclude by providing a thorough feature analysis that helps determine which are the emerging signals that provide predictive power in different scenarios. 	1605.00659	Emilio Ferrara, Wen-Qiang Wang, Onur Varol, Alessandro Flammini, Aram Galstyan	2016-05-02	10.1007/978-3-319-47874-6_3	5	3	Predicting online extremism, content adopters, and interaction reciprocity		cs.SI, cs.LG, physics.soc-ph	https://arxiv.org/pdf/1605.00659.pdf
1326192	 This paper maps the large-scale variation of the Spanish language by employing a corpus based on geographically tagged Twitter messages. Lexical dialects are extracted from an analysis of variants of tens of concepts. The resulting maps show linguistic variation on an unprecedented scale across the globe. We discuss the properties of the main dialects within a machine learning approach and find that varieties spoken in urban areas have an international character in contrast to country areas where dialects show a more regional uniformity. 	1511.04970	Bruno Gonçalves, David Sánchez	2015-11-16		2	5	Learning about Spanish dialects through Twitter	2017-02-05	stat.ML, cs.CL, cs.CY, physics.soc-ph, stat.AP	https://arxiv.org/pdf/1511.04970.pdf
41709	 Bots, social media accounts controlled by software rather than by humans, have recently been under the spotlight for their association with various forms of online manipulation. To date, much work has focused on social bot detection, but little attention has been devoted to the characterization and measurement of the behavior and activity of bots, as opposed to humans'. Over the course of the years, bots have become more sophisticated, and capable to reflect some short-term behavior, emulating that of human users. The goal of this paper is to study the behavioral dynamics that bots exhibit over the course of one activity session, and highlight if and how these differ from human activity signatures. By using a large Twitter dataset associated with recent political events, we first separate bots and humans, then isolate their activity sessions. We compile a list of quantities to be measured, like the propensity of users to engage in social interactions or to produce content. Our analysis highlights the presence of short-term behavioral trends in humans, which can be associated with a cognitive origin, that are absent in bots, intuitively due to their automated activity. These findings are finally codified to create and evaluate a machine learning algorithm to detect activity sessions produced by bots and humans, to allow for more nuanced bot detection strategies. 	1802.04286	Iacopo Pozzana, Emilio Ferrara	2018-02-12		2	2	Measuring bot and human behavioral dynamics	2018-02-14	cs.HC, cs.SI	https://arxiv.org/pdf/1802.04286.pdf
47287	 Recent accounts from researchers, journalists, as well as federal investigators, reached a unanimous conclusion: social media are systematically exploited to manipulate and alter public opinion. Some disinformation campaigns have been coordinated by means of bots, social media accounts controlled by computer scripts that try to disguise themselves as legitimate human users. In this study, we describe one such operation occurred in the run up to the 2017 French presidential election. We collected a massive Twitter dataset of nearly 17 million posts occurred between April 27 and May 7, 2017 (Election Day). We then set to study the MacronLeaks disinformation campaign: By leveraging a mix of machine learning and cognitive behavioral modeling techniques, we separated humans from bots, and then studied the activities of the two groups taken independently, as well as their interplay. We provide a characterization of both the bots and the users who engaged with them and oppose it to those users who didn't. Prior interests of disinformation adopters pinpoint to the reasons of the scarce success of this campaign: the users who engaged with MacronLeaks are mostly foreigners with a preexisting interest in alt-right topics and alternative news media, rather than French users with diverse political views. Concluding, anomalous account usage patterns suggest the possible existence of a black-market for reusable political disinformation bots. 	1707.00086	Emilio Ferrara	2017-06-30	10.5210/fm.v22i8.8005	1	3	Disinformation and Social Bot Operations in the Run Up to the 2017 French Presidential Election		cs.SI, cs.HC, physics.soc-ph	https://arxiv.org/pdf/1707.00086.pdf
212736	 Hate content in social media is ever increasing. While Facebook, Twitter, Google have attempted to take several steps to tackle this hate content, they most often risk the violation of freedom of speech. Counterspeech, on the other hand, provides an effective way of tackling the online hate without the loss of freedom of speech. Thus, an alternative strategy for these platforms could be to promote counterspeech as a defense against hate content. However, in order to have a successful promotion of such counterspeech, one has to have a deep understanding of its dynamics in the online world. Lack of carefully curated data largely inhibits such understanding. In this paper, we create and release the first ever dataset for counterspeech using comments from YouTube. The data contains 9438 manually annotated comments where the labels indicate whether a comment is a counterspeech or not. This data allows us to perform a rigorous measurement study characterizing the linguistic structure of counterspeech for the first time. This analysis results in various interesting insights such as: the counterspeech comments receive double the likes received by the non-counterspeech comments, for certain communities majority of the non-counterspeech comments tend to be hate speech, the different types of counterspeech are not all equally effective and the language choice of users posting counterspeech is largely different from those posting non-counterspeech as revealed by a detailed psycholinguistic analysis. Finally, we build a set of machine learning models that are able to automatically detect counterspeech in YouTube videos with an F1-score of 0.73. 	1808.04409	Binny Mathew, Hardik Tharad, Subham Rajgaria, Prajwal Singhania, Suman Kalyan Maity, Pawan Goyal, Animesh Mukherje	2018-08-13	10.13140/RG.2.2.31128.85765	7	1	Thou shalt not hate: Countering Online Hate Speech		cs.SI	https://arxiv.org/pdf/1808.04409.pdf
247926	 Microbloging is an extremely prevalent broadcast medium amidst the Internet fraternity these days. People share their opinions and sentiments about variety of subjects like products, news, institutions, etc., every day on microbloging websites. Sentiment analysis plays a key role in prediction systems, opinion mining systems, etc. Twitter, one of the microbloging platforms allows a limit of 140 characters to its users. This restriction stimulates users to be very concise about their opinion and twitter an ocean of sentiments to analyze. Twitter also provides developer friendly streaming API for data retrieval purpose allowing the analyst to search real time tweets from various users. In this paper, we discuss the state-of-art of the works which are focused on Twitter, the online social network platform, for sentiment analysis. We survey various lexical, machine learning and hybrid approaches for sentiment analysis on Twitter. 	1512.01043	Harsh Thakkar, Dhiren Patel	2015-12-03		2	3	Approaches for Sentiment Analysis on Twitter: A State-of-Art study		cs.SI, cs.CL, cs.IR	https://arxiv.org/pdf/1512.01043.pdf
304936	 Analysis of information retrieved from microblogging services such as Twitter can provide valuable insight into public sentiment in a geographic region. This insight can be enriched by visualising information in its geographic context. Two underlying approaches for sentiment analysis are dictionary-based and machine learning. The former is popular for public sentiment analysis, and the latter has found limited use for aggregating public sentiment from Twitter data. The research presented in this paper aims to extend the machine learning approach for aggregating public sentiment. To this end, a framework for analysing and visualising public sentiment from a Twitter corpus is developed. A dictionary-based approach and a machine learning approach are implemented within the framework and compared using one UK case study, namely the royal birth of 2013. The case study validates the feasibility of the framework for analysis and rapid visualisation. One observation is that there is good correlation between the results produced by the popular dictionary-based approach and the machine learning approach when large volumes of tweets are analysed. However, for rapid analysis to be possible faster methods need to be developed using big data techniques and parallel methods. 	1308.1847	Vu Dung Nguyen, Blesson Varghese, Adam Barker	2013-08-08	10.1109/BigData.2013.6691669	3	4	The Royal Birth of 2013: Analysing and Visualising Public Sentiment in the UK Using Twitter	2013-08-16	cs.CL, cs.IR, cs.SI, physics.soc-ph	https://arxiv.org/pdf/1308.1847.pdf
422498	 The right to protest is perceived as one of the primary civil rights. Citizens participate in mass demonstrations to express themselves and exercise their democratic rights. However, because of the large number of participants, protests may lead to violence and destruction, and hence can be costly. Thus, it is important to predict such demonstrations in advance to safeguard against such damages. Recent research has shown that about 75 percent of protests that are regarded as legal, are planned in advance. Twitter, the prominent micro-blogging website, has been used as a tool by protestors for planning, organizing, and announcing many of the recent protests worldwide such as those that led to the Arab Spring, Britain riots, and those against Mr. Trump after the presidential election in the U.S. In this paper, we aim to predict protests by means of machine learning algorithms. In particular, we consider the case of protests against the then-president-elect Mr. Trump after the results of the presidential election were announced in November 2016. We first identify the hashtags calling for demonstration from Trending Topics on Twitter, and download the corresponding tweets. We then apply four machine learning algorithms to make predictions. Our findings indicate that Twitter can be used as a powerful tool for predicting future protests with an average prediction accuracy of over 75 percent (up to 100 percent). We further validate our model by predicting the protests held in the U.S. airports after President Trump's executive order banning citizens of seven Muslim countries from entering the U.S. An important contribution of our study is the inclusion of event specific features for prediction purposes which helps to achieve high levels of accuracy. 	1805.00358	Mohsen Bahrami, Yasin Findik, Burcin Bozkaya, Selim Balcisoy	2018-05-01		4	1	Twitter Reveals: Using Twitter Analytics to Predict Public Protests		cs.SI	https://arxiv.org/pdf/1805.00358.pdf
491109	 Social media have substantially altered the way brands and businesses advertise: Online Social Networks provide brands with more versatile and dynamic channels for advertisement than traditional media (e.g., TV and radio). Levels of engagement in such media are usually measured in terms of content adoption (e.g., likes and retweets) and sentiment, around a given topic. However, sentiment analysis and topic identification are both non-trivial tasks. In this paper, using data collected from Twitter as a case study, we analyze how engagement and sentiment in promoted content spread over a 10-day period. We find that promoted tweets lead to higher positive sentiment than promoted trends; although promoted trends pay off in response volume. We observe that levels of engagement for the brand and promoted content are highest on the first day of the campaign, and fall considerably thereafter. However, we show that these insights depend on the use of robust machine learning and natural language processing techniques to gather focused, relevant datasets, and to accurately gauge sentiment, rather than relying on the simple keyword- or frequency-based metrics sometimes used in social media research. 	1312.6635	Shana Dacres, Hamed Haddadi, Matthew Purver	2013-12-23		3	2	Topic and Sentiment Analysis on OSNs: a Case Study of Advertising Strategies on Twitter		cs.SI, physics.soc-ph	https://arxiv.org/pdf/1312.6635.pdf
567795	 According to the Center for Disease Control and Prevention, in the United States hundreds of thousands initiate smoking each year, and millions live with smoking-related dis- eases. Many tobacco users discuss their habits and preferences on social media. This work conceptualizes a framework for targeted health interventions to inform tobacco users about the consequences of tobacco use. We designed a Twitter bot named Notobot (short for No-Tobacco Bot) that leverages machine learning to identify users posting pro-tobacco tweets and select individualized interventions to address their interest in tobacco use. We searched the Twitter feed for tobacco-related keywords and phrases, and trained a convolutional neural network using over 4,000 tweets dichotomously manually labeled as either pro- tobacco or not pro-tobacco. This model achieves a 90% recall rate on the training set and 74% on test data. Users posting pro- tobacco tweets are matched with former smokers with similar interests who posted anti-tobacco tweets. Algorithmic matching, based on the power of peer influence, allows for the systematic delivery of personalized interventions based on real anti-tobacco tweets from former smokers. Experimental evaluation suggests that our system would perform well if deployed. This research offers opportunities for public health researchers to increase health awareness at scale. Future work entails deploying the fully operational Notobot system in a controlled experiment within a public health campaign. 	1804.07886	Ashok Deb, Anuja Majmundar, Sungyong Seo, Akira Matsui, Rajat Tandon, Shen Yan, Jon-Patrick Allem, Emilio Ferrara	2018-04-21		8	1	Social Bots for Online Public Health Interventions		cs.SI	https://arxiv.org/pdf/1804.07886.pdf
635108	 Online social media, such as Twitter and Instagram, democratized information broadcast, allowing anyone to share information about themselves and their surroundings at an unprecedented scale. The large volume of information thus posted on these media offer a new lens into the physical world through the eyes of the social network. The exploitation of this lens to inspect aspects of world state has recently been termed social sensing. The power of manipulating reality via the use (or intentional misuse) of social media opened concerns with issues ranging from radicalization by terror propaganda to potential manipulation of elections in mature democracies. Many important challenges and open research questions arise in this emerging field that aims to better understand how information can be extracted from the medium and what properties characterize the extracted information and the world it represents. Addressing the above challenges requires multi-disciplinary research at the intersection of computer science and social sciences that combines cyber-physical computing, sociology, sensor networks, social networks, cognition, data mining, estimation theory, data fusion, information theory, linguistics, machine learning, behavioral economics, and possibly others. This paper surveys important directions in social sensing, identifies current research challenges, and outlines avenues for future research. 	1801.09116	Dong Wang, Boleslaw K. Szymanski, Tarek Abdelzaher, Heng Ji, Lance Kaplan	2018-01-27		5	2	The Age of Social Sensing		cs.SI, physics.soc-ph	https://arxiv.org/pdf/1801.09116.pdf
672631	 Current social networks are of extremely large-scale generating tremendous information flows at every moment. How information diffuse over social networks has attracted much attention from both industry and academics. Most of the existing works on information diffusion analysis are based on machine learning methods focusing on social network structure analysis and empirical data mining. However, the dynamics of information diffusion, which are heavily influenced by network users' decisions, actions and their socio-economic interactions, is generally ignored by most of existing works. In this paper, we propose an evolutionary game theoretic framework to model the dynamic information diffusion process in social networks. Specifically, we derive the information diffusion dynamics in complete networks, uniform degree and non-uniform degree networks, with the highlight of two special networks, Erd\H{o}s-R\'enyi random network and the Barab\'asi-Albert scale-free network. We find that the dynamics of information diffusion over these three kinds of networks are scale-free and the same with each other when the network scale is sufficiently large. To verify our theoretical analysis, we perform simulations for the information diffusion over synthetic networks and real-world Facebook networks. Moreover, we also conduct experiment on Twitter hashtags dataset, which shows that the proposed game theoretic model can well fit and predict the information diffusion over real social networks. 	1312.0317	Chunxiao Jiang, Yan Chen, K. J. Ray Liu	2013-12-01	10.1109/JSTSP.2014.2313024	3	2	Evolutionary Dynamics of Information Diffusion over Social Networks		cs.SI, physics.soc-ph	https://arxiv.org/pdf/1312.0317.pdf
680641	 Twitter is now an established and a widely popular news medium. Be it normal banter or a discussion on high impact events like Boston marathon blasts, February 2014 US Icestorm, etc., people use Twitter to get updates. Twitter bots have today become very common and acceptable. People are using them to get updates about emergencies like natural disasters, terrorist strikes, etc. Twitter bots provide these users a means to perform certain tasks on Twitter that are both simple and structurally repetitive. During high impact events these Twitter bots tend to provide time critical and comprehensive information. We present how bots participate in discussions and augment them during high impact events. We identify bots in high impact events for 2013: Boston blasts, February 2014 US Icestorm, Washington Navy Yard Shooting, Oklahoma tornado, and Cyclone Phailin. We identify bots among top tweeters by getting all such accounts manually annotated. We then study their activity and present many important insights. We determine the impact bots have on information diffusion during these events and how they tend to aggregate and broker information from various sources to different users. We also analyzed their tweets, list down important differentiating features between bots and non bots (normal or human accounts) during high impact events. We also show how bots are slowly moving away from traditional API based posts towards web automation platforms like IFTTT, dlvr.it, etc. Using standard machine learning, we proposed a methodology to identify bots/non bots in real time during high impact events. This study also looks into how the bot scenario has changed by comparing data from high impact events from 2013 with data from similar type of events from 2011. Lastly, we also go through an in-depth analysis of Twitter bots who were active during 2013 Boston Marathon Blast. 	1406.4286	Sudip Mittal, Ponnurangam Kumaraguru	2014-06-17		2	2	Broker Bots: Analyzing automated activity during High Impact Events on Twitter		cs.SI, physics.soc-ph	https://arxiv.org/pdf/1406.4286.pdf
722282	 Given the complexity of human minds and their behavioral flexibility, it requires sophisticated data analysis to sift through a large amount of human behavioral evidence to model human minds and to predict human behavior. People currently spend a significant amount of time on social media such as Twitter and Facebook. Thus many aspects of their lives and behaviors have been digitally captured and continuously archived on these platforms. This makes social media a great source of large, rich and diverse human behavioral evidence. In this paper, we survey the recent work on applying machine learning to infer human traits and behavior from social media data. We will also point out several future research directions. 	1804.04191	Shimei Pan, Tao Ding	2018-04-11		2	3	Automatically Infer Human Traits and Behavior from Social Media Data		cs.SI, cs.CL, cs.IR	https://arxiv.org/pdf/1804.04191.pdf
749458	 In February 2016, World Health Organization declared the Zika outbreak a Public Health Emergency of International Concern. With developing evidence it can cause birth defects, and the Summer Olympics coming up in the worst affected country, Brazil, the virus caught fire on social media. In this work, use Zika as a case study in building a tool for tracking the misinformation around health concerns on Twitter. We collect more than 13 million tweets -- spanning the initial reports in February 2016 and the Summer Olympics -- regarding the Zika outbreak and track rumors outlined by the World Health Organization and Snopes fact checking website. The tool pipeline, which incorporates health professionals, crowdsourcing, and machine learning, allows us to capture health-related rumors around the world, as well as clarification campaigns by reputable health organizations. In the case of Zika, we discover an extremely bursty behavior of rumor-related topics, and show that, once the questionable topic is detected, it is possible to identify rumor-bearing tweets using automated techniques. Thus, we illustrate insights the proposed tools provide into potentially harmful information on social media, allowing public health researchers and practitioners to respond with a targeted and timely action. 	1707.03778	Amira Ghenai, Yelena Mejova	2017-07-12		2	2	Catching Zika Fever: Application of Crowdsourcing and Machine Learning for Tracking Health Misinformation on Twitter		cs.SI, cs.CY	https://arxiv.org/pdf/1707.03778.pdf
753225	 Representation learning of networks via embeddings has garnered popularity and has witnessed significant progress recently. Such representations have been effectively used for classic network-based machine learning tasks like link prediction, community detection, and network alignment. However, most existing network embedding techniques largely focus on developing distributed representations for traditional flat networks and are unable to capture representations for multilayer networks. Large scale networks such as social networks and human brain tissue networks, for instance, can be effectively captured in multiple layers. In this work, we propose Multi-Net a fast and scalable embedding technique for multilayer networks. Our work adds a new wrinkle to the the recently introduced family of network embeddings like node2vec, LINE, DeepWalk, SIGNet, sub2vec, graph2vec, and OhmNet. We demonstrate the usability of Multi-Net by leveraging it to reconstruct the friends and followers network on Twitter using network layers mined from the body of tweets, like mentions network and the retweet network. This is the Work-in-progress paper and our preliminary contribution for multilayer network embeddings. 	1805.10172	Arunkumar Bagavathi, Siddharth Krishnan	2018-05-25		2	2	MultiNet: Scalable Multilayer Network Embeddings		cs.SI, physics.soc-ph	https://arxiv.org/pdf/1805.10172.pdf
840671	 Content polluters, or bots that hijack a conversation for political or advertising purposes are a known problem for event prediction, election forecasting and when distinguishing real news from fake news in social media data. Identifying this type of bot is particularly challenging, with state-of-the-art methods utilising large volumes of network data as features for machine learning models. Such datasets are generally not readily available in typical applications which stream social media data for real-time event prediction. In this work we develop a methodology to detect content polluters in social media datasets that are streamed in real-time. Applying our method to the problem of civil unrest event prediction in Australia, we identify content polluters from individual tweets, without collecting social network or historical data from individual accounts. We identify some peculiar characteristics of these bots in our dataset and propose metrics for identification of such accounts. We then pose some research questions around this type of bot detection, including: how good Twitter is at detecting content polluters and how well state-of-the-art methods perform in detecting bots in our dataset. 	1804.01235	Mehwish Nasim, Andrew Nguyen, Nick Lothian, Robert Cope, Lewis Mitchell	2018-04-04		5	1	Real-time Detection of Content Polluters in Partially Observable Twitter Networks	2018-04-17	cs.SI	https://arxiv.org/pdf/1804.01235.pdf
924117	 Measuring and forecasting opinion trends from real-time social media is a long-standing goal of big-data analytics. Despite its importance, there has been no conclusive scientific evidence so far that social media activity can capture the opinion of the general population. Here we develop a method to infer the opinion of Twitter users regarding the candidates of the 2016 US Presidential Election by using a combination of statistical physics of complex networks and machine learning based on hashtags co-occurrence to develop an in-domain training set approaching 1 million tweets. We investigate the social networks formed by the interactions among millions of Twitter users and infer the support of each user to the presidential candidates. The resulting Twitter trends follow the New York Times National Polling Average, which represents an aggregate of hundreds of independent traditional polls, with remarkable accuracy. Moreover, the Twitter opinion trend precedes the aggregated NYT polls by 10 days, showing that Twitter can be an early signal of global opinion trends. Our analytics unleash the power of Twitter to uncover social trends from elections, brands to political movements, and at a fraction of the cost of national polls. 	1610.01587	Alexandre Bovet, Flaviano Morone, Hernan A. Makse	2016-10-05	10.1038/s41598-018-26951-y	3	2	Validation of Twitter opinion trends with national polling aggregates: Hillary Clinton vs Donald Trump	2017-04-26	cs.SI, physics.soc-ph	https://arxiv.org/pdf/1610.01587.pdf
924338	 People increasingly use microblogging platforms such as Twitter during natural disasters and emergencies. Research studies have revealed the usefulness of the data available on Twitter for several disaster response tasks. However, making sense of social media data is a challenging task due to several reasons such as limitations of available tools to analyze high-volume and high-velocity data streams. This work presents an extensive multidimensional analysis of textual and multimedia content from millions of tweets shared on Twitter during the three disaster events. Specifically, we employ various Artificial Intelligence techniques from Natural Language Processing and Computer Vision fields, which exploit different machine learning algorithms to process the data generated during the disaster events. Our study reveals the distributions of various types of useful information that can inform crisis managers and responders as well as facilitate the development of future automated systems for disaster management. 	1805.05144	Firoj Alam, Ferda Ofli, Muhammad Imran, Michael Aupetit	2018-05-14		4	1	A Twitter Tale of Three Hurricanes: Harvey, Irma, and Maria	2018-05-15	cs.SI	https://arxiv.org/pdf/1805.05144.pdf
964212	 Social network has gained remarkable attention in the last decade. Accessing social network sites such as Twitter, Facebook LinkedIn and Google+ through the internet and the web 2.0 technologies has become more affordable. People are becoming more interested in and relying on social network for information, news and opinion of other users on diverse subject matters. The heavy reliance on social network sites causes them to generate massive data characterised by three computational issues namely; size, noise and dynamism. These issues often make social network data very complex to analyse manually, resulting in the pertinent use of computational means of analysing them. Data mining provides a wide range of techniques for detecting useful knowledge from massive datasets like trends, patterns and rules [44]. Data mining techniques are used for information retrieval, statistical modelling and machine learning. These techniques employ data pre-processing, data analysis, and data interpretation processes in the course of data analysis. This survey discusses different data mining techniques used in mining diverse aspects of the social network over decades going from the historical techniques to the up-to-date models, including our novel technique named TRCM. All the techniques covered in this survey are listed in the Table.1 including the tools employed as well as names of their authors. 	1312.4617	Mariam Adedoyin-Olowe, Mohamed Medhat Gaber, Frederic Stahl	2013-12-16		3	2	A Survey of Data Mining Techniques for Social Media Analysis	2014-04-16	cs.SI, cs.CL	https://arxiv.org/pdf/1312.4617.pdf
1029224	" Online social networks play a major role in the spread of information at very large scale and it becomes essential to provide means to analyse this phenomenon. In this paper we address the issue of predicting the temporal dynamics of the information diffusion process. We develop a graph-based approach built on the assumption that the macroscopic dynamics of the spreading process are explained by the topology of the network and the interactions that occur through it, between pairs of users, on the basis of properties at the microscopic level. We introduce a generic model, called T-BaSIC, and describe how to estimate its parameters from users behaviours using machine learning techniques. Contrary to classical approaches where the parameters are fixed in advance, T-BaSIC's parameters are functions depending of time, which permit to better approximate and adapt to the diffusion phenomenon observed in online social networks. Our proposal has been validated on real Twitter datasets. Experiments show that our approach is able to capture the particular patterns of diffusion depending of the studied sub-networks of users and topics. The results corroborate the ""two-step"" theory (1955) that states that information flows from media to a few ""opinion leaders"" who then transfer it to the mass population via social networks and show that it applies in the online context. This work also highlights interesting recommendations for future investigations. "	1302.5235	Adrien Guille, Hakim Hacid, Cécile Favre	2013-02-21		3	2	Predicting the Temporal Dynamics of Information Diffusion in Social Networks	2013-03-01	cs.SI, physics.soc-ph	https://arxiv.org/pdf/1302.5235.pdf
1060416	 The recent availability of large, high-resolution data sets of online human activity allowed for the study and characterization of the mechanisms shaping human interactions at an unprecedented level of accuracy. To this end, many efforts have been put forward to understand how people share and retrieve information when forging their opinion about a certain topic. Specifically, the detection of the political leaning of a person based on its online activity can support the forecasting of opinion trends in a given population. Here, we tackle this challenging task by combining complex networks theory and machine learning techniques. In particular, starting from a collection of more than 6 millions tweets, we characterize the structure and dynamics of the Italian online political debate about the constitutional referendum held in December 2016. We analyze the discussion pattern between different political communities and characterize the network of contacts therein. Moreover, we set up a procedure to infer the political leaning of Italian Twitter users, which allows us to accurately reconstruct the overall opinion trend given by official polls (Pearson's r=0.88) as well as to predict with good accuracy the final outcome of the referendum. Our study provides a large-scale examination of the Italian online political discussion through sentiment-analysis, thus setting a baseline for future studies on online political debate modeling. 	1805.07388	Jacopo Bindi, Davide Colombi, Flavio Iannelli, Nicola Politi, Michele Sugarelli, Raffaele Tavarone, Enrico Ubaldi	2018-05-18		7	2	Political Discussion and Leanings on Twitter: the 2016 Italian Constitutional Referendum		physics.soc-ph, cs.SI	https://arxiv.org/pdf/1805.07388.pdf
1187158	 Social media can be a double-edged sword for modern communications, either a convenient channel exchanging ideas or an unexpected conduit circulating fake news through a large population. Existing studies of fake news focus on efforts on theoretical modelling of propagation or identification methods based on black-box machine learning, neglecting the possibility of identifying fake news using only structural features of propagation of fake news compared to those of real news and in particular the ability to identify fake news at early stages of propagation. Here we track large databases of fake news and real news in both, Twitter in Japan and its counterpart Weibo in China, and accumulate their complete traces of re-posting. It is consistently revealed in both media that fake news spreads distinctively, even at early stages of spreading, in a structure that resembles multiple broadcasters, while real news circulates with a dominant source. A novel predictability feature emerges from this difference in their propagation networks, offering new paths of early detection of fake news in social media. Instead of commonly used features like texts or users for fake news identification, our finding demonstrates collective structural signals that could be useful for filtering out fake news at early stages of their propagation evolution. 	1803.03443	Zilong Zhao, Jichang Zhao, Yukie Sano, Orr Levy, Hideki Takayasu, Misako Takayasu, Daqing Li, Shlomo Havlin	2018-03-09		8	2	Fake news propagate differently from real news even at early stages of spreading	2018-03-11	physics.soc-ph, cs.SI	https://arxiv.org/pdf/1803.03443.pdf
1274954	" Social media is a rich source of user behavior and opinions. Twitter senses nearly 500 million tweets per day from 328 million users.An appropriate machine learning pipeline over this information enables up-to-date and cost-effective data collection for a wide variety of domains such as; social science, public health, the wisdom of the crowd, etc. In many of the domains, users demographic information is key to the identification of segments of the populations being studied. For instance, Which age groups are observed to abuse which drugs?, Which ethnicities are most affected by depression per location?. Twitter in its current state does not require users to provide any demographic information. We propose to create a machine learning system coupled with the DBpedia graph that predicts the most probable age of the Twitter user. In our process to build an age prediction model using social media text and user meta-data, we explore the existing state of the art approaches. Detailing our data collection, feature engineering cycle, model selection and evaluation pipeline, we will exhibit the efficacy of our approach by comparing with the ""predict mean"" age estimator baseline. "	1804.03362	Alan Smith, Manas Gaur	2018-04-10		2	1	What's my age?: Predicting Twitter User's Age using Influential Friend Network and DBpedia		cs.SI	https://arxiv.org/pdf/1804.03362.pdf
1379259	 Social media platforms provide continuous access to user generated content that enables real-time monitoring of user behavior and of events. The geographical dimension of such user behavior and events has recently caught a lot of attention in several domains: mobility, humanitarian, or infrastructural. While resolving the location of a user can be straightforward, depending on the affordances of their device and/or of the application they are using, in most cases, locating a user demands a larger effort, such as exploiting textual features. On Twitter for instance, only 2% of all tweets are geo-referenced. In this paper, we present a system for zoomed-in grounding (below city level) for short messages (e.g., tweets). The system combines different natural language processing and machine learning techniques to increase the number of geo-grounded tweets, which is essential to many applications such as disaster response and real-time traffic monitoring. 	1703.04280	Noora Al Emadi, Sofiane Abbar, Javier Borge-Holthoefer, Francisco Guzman, Fabrizio Sebastiani	2017-03-13		5	1	QT2S: A System for Monitoring Road Traffic via Fine Grounding of Tweets		cs.SI	https://arxiv.org/pdf/1703.04280.pdf
251438	" Mixed language data is one of the difficult yet less explored domains of natural language processing. Most research in fields like machine translation or sentiment analysis assume monolingual input. However, people who are capable of using more than one language often communicate using multiple languages at the same time. Sociolinguists believe this ""code-switching"" phenomenon to be socially motivated. For example, to express solidarity or to establish authority. Most past work depend on external tools or resources, such as part-of-speech tagging, dictionary look-up, or named-entity recognizers to extract rich features for training machine learning models. In this paper, we train recurrent neural networks with only raw features, and use word embedding to automatically learn meaningful representations. Using the same mixed-language Twitter corpus, our system is able to outperform the best SVM-based systems reported in the EMNLP'14 Code-Switching Workshop by 1% in accuracy, or by 17% in error rate reduction. "	1412.4314	Joseph Chee Chang, Chu-Cheng Lin	2014-12-14		2	2	Recurrent-Neural-Network for Language Detection on Twitter Code-Switching Corpus	2014-12-22	cs.NE, cs.CL	https://arxiv.org/pdf/1412.4314.pdf
603287	 Since a tweet is limited to 140 characters, it is ambiguous and difficult for traditional Natural Language Processing (NLP) tools to analyse. This research presents KeyXtract which enhances the machine learning based Stanford CoreNLP Part-of-Speech (POS) tagger with the Twitter model to extract essential keywords from a tweet. The system was developed using rule-based parsers and two corpora. The data for the research was obtained from a Twitter profile of a telecommunication company. The system development consisted of two stages. At the initial stage, a domain specific corpus was compiled after analysing the tweets. The POS tagger extracted the Noun Phrases and Verb Phrases while the parsers removed noise and extracted any other keywords missed by the POS tagger. The system was evaluated using the Turing Test. After it was tested and compared against Stanford CoreNLP, the second stage of the system was developed addressing the shortcomings of the first stage. It was enhanced using Named Entity Recognition and Lemmatization. The second stage was also tested using the Turing test and its pass rate increased from 50.00% to 83.33%. The performance of the final system output was measured using the F1 score. Stanford CoreNLP with the Twitter model had an average F1 of 0.69 while the improved system had a F1 of 0.77. The accuracy of the system could be improved by using a complete domain specific corpus. Since the system used linguistic features of a sentence, it could be applied to other NLP tools. 	1708.02912	Tharindu Weerasooriya, Nandula Perera, S. R. Liyanage	2017-08-09		3	2	KeyXtract Twitter Model - An Essential Keywords Extraction Model for Twitter Designed using NLP Tools		cs.CL, cs.IR	https://arxiv.org/pdf/1708.02912.pdf
687964	 Events of various kinds are mentioned and discussed in text documents, whether they are books, news articles, blogs or microblog feeds. The paper starts by giving an overview of how events are treated in linguistics and philosophy. We follow this discussion by surveying how events and associated information are handled in computationally. In particular, we look at how textual documents can be mined to extract events and ancillary information. These days, it is mostly through the application of various machine learning techniques. We also discuss applications of event detection and extraction systems, particularly in summarization, in the medical domain and in the context of Twitter posts. We end the paper with a discussion of challenges and future directions. 	1601.04012	Jugal Kalita	2016-01-15		1	1	Detecting and Extracting Events from Text Documents		cs.CL	https://arxiv.org/pdf/1601.04012.pdf
1034359	 With the advancement of web technology and its growth, there is a huge volume of data present in the web for internet users and a lot of data is generated too. Internet has become a platform for online learning, exchanging ideas and sharing opinions. Social networking sites like Twitter, Facebook, Google+ are rapidly gaining popularity as they allow people to share and express their views about topics,have discussion with different communities, or post messages across the world. There has been lot of work in the field of sentiment analysis of twitter data. This survey focuses mainly on sentiment analysis of twitter data which is helpful to analyze the information in the tweets where opinions are highly unstructured, heterogeneous and are either positive or negative, or neutral in some cases. In this paper, we provide a survey and a comparative analyses of existing techniques for opinion mining like machine learning and lexicon-based approaches, together with evaluation metrics. Using various machine learning algorithms like Naive Bayes, Max Entropy, and Support Vector Machine, we provide a research on twitter data streams.General challenges and applications of Sentiment Analysis on Twitter are also discussed in this paper. 	1601.06971	Vishal. A. Kharde, Prof. Sheetal. Sonawane	2016-01-26	10.5120/ijca2016908625	2	1	Sentiment Analysis of Twitter Data: A Survey of Techniques	2016-04-22	cs.CL	https://arxiv.org/pdf/1601.06971.pdf
1046481	 Adverse reactions caused by drugs following their release into the market are among the leading causes of death in many countries. The rapid growth of electronically available health related information, and the ability to process large volumes of them automatically, using natural language processing (NLP) and machine learning algorithms, have opened new opportunities for pharmacovigilance. Survey found that more than 70% of US Internet users consult the Internet when they require medical information. In recent years, research in this area has addressed for Adverse Drug Reaction (ADR) pharmacovigilance using social media, mainly Twitter and medical forums and websites. This paper will show the information which can be collected from a variety of Internet data sources and search engines, mainly Google Trends and Google Correlate. While considering the case study of two popular Major depressive Disorder (MDD) drugs, Duloxetine and Venlafaxine, we will provide a comparative analysis for their reactions using publicly-available alternative data sources. 	1610.02567	Abbas Chokor, Abeed Sarker, Graciela Gonzalez	2016-10-08		3	2	Mining the Web for Pharmacovigilance: the Case Study of Duloxetine and Venlafaxine		cs.CL, cs.CY	https://arxiv.org/pdf/1610.02567.pdf
1396183	 'Health utilities' measure patient preferences for perfect health compared to specific unhealthy states, such as asthma, a fractured hip, or colon cancer. When integrated over time, these estimations are called quality adjusted life years (QALYs). Until now, characterizing health utilities (HUs) required detailed patient interviews or written surveys. While reliable and specific, this data remained costly due to efforts to locate, enlist and coordinate participants. Thus the scope, context and temporality of diseases examined has remained limited. Now that more than a billion people use social media, we propose a novel strategy: use natural language processing to analyze public online conversations for signals of the severity of medical conditions and correlate these to known HUs using machine learning. In this work, we filter a dataset that originally contained 2 billion tweets for relevant content on 60 diseases. Using this data, our algorithm successfully distinguished mild from severe diseases, which had previously been categorized only by traditional techniques. This represents progress towards two related applications: first, predicting HUs where such information is nonexistent; and second, (where rich HU data already exists) estimating temporal or geographic patterns of disease severity through data mining. 	1608.03938	Christopher Thompson, Josh Introne, Clint Young	2016-08-13		3	4	Determining Health Utilities through Data Mining of Social Media		cs.CL, cs.AI, cs.CY, cs.SI	https://arxiv.org/pdf/1608.03938.pdf
1097213	" Machine learning methods have gained a great deal of popularity in recent years among public administration scholars and practitioners. These techniques open the door to the analysis of text, image and other types of data that allow us to test foundational theories of public administration and to develop new theories. Despite the excitement surrounding machine learning methods, clarity regarding their proper use and potential pitfalls is lacking. This paper attempts to fill this gap in the literature through providing a machine learning ""guide to practice"" for public administration scholars and practitioners. Here, we take a foundational view of machine learning and describe how these methods can enrich public administration research and practice through their ability develop new measures, tap into new sources of data and conduct statistical inference and causal inference in a principled manner. We then turn our attention to the pitfalls of using these methods such as unvalidated measures and lack of interpretability. Finally, we demonstrate how machine learning techniques can help us learn about organizational reputation in federal agencies through an illustrated example using tweets from 13 executive federal agencies. "	1805.05409	L. Jason Anastasopoulos, Andrew B. Whitford	2018-05-11		2	3	Machine Learning for Public Administration Research, with Application to Organizational Reputation	2018-09-11	cs.CY, cs.LG, stat.ML	https://arxiv.org/pdf/1805.05409.pdf
3529	 Using machine learning algorithms, including deep learning, we studied the prediction of personal attributes from the text of tweets, such as gender, occupation, and age groups. We applied word2vec to construct word vectors, which were then used to vectorize tweet blocks. The resulting tweet vectors were used as inputs for training models, and the prediction accuracy of those models was examined as a function of the dimension of the tweet vectors and the size of the tweet blacks. The results showed that the machine learning algorithms could predict the three personal attributes of interest with 60-70% accuracy. 	1709.09927	Take Yo, Kazutoshi Sasahara	2017-09-28		2	3	Inference of Personal Attributes from Tweets Using Machine Learning	2017-12-24	cs.CY, cs.CL, cs.SI	https://arxiv.org/pdf/1709.09927.pdf
175759	 Hurricane Sandy was one of the deadliest and costliest of hurricanes over the past few decades. Many states experienced significant power outage, however many people used social media to communicate while having limited or no access to traditional information sources. In this study, we explored the evolution of various communication patterns using machine learning techniques and determined user concerns that emerged over the course of Hurricane Sandy. The original data included ~52M tweets coming from ~13M users between October 14, 2012 and November 12, 2012. We run topic model on ~763K tweets from top 4,029 most frequent users who tweeted about Sandy at least 100 times. We identified 250 well-defined communication patterns based on perplexity. Conversations of most frequent and relevant users indicate the evolution of numerous storm-phase (warning, response, and recovery) specific topics. People were also concerned about storm location and time, media coverage, and activities of political leaders and celebrities. We also present each relevant keyword that contributed to one particular pattern of user concerns. Such keywords would be particularly meaningful in targeted information spreading and effective crisis communication in similar major disasters. Each of these words can also be helpful for efficient hash-tagging to reach target audience as needed via social media. The pattern recognition approach of this study can be used in identifying real time user needs in future crises. 	1710.01887	Arif Mohaimin Sadri, Samiul Hasan, Satish V. Ukkusuri, Manuel Cebrian	2017-10-05		4	1	Crisis Communication Patterns in Social Media during Hurricane Sandy		cs.SI	https://arxiv.org/pdf/1710.01887.pdf
566030	 As breaking news unfolds people increasingly rely on social media to stay abreast of the latest updates. The use of social media in such situations comes with the caveat that new information being released piecemeal may encourage rumours, many of which remain unverified long after their point of release. Little is known, however, about the dynamics of the life cycle of a social media rumour. In this paper we present a methodology that has enabled us to collect, identify and annotate a dataset of 330 rumour threads (4,842 tweets) associated with 9 newsworthy events. We analyse this dataset to understand how users spread, support, or deny rumours that are later proven true or false, by distinguishing two levels of status in a rumour life cycle i.e., before and after its veracity status is resolved. The identification of rumours associated with each event, as well as the tweet that resolved each rumour as true or false, was performed by a team of journalists who tracked the events in real time. Our study shows that rumours that are ultimately proven true tend to be resolved faster than those that turn out to be false. Whilst one can readily see users denying rumours once they have been debunked, users appear to be less capable of distinguishing true from false rumours when their veracity remains in question. In fact, we show that the prevalent tendency for users is to support every unverified rumour. We also analyse the role of different types of users, finding that highly reputable users such as news organisations endeavour to post well-grounded statements, which appear to be certain and accompanied by evidence. Nevertheless, these often prove to be unverified pieces of information that give rise to false rumours. Our study reinforces the need for developing robust machine learning techniques that can provide assistance for assessing the veracity of rumours. 	1511.07487	Arkaitz Zubiaga, Maria Liakata, Rob Procter, Geraldine Wong Sak Hoi, Peter Tolmie	2015-11-23	10.1371/journal.pone.0150989	5	1	Analysing How People Orient to and Spread Rumours in Social Media by Looking at Conversational Threads	2016-02-25	cs.SI	https://arxiv.org/pdf/1511.07487.pdf
1021775	 Cybercriminals have found in online social networks a propitious medium to spread spam and malicious content. Existing techniques for detecting spam include predicting the trustworthiness of accounts and analyzing the content of these messages. However, advanced attackers can still successfully evade these defenses. Online social networks bring people who have personal connections or share common interests to form communities. In this paper, we first show that users within a networked community share some topics of interest. Moreover, content shared on these social network tend to propagate according to the interests of people. Dissemination paths may emerge where some communities post similar messages, based on the interests of those communities. Spam and other malicious content, on the other hand, follow different spreading patterns. In this paper, we follow this insight and present POISED, a system that leverages the differences in propagation between benign and malicious messages on social networks to identify spam and other unwanted content. We test our system on a dataset of 1.3M tweets collected from 64K users, and we show that our approach is effective in detecting malicious messages, reaching 91% precision and 93% recall. We also show that POISED's detection is more comprehensive than previous systems, by comparing it to three state-of-the-art spam detection systems that have been proposed by the research community in the past. POISED significantly outperforms each of these systems. Moreover, through simulations, we show how POISED is effective in the early detection of spam messages and how it is resilient against two well-known adversarial machine learning attacks. 	1708.09058	Shirin Nilizadeh, Francois Labreche, Alireza Sedighian, Ali Zand, Jose Fernandez, Christopher Kruegel, Gianluca Stringhini, Giovanni Vigna	2017-08-29		8	2	POISED: Spotting Twitter Spam Off the Beaten Paths		cs.CR, cs.SI	https://arxiv.org/pdf/1708.09058.pdf
1072819	 Being dominant factors driving the human actions, personalities can be excellent indicators in predicting the offline and online behavior of different individuals. However, because of the great expense and inevitable subjectivity in questionnaires and surveys, it is challenging for conventional studies to explore the connection between personality and behavior and gain insights in the context of large amount individuals. Considering the more and more important role of the online social media in daily communications, we argue that the footprint of massive individuals, like tweets in Weibo, can be the inspiring proxy to infer the personality and further understand its functions in shaping the online human behavior. In this study, a map from self-reports of personalities to online profiles of 293 active users in Weibo is established to train a competent machine learning model, which then successfully identifies over 7,000 users as extroverts or introverts. Systematical comparisons from perspectives of tempo-spatial patterns, online activities, emotion expressions and attitudes to virtual honor surprisingly disclose that the extrovert indeed behaves differently from the introvert in Weibo. Our findings provide solid evidence to justify the methodology of employing machine learning to objectively study personalities of massive individuals and shed lights on applications of probing personalities and corresponding behaviors solely through online profiles. 	1703.06637	Zhenkun Zhou, Ke Xu, Jichang Zhao	2017-03-20		3	3	Extroverts Tweet Differently from Introverts in Weibo		cs.CY, cs.HC, cs.SI	https://arxiv.org/pdf/1703.06637.pdf
276195	 Clickbait is a pejorative term describing web content that is aimed at generating online advertising revenue, especially at the expense of quality or accuracy, relying on sensationalist headlines or eye-catching thumbnail pictures to attract click-throughs and to encourage forwarding of the material over online social networks. We use distributed word representations of the words in the title as features to identify clickbaits in online news media. We train a machine learning model using linear regression to predict the cickbait score of a given tweet. Our methods achieve an F1-score of 64.98\% and an MSE of 0.0791. Compared to other methods, our method is simple, fast to train, does not require extensive feature engineering and yet moderately effective. 	1710.02861	Vijayasaradhi Indurthi, Subba Reddy Oota	2017-10-08		2	2	Clickbait detection using word embeddings		cs.CL, cs.IR	https://arxiv.org/pdf/1710.02861.pdf
631585	 We present the first shared task on detecting the intensity of emotion felt by the speaker of a tweet. We create the first datasets of tweets annotated for anger, fear, joy, and sadness intensities using a technique called best--worst scaling (BWS). We show that the annotations lead to reliable fine-grained intensity scores (rankings of tweets by intensity). The data was partitioned into training, development, and test sets for the competition. Twenty-two teams participated in the shared task, with the best system obtaining a Pearson correlation of 0.747 with the gold intensity scores. We summarize the machine learning setups, resources, and tools used by the participating teams, with a focus on the techniques and resources that are particularly useful for the task. The emotion intensity dataset and the shared task are helping improve our understanding of how we convey more or less intense emotions through language. 	1708.03700	Saif M. Mohammad, Felipe Bravo-Marquez	2017-08-11		2	1	WASSA-2017 Shared Task on Emotion Intensity		cs.CL	https://arxiv.org/pdf/1708.03700.pdf
